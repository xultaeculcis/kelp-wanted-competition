This guide contains steps necessary to reproduce the competition results.

Before running any of the commands in this section, please make sure you have configured your local development
environment by following [this](setup-dev-env.md) guide.

## Preparing the data

### Download the data

Download and extract the data to following directories in the root of the repo:

```
kelp-wanted-competition/
└── data
   └── raw
      ├── train
      │   ├── images             <= place training images here
      │   └── masks              <= place training masks here
      ├── test
      │   └── images             <= place test images here
      └── metadata_fTq0l2T.csv   <= place the metadata file directly in the `raw` dir
```

Run in order:

* Plot samples - for better understanding of the data and quick visual inspection

    ```shell
    make sample-plotting
    ```

* AOI Grouping - will group the similar images into AOIs and use those groups to generate CV-folds.

    ```shell
    make aoi-grouping
    ```

* EDA - run Exploratory Data Analysis to visualize statistical distributions of different image features

    ```shell
    make eda
    ```

* Calculate band statistics - will calculate per-band min, max, mean, std etc. statistics (including spectral indices)

    ```shell
    make calculate-band-stats
    ```

* Train-Val-Test split with Stratified K-Fold Cross Validation

    ```shell
    make train-val-test-split-cv
    ```

The generated `train_val_test_dataset_strategy=cross_val.parquet` metadata lookup file
and `YYYY-MM-DD-Thh:mm:ss-stats-fill_value=nan-mask_using_qa=True-mask_using_water_mask=True.json`
files will have to be used as inputs for the training scripts, both locally and for Azure ML Pipelines.

## Training the models

### Via Azure ML (AML)

> Note: You'll need Azure Subscription and Azure DevOps Organization for that one.
> You'll also need a basic knowledge of Azure services such as Entra ID, Blob Storage and Azure ML.

1. Create Azure ML Workspace
2. Setup Service Principal with access to the Azure ML Workspace
3. Setup Azure DevOps variable group
(see [.env-sample](https://github.com/xultaeculcis/kelp-wanted-competition/blob/main/.env-sample) for what variables are needed)
4. Setup Service Connections for GitHub, AML Workspace and ARM Resource Group (use SP created earlier for it)
5. Setup Azure DevOps Pipelines
6. In Azure ML set up the following:
   1. [Datasets datastore](https://github.com/xultaeculcis/kelp-wanted-competition/blob/main/aml/resources/datastores/datasets-datastore.yaml)
   2. Training Dataset Data Asset - please upload training data to Blob Storage and register it as Folder Asset
   3. Dataset Stats Data Asset - please upload the stats file to Blob Storage and register it as File Asset
   4. Dataset Metadata Data Asset - please upload the metadata parquet file (generated by `train-val-test-split-cv` Makefile command)
   to Blob Storage and register it as File Asset
   5. [Compute Clusters with spot instances](https://github.com/xultaeculcis/kelp-wanted-competition/tree/main/aml/resources/compute)
   6. [Training Environment](https://github.com/xultaeculcis/kelp-wanted-competition/tree/main/aml/environments/acpt_train_env)
7. Once done you'll need to modify the versions and names in the
[AML components](https://github.com/xultaeculcis/kelp-wanted-competition/tree/main/aml/components)
and [AML pipelines](https://github.com/xultaeculcis/kelp-wanted-competition/tree/main/aml/pipelines) to match the resource
names you have just created. I recommend you use
[Azure ML CLI](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-cli?view=azureml-api-2&tabs=public)
to set them up from the terminal.
See `yaml` files in the [aml](https://github.com/xultaeculcis/kelp-wanted-competition/tree/main/aml) folder for details.
8. You can now trigger the Azure ML Hyperparameter Search or Model Training Pipelines via Azure DevOps Pipelines

### Locally

* Run all folds training:

    ```shell
    make train-all-folds
    ```

* Run single fold training:

    ```shell
    make train FOLD_NUMBER=<fold-number>
    ```

See the Makefile definition to see all available options.

> Note: Both Azure ML Pipelines and Makefile commands have been adjusted to use hyperparameters used in the best model submission.

## Making predictions

Once the models have been trained you can generate submission file with them by adjusting the run_dir paths in the
Makefile and running following commands:

### Single model

Run:

```shell
make predict-and-submit
```

### Ensemble

Running ensemble prediction is just as easy with following command:

```shell
make cv-predict
```

This will run prediction with each fold individually and then average the predictions using weights specified in the
Makefile. The weights and decision thresholds in the Makefile commands are already set up to be in line with the winning
ones.

> Note: Please note that all folds were used for the best submission. You'll need to train all folds!
