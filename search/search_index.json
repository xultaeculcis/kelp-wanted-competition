{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"kelp-wanted-competition","text":"<p>Estimating the extent of Giant Kelp Forests by segmenting Landsat imagery</p> <p></p> <p>Overhead drone footage of giant kelp canopy. Image Credit: Tom Bell, All Rights Reserved.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Read the Setting up dev environment.</p>"},{"location":"#reproducing-the-results","title":"Reproducing the results","text":"<p>See the reproducibility guide for details.</p>"},{"location":"#technical-report","title":"Technical report","text":"<p>The technical report is hosted here.</p>"},{"location":"#dev-log","title":"Dev Log","text":"<p>To see in details what was done to train the model please see the DEV Log page.</p>"},{"location":"#how-to-guides","title":"How-to guides","text":"<p>On the documentation page you'll find following How-to guides:</p> <ul> <li>Setting up dev environment</li> <li>Contributing</li> <li>Running tests</li> <li>Using Makefile commands</li> <li>Reproducibility of results</li> <li>Preparing data</li> <li>Training models</li> <li>MLFlow artifacts</li> <li>Evaluating models</li> <li>Running inference</li> <li>Making submissions</li> <li>XGBoost</li> <li>SAHI</li> </ul>"},{"location":"#code-docs","title":"Code docs","text":"<p>The code documentation is here.</p>"},{"location":"dev-log/","title":"Dev Log","text":"<p>Checklist:</p> <ul> <li>[x] EDA</li> <li>[x] Use metadata csv as lookup for the dataset</li> <li>[x] Log a few hundred images during validation loop</li> <li>[x] Log segmentation metrics</li> <li>[x] MLFlow logger</li> <li>[x] Pre-process data and add extra channels: NDVI, NDWI, EVI, water masks from different sources (NDVI, DEM) etc.</li> <li>[x] Unet baseline with pre-trained ResNet-50 backbone</li> <li>[x] Inference script</li> <li>[x] Submission script</li> <li>[x] 10-fold CV instead of 5-fold</li> <li>[x] Change channel order (SWIR, NIR, R) -&gt; (R, G, B)</li> <li>[x] Training from scratch vs pre-trained weights</li> <li>[x] <code>OneCycleLR</code> or Cosine schedule</li> <li>[x] Log confusion matrix</li> <li>[x] Log prediction grid during eval loop</li> <li>[x] Find images of the same area and bin them together to avoid data leakage (must have since CRS is missing) - use   embeddings to find similar images (DEM layer can be good candidate to find images of the same AOI)</li> <li>[x] More robust CV split with deduplication of images from val set</li> <li>[x] Different data normalization strategies (min-max, quantile, z-score, per-image min-max)</li> <li>[x] Different loss functions</li> <li>[x] Weighted sampler</li> <li>[x] Azure ML Hparam Search</li> <li>[x] Add extra spectral indices combinations</li> <li>[x] Eval script</li> <li>[x] TTA</li> <li>[x] Decision threshold optimization</li> <li>[x] ConvNeXt v1/v2 - not supported by <code>segmentation-models-pytorch</code></li> <li>[x] EfficientNet v1/v2</li> <li>[x] ResNeXt</li> <li>[x] SwinV2-B - not supported by <code>segmentation-models-pytorch</code></li> <li>[x] Model Ensemble</li> <li>[x] Mask post-processing</li> <li>[x] Build parquet dataset for training Tree-based models -&gt; all <code>kelp</code> pixels, few-pixel buffer around them,   and random sample of 1000 <code>non-kelp</code> pixels per image</li> <li>[x] Train Random Forest, XGBoost, LightGBM, CatBoost on enhanced data</li> <li>[x] Soft labels</li> <li>[x] Model weights averaging</li> <li>[x] SAHI</li> <li>[x] Prepare docs on how to train and predict</li> <li>[x] Build a CLI for eda, training, prediction and submission</li> </ul>"},{"location":"dev-log/#what-worked","title":"What worked","text":"<ul> <li>Pre-trained weights</li> <li>Appending NDVI</li> <li>Reorder channels into R,G,B,SWIR,NIR,QA,DEM,NDVI</li> <li>AdamW instead of Adam</li> <li>Weight decay = 1e-4</li> <li>Learning rate 3e-4</li> <li>AOI grouping (removing leakage)</li> <li>Quantile normalization</li> <li>Dice Loss</li> <li>Weighted sampler<ul> <li><code>has_kelp_importance_factor=3.0</code></li> <li><code>kelp_pixels_pct_importance_factor=0.2</code></li> <li><code>qa_ok_importance_factor=0.0</code></li> <li><code>qa_corrupted_pixels_pct_importance_factor=-1.0</code></li> <li><code>almost_all_water_importance_factor=0.5</code></li> <li><code>dem_nan_pixels_pct_importance_factor=0.25</code></li> <li><code>dem_zero_pixels_pct_importance_factor=-1.0</code></li> </ul> </li> <li>Masking indices with QA and DEM Water Mask</li> <li>Extra spectral indices:<ul> <li>DEMWM,</li> <li>NDVI,</li> <li>ATSAVI,</li> <li>AVI,</li> <li>CI,</li> <li>ClGreen,</li> <li>GBNDVI,</li> <li>GVMI,</li> <li>IPVI,</li> <li>KIVU,</li> <li>MCARI,</li> <li>MVI,</li> <li>NormNIR,</li> <li>PNDVI,</li> <li>SABI,</li> <li>WDRVI,</li> <li>mCRIG</li> </ul> </li> <li>Test Time Augmentations (only local runs)</li> <li>EfficientNet-B5</li> <li>Decision threshold change to 0.45-0.48</li> <li><code>OneCycleLR</code></li> <li>10-fold CV</li> <li>Training for 50 epochs (best model ensemble)</li> <li>Mixing models with best dice per split in the ensemble</li> <li>Soft labels (second-best model used them)</li> <li>Mixing model architectures and encoders such as:<ul> <li><code>unet</code></li> <li><code>linknet</code></li> <li><code>unet++</code></li> <li><code>tu-efficeintnet_b5</code></li> <li><code>tu-mobilevitv2_150.cvnets_in22k_ft_in1k_384</code></li> <li><code>tu-maxvit_small_tf_384.in1k</code></li> <li><code>tu-seresnextaa101d_32x8d.sw_in12k_ft_in1k</code></li> <li><code>tu-rexnetr_200.sw_in12k_ft_in1k</code></li> <li><code>tu-seresnext26d_32x4d</code></li> <li><code>tu-gcresnet33ts.ra2_in1k</code></li> <li><code>tu-seresnext101_32x4d</code></li> </ul> </li> </ul>"},{"location":"dev-log/#what-did-not-work","title":"What did not work","text":"<ul> <li>Training from scratch</li> <li>Larger or smaller <code>weight_decay</code></li> <li>Larger or smaller <code>lr</code></li> <li><code>decoder_attention_type=\"scse\"</code></li> <li>Losses other than dice (CE with weighting was close)</li> <li>Compiling the model and <code>torch-ort</code></li> <li>Normalization strategies other than <code>quantile</code> / <code>z-score</code></li> <li>Bunch of different index combinations</li> <li>TTA (for leaderboard)</li> <li>LR Schedulers other than <code>OneCycleLR</code></li> <li>Random split</li> <li>XGBoost and other tree based models predicting on pixel level</li> <li>More decoder channels</li> <li>SAHI</li> <li>Resize strategy different than <code>pad</code></li> <li>Training with larger images</li> <li>Bigger batches</li> <li>More frequent val checks</li> <li>Smaller batch sizes and <code>accumulate_grad_batches</code> &gt; 1</li> </ul>"},{"location":"dev-log/#2023-12-02","title":"2023-12-02","text":"<ul> <li>Initial commit</li> <li>Data downloaded - slow transfer speeds observed - took over 2h to download all assets on 1Gb Internet connection</li> <li>No CRS specified in the GeoTiff files - cannot perform split easily... Idea: find the same regions using embeddings</li> <li><code>torchgeo.RasterDataset</code> cannot be used - use <code>torchgeo.VisionDataset</code> instead</li> </ul>"},{"location":"dev-log/#2023-12-03","title":"2023-12-03","text":"<ul> <li>WIP torch dataset implementation</li> <li>Added pre-processing script, need to calculate indices</li> <li>Plotted the data for quick visual inspection</li> </ul> <p>Findings:</p> <ul> <li>DEM will need to be clipped due to nan values -&gt; use <code>np.maximum(0, arr)</code></li> <li>Cloud mask is actually the QA mask - it also contains faulty pixels</li> </ul>"},{"location":"dev-log/#2023-12-08","title":"2023-12-08","text":"<ul> <li>-32k pixels are also in the main bands not just the DEM layer -&gt; substitute them with zeroes</li> <li>Spent last few days implementing spectral indices for Landsat scenes</li> <li>Standardization will not work due to data corruption issues - clouds, saturated pixels, striping etc</li> <li>Mask indices using QA band after computation</li> <li>Clamp values using quantiles (1-99)</li> <li>Add option to use min-max normalize using quantiles (1-99)</li> <li>WIP. Calculate stats using masked and clamped data</li> </ul>"},{"location":"dev-log/#2023-12-09","title":"2023-12-09","text":"<ul> <li>Finally, computed stats for whole dataset</li> <li>Added code to find images with a lot of invalid values in the QA mask</li> <li>Added sample plotting logic on <code>Dataset</code> class</li> </ul>"},{"location":"dev-log/#2023-12-12","title":"2023-12-12","text":"<ul> <li>Add stratified k-fold split - stratification using following per image flag combination: <code>qa_ok</code>, <code>has_kelp</code>,   <code>dem_has_nans</code>, <code>high_corrupted_pixels_pct</code></li> <li>Update augmentations</li> <li>Pad images to 352x352 - required by the model for the image shape to be divisible by 32</li> <li>Messed up something in the dataset - training batch has some <code>torch.nan</code> values</li> </ul>"},{"location":"dev-log/#2023-12-13","title":"2023-12-13","text":"<ul> <li>Add GPU utils to power limit RTX 3090 for more comfortable training temperatures</li> <li>Add dataset stats to <code>consts</code></li> <li>Remove <code>torchgeo</code> package dependency since its newest version conflicted with pydantic v2 - didn't use its   features anyway...</li> <li>Use index min value instead of <code>torch.nan</code> to mask corrupted pixels</li> <li>Finally, train a single epoch</li> <li><code>MLFlowLogger</code> is wonky - will use <code>mlflow</code> autologging capabilities</li> <li>Make <code>mlflow</code> logging work correctly</li> <li>Need to ensure checkpoint logging works as it should</li> <li>Make image logging work fine</li> <li>Image logging should no longer work during validation sanity checks</li> </ul>"},{"location":"dev-log/#2023-12-14","title":"2023-12-14","text":"<ul> <li>Remove device stats monitor callback</li> <li>Adjust checkpoint saving path to the same location as <code>mlflow</code> artifacts</li> <li>Training cannot converge, the network is not learning. A bug somewhere - most likely data normalization issue</li> <li>Fix data normalization issue when during validation the transforms were not being applied</li> <li>Train first <code>UNet</code> model with <code>ResNet-50</code> encoder for 10 epochs successfully - final <code>val/dice</code> score was 0.782</li> <li>TODO: Validation is slow - instead of logging figures per sample each epoch log a grid of targets vs predictions</li> <li>WIP. inference script - the datamodule needs refactoring, the user should be able to crate it either from metadata   file or from list of file paths</li> </ul>"},{"location":"dev-log/#2023-12-15","title":"2023-12-15","text":"<ul> <li>Add factory methods to <code>KelpForestDataModule</code></li> <li><code>TrainConfig</code> now has dedicated properties for data module, model and trainer kwargs</li> <li>Prediction script works, preds look ok</li> <li>Tried to install <code>lightning-bolts</code> for torch ORT support - PL ends up being downgraded since bolts require it to be   less than 2.0</li> <li>Needed to bring the images to original shape because of padding necessary by unet -&gt; hacked ugly solution to remove   the padding</li> <li>Training a few more models - looks like seed is not respected and each model ends up having different training curves   and final performance</li> <li>PL way of logging metrics results in <code>epoch</code> being treated as a step in visualizations of learning curves in MLFlow   UI - a bit annoying</li> </ul>"},{"location":"dev-log/#2023-12-16","title":"2023-12-16","text":"<ul> <li>Add submission creation script</li> <li>First submission using unet/resnet50 combination trained for 10 epochs - score 0.3861 - let's train for longer</li> <li>Need to implement a few makefile commands for training and prediction</li> <li>Hmmm, maybe run hyperparameter search on Azure ML?</li> <li>Trying to install <code>torch-ort</code> for training speedups (docs say about 37% speedups)</li> <li>No speedups at all - some new package messed up logging, debug statements all over the place...</li> <li><code>torch.compile</code> is the same - no speedups, takes forever to compile the model using <code>mode</code> != <code>default</code>   (which too is painfully slow)</li> <li>Reverting the env changes</li> <li>Run 10-fold CV and re-trained model, new submission score using lightning's best checkpoint = 0.6569, using   <code>mlflow</code> model 0.6338 WTF!?</li> <li><code>mlflow</code> must have saved last checkpoint instead of the best one... Need to fix that</li> <li><code>MLFlowLogger</code> now uses <code>log_model=True</code>, instead of <code>log_model=\"all\"</code> - final logged model is the same as the   best one even if the latest epoch resulted in worse model</li> <li>Figured out that <code>--benchmark</code> resulted in difference in non-deterministic model performance, will not use it again   for reproducibility</li> <li>Tried training from scratch - very slow convergence, training using pre-trained model is a must in this case.   Final DICE after 10 epochs=0.736, compared to 0.760 with <code>imagenet</code> weights</li> <li>Removing NDVI -&gt; dice=0.758, keep NDVI</li> <li>Adding <code>decoder_attention_type=\"scse\"</code> did not improve the performance (dice=0.755)</li> <li>Reorder channels into R,G,B,SWIR,NIR,QA,DEM,NDVI -&gt; bump performance to dice=0.762</li> <li>WIP. <code>OneCycleLR</code></li> </ul>"},{"location":"dev-log/#2023-12-17","title":"2023-12-17","text":"<ul> <li>Trying out different hyperparameter combinations</li> <li>OneCycleLR vs no LR scheduler: 0.76640 vs 0.764593 but overall stability is better with 1Cycle</li> <li>Adam vs AdamW +0.02 for AdamW</li> <li><code>weight_decay</code>:<ul> <li>1e-2: 0.759</li> <li>1e-3: 0.763</li> <li>1e-4: 0.765</li> <li>1e-5: 0.762</li> </ul> </li> <li>10-fold CV splits:<ul> <li>split 0: 0.764593 - public score: 0.6551</li> <li>split 1: 0.831507 - public score: 0.6491</li> <li>split 2: 0.804093 - public score: 0.6608</li> <li>split 3: 0.820495 - public score: 0.6637</li> <li>split 4: 0.815217 - public score: 0.6529</li> <li>split 5: 0.825403 - public score: 0.6653</li> <li>split 6: 0.815222 - public score: 0.6507</li> <li>split 7: 0.823355 - public score: 0.6626</li> <li>split 8: 0.829409 - public score: 0.6411</li> <li>split 9: 0.820984 - public score: 0.6506</li> </ul> </li> <li>Add confusion matrix logging</li> <li>Use <code>mlflow</code> autogenerated run names</li> </ul>"},{"location":"dev-log/#2023-12-18","title":"2023-12-18","text":"<ul> <li>Implementing image grid logging during eval loop</li> <li>Plot new submission scores</li> <li>Allow experiment name to be passed from command line</li> </ul>"},{"location":"dev-log/#2023-12-19","title":"2023-12-19","text":"<ul> <li>Submit more CV split models</li> <li>Finish up plotting image batch</li> <li>Add NDVI grid</li> <li>Fix reordered bands plotting issue</li> <li>Remove IoU from training metrics</li> <li>Used <code>digiKam</code> to find if duplicates are indeed present in the data using DEM layer - yup, and a lot of them</li> <li>Need to group duplicates and redesign CV splits</li> <li>WIP. AOI grouping</li> </ul>"},{"location":"dev-log/#2023-12-20","title":"2023-12-20","text":"<ul> <li>There is a single mask in the mask grid that's 90% kelp - cloudy image, open water - this can skew the results,   need to verify the rest of the masks</li> <li>Did some more work on AOI resolution</li> </ul>"},{"location":"dev-log/#2023-12-21","title":"2023-12-21","text":"<ul> <li>Implemented AOI grouping logic after grouping we have 3K of AOIs</li> <li>Added some more commands to Makefile</li> <li>WIP. More plotting logic to EDA - need to incorporate AOI ID column into EDA</li> <li>Maybe split EDA into 2 parts - composite plotting, stats plotting</li> </ul>"},{"location":"dev-log/#2023-12-22","title":"2023-12-22","text":"<ul> <li>Split EDA into tile plotting and stats plotting</li> <li>AOI resolution should be done after tile plotting and before EDA</li> <li>Updated makefile commands</li> <li>Renamed <code>preprocessing.py</code> to <code>calculate_band_stats.py</code></li> <li>Added few new stats calculation</li> <li>Updated train-val-test split logic using deduplicated AOIs</li> <li>Trained new models using new dataset:<ul> <li>split 0: 0.820488 - public score: 0.6648</li> <li>split 1: 0.818475 - public score: 0.6583</li> <li>split 2: 0.819387 - public score: ****</li> <li>split 3: 0.837715 - public score: 0.6566</li> <li>split 4: 0.828322 - public score: ****</li> <li>split 5: 0.829196 - public score: ****</li> <li>split 6: 0.832407 - public score: 0.6678</li> <li>split 7: 0.848665 - public score: 0.6663</li> <li>split 8: 0.823535 - public score: ****</li> <li>split 9: 0.832882 - public score: ****</li> </ul> </li> <li>There is too much splits to check each time... Will use split #6 to train from now on</li> </ul>"},{"location":"dev-log/#2023-12-23","title":"2023-12-23","text":"<ul> <li>Checking out different normalization strategies</li> <li>Move stats computation to GPU if available - 45x speedup</li> <li>Commented out some indices, as they result in nan/inf values</li> <li>Trained new models on split=6 with different normalization strategies:<ul> <li><code>z-score</code>: 0.834168</li> <li><code>quantile</code>: 0.834134</li> <li><code>min-max</code>: 0.831865</li> <li><code>per-sample-quantile</code>: 0.806227</li> <li><code>per-sample-min-max</code>: 0.801893</li> </ul> </li> <li>Will use <code>quantile</code> since it produces the most appealing visual samples and is more robust for outliers,   the learning curve also seems to converge faster</li> </ul>"},{"location":"dev-log/#2023-12-24","title":"2023-12-24","text":"<ul> <li>Fixed smp losses - <code>mode</code> param must be \"multiclass\" since our predictions have shape <code>NxCxHxW</code></li> <li>Some results from best to worst:</li> </ul> Loss class specs val/dice <code>torch.nn.CrossEntropyLoss</code> (<code>weight=[0.4,0.6]</code>) 0.840475 <code>smp.losses.DiceLoss</code> 0.840451 <code>smp.losses.JaccardLoss</code> 0.839553 <code>smp.losses.TverskyLoss</code> 0.839455 <code>torch.nn.CrossEntropyLoss</code> (<code>weight=[0.3,0.7]</code>) 0.83682 <code>torch.nn.CrossEntropyLoss</code> (<code>weight=None</code>) 0.834134 <code>smp.losses.SoftCrossEntropyLoss</code> (<code>smooth_factor=0.1</code>)) 0.833259 <code>smp.losses.FocalLoss</code> 0.832160 <code>smp.losses.LovaszLoss</code> 0.82262 <code>smp.losses.SoftCrossEntropyLoss</code> (<code>smooth_factor=0.2</code>) 0.83195 <code>smp.losses.SoftCrossEntropyLoss</code> (<code>smooth_factor=0.3</code>) 0.83040 <code>torch.nn.CrossEntropyLoss</code> (<code>weight=[0.1,0.9]</code>) 0.80692 <ul> <li>Removed <code>smp.losses.MCCLoss</code> and <code>smp.losses.SoftBCEWithLogitsLoss</code> since they require different input shapes -   have no time for resolving this behaviour - error with <code>requires_grad</code> not being enabled or something...</li> <li>Will use Dice since it performed better on public leaderboard 0.6824 vs 0.6802 (CE with weights)</li> </ul>"},{"location":"dev-log/#2023-12-25","title":"2023-12-25","text":"<ul> <li>Comparing weighted sampler results with different weights:<ul> <li>9600 samples per epoch (300 batches - 2x as many as without the sampler)</li> <li>5120 (160 batches - as many as without the sampler)</li> <li>Ran 84 experiments with 5120 samples/epoch for 10 epochs, but not with all combinations of weights that I   wanted...</li> <li>Did not run 9600 samples / epoch at all</li> <li>Need to scale the experiments to the cloud</li> <li>1 experiment takes ~10 min, running full hparam search for weights would take ~8 days</li> </ul> </li> <li>Top 5 local runs:</li> </ul> samples_per_epoch has_kelp kelp_pixels_pct qa_ok qa_corrupted_pixels_pct almost_all_water dem_nan_pixels_pct dem_zero_pixels_pct val/dice 5120 1.0 1.0 1.0 -1.0 0.0 -1.0 -1.0 0.840818 5120 2.0 1.0 0.5 1.0 -1.0 -1.0 -1.0 0.840141 5120 1.0 1.0 1.0 -1.0 0.0 -1.0 -1.0 0.832289 5120 1.0 1.0 1.0 -1.0 1.0 1.0 -1.0 0.832120 5120 1.0 1.0 1.0 -1.0 1.0 0.0 0.0 0.831936 5120 1.0 1.0 1.0 0.0 1.0 -1.0 -1.0 0.831921 5120 1.0 1.0 1.0 -1.0 0.0 0.0 0.0 0.831608 <ul> <li>Just realised, that I was running the experiments on wrong CV split</li> <li>First 2 runs are on CV Split #6, the rest are on #0</li> <li>So dumb...</li> </ul>"},{"location":"dev-log/#2023-12-26","title":"2023-12-26","text":"<ul> <li>WIP. Azure ML integration</li> <li>Env issues - no GPU detected</li> </ul>"},{"location":"dev-log/#2023-12-27","title":"2023-12-27","text":"<ul> <li>WIP. Azure ML integration</li> <li>Finally, GPU was detected - had to recreate the env from scratch using Azure's curated base env</li> <li>Installing dependencies via pip... Well, fuck the lock-files I guess \u00af_(\u30c4)_/\u00af</li> <li>Training takes forever WTF M$???</li> <li>Alright, downloading dataset instead of using ro_mount fixed slow training</li> <li>Fixed a few issues with confusion matrix logging</li> <li>Fixed double logging</li> <li>Added temporary fix for DEBUG level logging being permanently set by some 3rd party package</li> <li>Will run few hundred experiments overnight</li> </ul>"},{"location":"dev-log/#2023-12-28","title":"2023-12-28","text":"<ul> <li>Some results after whole night of training:</li> </ul> samples_per_epoch has_kelp kelp_pixels_pct qa_ok qa_corrupted_pixels_pct almost_all_water dem_nan_pixels_pct dem_zero_pixels_pct val/dice 5120 2 0.5 0.5 -0.5 -1 0 -0.25 0.84405 5120 0.2 0 -0.5 -0.5 -0.5 0.25 0.5 0.84402 5120 3 0.5 -1 0 -1 0.25 0 0.84396 5120 3 0.2 0 -1 0.5 0.25 -1 0.84396 5120 2 0 0.5 0 0.25 0 -0.5 0.84391 5120 0.5 0.2 -0.5 0.75 0.5 -1 -0.25 0.84390 5120 3 0.2 0.5 -0.25 0.75 -0.25 0.5 0.84382 5120 3 0.5 1 -0.25 0.5 -0.5 0 0.84382 5120 3 2 -0.25 -0.5 -0.5 -1 0.75 0.84380 5120 2 1 0.25 -1 0.75 0.75 1 0.84377 5120 2 0 -0.5 0 -1 -1 -1 0.84374 5120 0.5 0 -1 -0.25 0.25 -0.25 -0.5 0.84373 5120 0.2 0 0 0.25 -1 0.25 0.5 0.84370 5120 2 0.5 0.25 -0.5 0.25 0.5 -0.5 0.84369 <ul> <li>After retraining using 10240 samples per epoch:</li> </ul> samples_per_epoch has_kelp kelp_pixels_pct qa_ok qa_corrupted_pixels_pct almost_all_water dem_nan_pixels_pct dem_zero_pixels_pct val/dice 10240 2 0.5 0.5 -0.5 -1 0 -0.25 0.84459 10240 0.2 0 -0.5 -0.5 -0.5 0.25 0.5 0.84456 10240 3 0.5 -1 0 -1 0.25 0 0.84501 10240 3 0.2 0 -1 0.5 0.25 -1 0.84801 10240 2 0 0.5 0 0.25 0 -0.5 0.84641 10240 0.5 0.2 -0.5 0.75 0.5 -1 -0.25 0.84622 10240 3 0.2 0.5 -0.25 0.75 -0.25 0.5 0.84546 10240 3 0.5 1 -0.25 0.5 -0.5 0 0.84619 10240 3 2 -0.25 -0.5 -0.5 -1 0.75 0.84500 10240 2 1 0.25 -1 0.75 0.75 1 0.84508 10240 2 0 -0.5 0 -1 -1 -1 0.84430 10240 0.5 0 -1 -0.25 0.25 -0.25 -0.5 0.84496 10240 0.2 0 0 0.25 -1 0.25 0.5 0.84522 10240 2 0.5 0.25 -0.5 0.25 0.5 -0.5 0.84538 <ul> <li>Using more samples over the basic configuration yields very small increase 0.84405 vs 0.84801. But will use those   weights for the future.</li> </ul>"},{"location":"dev-log/#2023-12-29","title":"2023-12-29","text":"<ul> <li>Working on verifying impact of adding different spectral indices to the input</li> <li>Added new indices</li> <li>Refactored all indices into Kornia compatible classes</li> </ul>"},{"location":"dev-log/#2023-12-30","title":"2023-12-30","text":"<ul> <li>Added new aquatic vegetation indices</li> <li>Re-calculate band stats</li> <li>Added spectral indices plotting logic to the dataset</li> <li>Tried again to add <code>decoder_attention_type=\"scse\"</code> but it gives worse performance</li> <li>DEMWM and NDVI are now always appended to the spectral_indices list</li> <li>Added option to mask spectral indices using DEMWM - needs testing</li> </ul>"},{"location":"dev-log/#2023-12-31","title":"2023-12-31","text":"<ul> <li>Enabled training using on-the-fly masking of indices using QA and DEM Water Mask</li> <li>Masking land and corrupted pixels in indices bumps the performance by over 1-2%</li> <li>Zeroes in the main bands (the ones with -32k pixel values) make the indices incorrect - maybe use NaNs and substitute   them with band min value instead?</li> <li>Changed indices back to inheriting from <code>torch.nn.Module</code> almost 1.6x speedup for stats calculation</li> <li>Recalculate dataset stats</li> <li>Added support for specifying fill value for \"missing\" pixels - either <code>torch.nan</code> or <code>0.0</code> for both stats calculation   and model training</li> <li>Load datasets stats from file instead of keeping them in the code</li> <li>Best combination so far:<ul> <li>CDOM,DOC,WATERCOLOR,SABI,KIVU,Kab1,NDAVI,WAVI</li> <li>WM masking</li> <li>without using <code>torch.nan</code> for missing pixels -&gt; using 0.0 instead</li> <li>with <code>2023-12-31T20:37:17-stats-fill_value=0.0-mask_using_qa=False-mask_using_water_mask=False-modified.json</code>   stats</li> <li>val/dice = 0.8477</li> </ul> </li> <li>WIP. Azure ML hparam search pipeline for best combination of spectral indices</li> </ul>"},{"location":"dev-log/#2024-01-01","title":"2024-01-01","text":"<ul> <li>WIP. Azure ML hparam search pipeline for best combination of spectral indices</li> <li>Removing <code>resources:shm_size</code> section from the pipeline spec results in workers dying from OOM errors</li> <li>Running 1k experiments with different spectral index combinations using zeros to mask missing pixels</li> </ul>"},{"location":"dev-log/#2024-01-02","title":"2024-01-02","text":"<ul> <li>Fixed issue with missing pixels not being filled</li> <li>Re-trained locally best models from Azure ML hparam search runs:</li> </ul> run_id stats_fp fill_val steps_per_epoch spectral_indices val/dice leaderboard ff896e93496344c2903a69fbf94f14fa nan-adjusted nan 10240 CI,CYA,ClGreen,IPVI,KIVU,NormNIR,SABI,mCRIG 0.85234 0.7034 3298cf9aad3845a1ad0517e6bcca2c85 nan-adjusted nan 10240 AFRI1600,ATSAVI,AVI,CHLA,GDVI,LogR,NormR,SRNIRR 0.85211 0.7045 072d8f5e55e941ea82242301a1c3a1d5 nan-adjusted nan 10240 BWDRVI,CI,ClGreen,GVMI,I,MCARI,SRNIRSWIR,WAVI 0.85199 0.7044 9b98c0ecd4554947bb23341cd4ae0191 nan-adjusted nan 10240 ARVI,AVI,CDOM,CI,GARI,I,SRNIRSWIR,mCRIG 0.85191 0.7049 f67b7cfc2faa449c9cef2d3ace98a15c nan-adjusted nan 10240 AVI,DOC,IPVI,Kab1,LogR,NDWIWM,NormR,SRGR 0.85133 0.7011 faf96942e21f4fa9b11b55287f4fb575 zero-adjusted 0.0 10240 AVI,CDOM,GBNDVI,PNDVI,SABI,SRGR,TVI,WDRVI 0.85131 0.7062 4ccf406b8fec4793aabfd986fd417d26 nan-adjusted nan 10240 AVI,I,Kab1,NDWIWM,NormNIR,SRNIRR,WDRVI,mCRIG 0.85115 0.7033 cc8d8af285474a9899e38f17f7397603 nan-adjusted nan 10240 AFRI1600,EVI22,MSAVI,NLI,NormR,RBNDVI,SRGR,TURB 0.85094 0.7025 7063fb98bc4e4cb1bfb33e67e1ee10de nan-adjusted nan 10240 ATSAVI,CDOM,CI,ClGreen,GVMI,I,MCARI,MVI 0.85051 0.7060 <ul> <li>Need to add eval script for those AML models, cannot re-train them each time or waste submissions...</li> <li>Best models from AML were not better than what I had trained locally - best model had dice=0.7019</li> <li>Will resubmit locally trained models tomorrow</li> <li>It seems that pixel masking with nans and using adjusted quantile normalization is going to work best</li> </ul>"},{"location":"dev-log/#2024-01-03","title":"2024-01-03","text":"<ul> <li>New submissions (see table above)</li> <li>Will submit a few more runs tomorrow, but it looks like I'm going to use AFRI1600, ATSAVI, AVI, CHLA, GDVI, LogR,   NormR, SRNIRR index combination for the future.</li> <li>Added evaluation script for AML models</li> <li>Evaluated all top models trained on Azure ML - best one had dice=0.85015</li> </ul>"},{"location":"dev-log/#2024-01-04","title":"2024-01-04","text":"<ul> <li>Added a few more submissions - best score 0.7062</li> <li>Working on Test Time Augmentations</li> </ul>"},{"location":"dev-log/#2024-01-05","title":"2024-01-05","text":"<ul> <li>Added a few more submissions - best score 0.7060 -</li> <li>Run another hparam search using 15 indices at once (previous was using max 8 indices)</li> </ul>"},{"location":"dev-log/#2024-01-06","title":"2024-01-06","text":"run_id stats_fp fill_val steps_per_epoch spectral_indices val/dice (AML) val/dice (local) leaderboard a9ea38cb5cf144c28b85cef99fbf0fc3 nan-adjusted nan 10240 ATSAVI,AVI,CI,ClGreen,GBNDVI,GVMI,IPVI,KIVU,MCARI,MVI,NormNIR,PNDVI,SABI,WDRVI,mCRIG N/A 0.85339 0.7083 e5560bce41ac48eaa9bdd5ea4fbb5ab5 nan-adjusted nan 10240 BWDRVI,GARI,H,I,MVI,NDAVI,NDWI,NLI,NormG,SRGR,SRNIRR,SRSWIRNIR,VARIGreen,WATERCOLOR,mCRIG 0.85303 0.85304 0.7064 3ab0ade31670498bbf2dd2368b485b60 nan-adjusted nan 10240 ARVI,BWDRVI,CYA,DVIMSS,EVI,GNDVI,H,I,KIVU,MCARI,MVI,NormG,NormNIR,SRNIRSWIR,TVI 0.85298 0.85123 0.7048 2654497d84bf466cb5508369bd83ce24 nan-adjusted nan 10240 AFRI1600,AVI,CHLA,ClGreen,H,IPVI,LogR,MVI,PNDVI,SQRTNIRR,SRGR,SRNIRG,SRNIRSWIR,WATERCOLOR,WAVI 0.85316 0.85316 0.7072 ec3d3613a9d04b1b81b934231360aebe nan-adjusted nan 10240 ARVI,AVI,CDOM,CI,CYA,EVI22,GBNDVI,GRNDVI,H,I,LogR,NormG,NormNIR,NormR,WDRVI 0.85299 0.85120 0.7028 834d204b70c645c2949b01adb1cdffef nan-adjusted nan 10240 ATSAVI,CHLA,CI,CVI,EVI2,GDVI,GRNDVI,H,I,NDWI,NormNIR,PNDVI,SABI,TURB,WATERCOLOR 0.85300 0.85300"},{"location":"dev-log/#2024-01-07","title":"2024-01-07","text":"<ul> <li>Added submissions log to the repo</li> <li>Resolve artifacts dir dynamically - allow raw AML export as input to eval script, log model after eval</li> <li>Updated Makefile to include the best combination of spectral indices</li> <li>Added TTA (baseline val/dice=0.85339):<ul> <li>max: 0.85490</li> <li>mean: 0.85458</li> <li>sum: 0.85458</li> <li>min: 0.85403</li> <li>gmean: 0.15955</li> <li>tsharpen: 0.00468 - loss was nan</li> </ul> </li> </ul>"},{"location":"dev-log/#2024-01-08","title":"2024-01-08","text":"<ul> <li>Submitted new preds with TTA:<ul> <li>no-tta: 0.7083</li> <li>max: 0.7076</li> <li>mean: 0.7073</li> </ul> </li> <li>So... that was a waste of time...</li> <li>Prediction threshold optimization</li> </ul> threshold test/dice 0.3 0.85301 0.31 0.85305 0.32 0.85306 0.33 0.85307 0.34 0.85309 0.35 0.85314 0.36 0.85314 0.37 0.85313 0.38 0.85315 0.39 0.85317 0.4 0.85316 0.41 0.85317 0.42 0.85317 0.43 0.85320 0.44 0.85319 0.45 0.85320 0.46 0.85318 0.47 0.85314 0.48 0.85316 0.49 0.85317 0.5 0.85316 0.51 0.85315 0.52 0.85314 0.53 0.85313 0.54 0.85311 0.55 0.85309 0.56 0.85305 0.57 0.85303 0.58 0.85300 0.59 0.85300 0.6 0.85296 0.61 0.85296 0.62 0.85290 0.63 0.85287 0.64 0.85285 0.65 0.85281 0.66 0.85278 0.67 0.85274 0.68 0.85267 0.69 0.85259 0.7 0.85253 <ul> <li>Leaderboard: 0.7077</li> </ul>"},{"location":"dev-log/#2024-01-09","title":"2024-01-09","text":"<ul> <li>Added new model architectures based on https://github.com/jlcsilva/segmentation_models.pytorch</li> </ul>"},{"location":"dev-log/#2024-01-10","title":"2024-01-10","text":"<ul> <li>Added hparam search AML pipeline</li> <li>Running a few runs overnight</li> </ul>"},{"location":"dev-log/#2024-01-11","title":"2024-01-11","text":"<ul> <li>Results after overnight training (Top 5 runs only):</li> </ul> encoder architecture val/dice tu-efficientnet_b5 unet 0.85854 tu-seresnext101_32x4d unet 0.85807 tu-resnest50d_4s2x40d unet 0.85787 tu-rexnetr_300 unet 0.85749 tu-seresnext26d_32x4d unet 0.85728 <ul> <li>Observation: bigger models = better</li> <li>TTA on test set worked for some models and failed for the others</li> <li>A lot of models failed due to lack of pre-trained weight - need to investigate more... are the docs lying?</li> <li>Very large models failed with OOM errors - neet to retrain with lower batch size + gradient accumulation</li> <li>Some model failed because of bad tensor shapes - probably those models require inputs to be divisible   by 128 or something</li> <li>Looks like checkpoint saving using MLFlow is broken and instead of saving the best model the latest one is saved...</li> <li>Added a workaround for this - always load model from checkpoints dir if exists</li> <li>New submissions:</li> </ul> encoder architecture val/dice leaderboard notes tu-efficientnet_b5 unet 0.85920 0.7119 trained on AML + bf16-mixed + dt=0.45 + fixed checkpoint restore tu-efficientnet_b5 unet 0.85854 0.7105 trained on AML + fp32-true tu-efficientnet_b5 unet 0.85817 0.7101 trained locally"},{"location":"dev-log/#2024-01-12","title":"2024-01-12","text":"<ul> <li>Update encoder list in hparam search pipeline</li> <li>Add option to pass <code>val_check_interval</code></li> <li><code>image_size=384</code> + <code>batch_size=16</code> + <code>accumulate_grad_batches=2</code></li> <li>Force training with random init when no weights exist</li> </ul>"},{"location":"dev-log/#2024-01-13","title":"2024-01-13","text":"<ul> <li>Fixed issue with logging images with the same key twice when using sub-1 <code>val_check_interval</code></li> </ul>"},{"location":"dev-log/#2024-01-16","title":"2024-01-16","text":"<ul> <li>Added support for different LR Schedulers</li> <li>New submissions - no breakthroughs</li> <li>Training with <code>batch_size=8</code> and <code>accumulate_grad_batches=4</code> resulted in better local eval scores,   but did not improve leaderboard scores</li> <li>Tried out different resize strategies - padding works best so far</li> <li>Some encoder models require input to be both divisible by 32, 7, 8 etc. - I cannot use the same image size</li> <li>ConvNext not supported</li> </ul>"},{"location":"dev-log/#2024-01-17","title":"2024-01-17","text":"<ul> <li>Added new params to AML Model training pipeline</li> <li>Added CV split training pipeline</li> <li>Fix with param names</li> </ul>"},{"location":"dev-log/#2024-01-18","title":"2024-01-18","text":"<ul> <li>Resolve issue where sending over model.parameters() to external function caused divergence in model performance</li> <li>Update defaults in argparse</li> <li>New submissions. Best: 0.7132 - fold #7 + bf16 + dt=0.45</li> </ul>"},{"location":"dev-log/#2024-01-19","title":"2024-01-19","text":"<ul> <li>Fixed import issue with <code>AdamW</code> being imported from <code>timm</code> instead of <code>torch</code></li> <li>Updated model hparam search pipeline - train with smaller batch size, remove all models that do not support   image_size = 352 or 384</li> <li>New submissions. Best -&gt; 0.7139 - fold #3 + bf16 + dt=0.45</li> </ul>"},{"location":"dev-log/#2024-01-20","title":"2024-01-20","text":"<ul> <li>Updated model training component. <code>arg=${{inputs.arg}}</code> results in default value being passed.   Use <code>arg ${{inputs.arg}}</code> instead.</li> <li>New submissions using different CV folds. No improvement.</li> </ul>"},{"location":"dev-log/#2024-01-21","title":"2024-01-21","text":"<ul> <li>New submissions. Best -&gt; 0.7152 - fold #8 + bf16 + dt=0.45</li> <li>Tested a few folds with different dt, precision and tta. Using just bf16 without tta or dt yields best results.   Will try it next.</li> <li>Running experiments with different architectures and best performing encoders.</li> <li>ResUnet++ often results in <code>NaN</code> loss.</li> <li>Added guard against <code>NaN</code> loss</li> <li>Fixed some typos, minor refactor in eval scripts</li> <li>Validate encoder config with - modify <code>image_size</code> if model does not support the one specified by the user</li> <li>Removed useless <code>--strategy</code> argument since the images are not RGB - most of the weights are   randomly initialized anyway</li> </ul>"},{"location":"dev-log/#2024-01-24","title":"2024-01-24","text":"<ul> <li>Re-trained best model with unet++ architecture</li> <li>Unet++ is not deterministic for some reason...</li> <li>Different results each time the training is run</li> <li>Tried to submit preds with a model that had the best looking confusion matrix instead (checkpoint with DICE=0.829)   vs best checkpoint (DICE=0.846). No improvement -&gt; public dice=0.7055</li> <li>Add FCN - model collapses</li> <li>Try out a bunch of <code>--benchmark</code> experiments</li> </ul>"},{"location":"dev-log/#2024-01-25","title":"2024-01-25","text":"<ul> <li>WIP. prediction averaging</li> <li>Used all folds for averaging - might not be optimal -&gt; best score so far: dice=0.7170</li> </ul>"},{"location":"dev-log/#2024-01-26","title":"2024-01-26","text":"<ul> <li>Added option to ignore folds if weight=0.0</li> <li>Added option to automatically plot submission samples after prediction using   <code>predict_and_submit.py</code> and <code>average_predictions.py</code></li> <li>New submission:<ul> <li>Top 3 folds on Public Leaderboard, DT=0.5: no improvement</li> <li>Top 3 folds on Public Leaderboard, DT=0.1: no improvement</li> <li>Top 6 folds on Public Leaderboard, DT=0.5: no improvement</li> </ul> </li> </ul>"},{"location":"dev-log/#2024-01-27","title":"2024-01-27","text":"<ul> <li>Training on random split with 98% of data in train set</li> <li>Best submission dice=0.7090 bf16 + dt=0.45</li> <li>Did not work very well compared to fold=8</li> <li>WIP. training for longer</li> <li>Added data prep script for pixel level training</li> <li>Added training script for Random Forest and Gradient Boosting Trees classifiers from <code>scikit-learn</code></li> <li>Trained on 0.05% of data -&gt; dice=0.673</li> <li>Forgot to remove samples with corrupted masks...</li> </ul>"},{"location":"dev-log/#2024-01-28","title":"2024-01-28","text":"<ul> <li>Log more plots and metrics during evaluation</li> <li>Fixing dataset</li> <li>Add support for xgboost, lightgbm and catboost</li> <li>Log sample predictions after training</li> <li>Training for longer results (50 epochs):</li> </ul> split 10 epochs exp 50 epochs exp 10 epoch dice 50 epoch dice 0 serene_nose strong_door 0.84391 0.84374 1 teal_pea keen_evening 0.85083 0.84871 2 jovial_neck hungry_loquat 0.84419 0.84723 3 joyful_chain elated_atemoya 0.85997 0.86425 4 brave_ticket nice_cheetah 0.85506 0.85540 5 willing_pin gentle_stamp 0.84931 0.85120 6 boring_eye dev_kelp_training_exp_67 0.85854 0.85985 7 strong_star dev_kelp_training_exp_65 0.86937 0.87241 8 sleepy_feijoa yellow_evening 0.84312 0.84425 9 sincere_pear icy_market 0.85242 0.85168 <ul> <li>Best score on public LB: 0.7169 - using val/dice scores as weights + dt=0.5</li> </ul>"},{"location":"dev-log/#2024-02-01","title":"2024-02-01","text":"<ul> <li>Fiddling with <code>xgboost</code> and <code>catboost</code></li> <li>Add predict and submit scripts for tree based models, need to refactor the code for NN stuff later to match</li> <li>Issues with predictions after moving stuff to different modules... need to debug</li> </ul>"},{"location":"dev-log/#2024-02-02","title":"2024-02-02","text":"<ul> <li>Fixed issues with empty predictions - train config <code>spectral_indices</code> resolution logic was the source of issues</li> <li>Added eval script</li> <li>Removed models other than <code>XGBClassifier</code></li> <li>New submission with XGB - dice = 0.5 on public LB - abandoning this approach</li> <li>Going back to NNs</li> </ul>"},{"location":"dev-log/#2024-02-03","title":"2024-02-03","text":"<ul> <li>Allow for training with reduced number of bands</li> <li>Refactor training stuff a bit (split <code>train.py</code> into smaller files)</li> <li>Use band names instead of indices for band order</li> <li>Add option to specify interpolation mode</li> <li>Update training pipelines</li> </ul>"},{"location":"dev-log/#2024-02-04","title":"2024-02-04","text":"<ul> <li>Add mask post predict resize transforms with accordance with predict transforms</li> <li>Testing out a few transformer-based encoders - did not work - they do not support features only mode</li> <li>A few new submissions with single model - retrained with:   <pre><code>--has_kelp_importance_factor 2\n--kelp_pixels_pct_importance_factor 0.5\n--qa_ok_importance_factor 0.5\n--qa_corrupted_pixels_pct_importance_factor -0.5\n--almost_all_water_importance_factor -1\n--dem_nan_pixels_pct_importance_factor 0\n--dem_zero_pixels_pct_importance_factor -0.25\n</code></pre></li> <li>Single model dice score in public LB improved from 0.7153 -&gt; 0.7155</li> </ul>"},{"location":"dev-log/#2024-02-05","title":"2024-02-05","text":"<ul> <li>A few new submissions - no change in public LB - best one is 0.7170</li> </ul>"},{"location":"dev-log/#2024-02-06","title":"2024-02-06","text":"<ul> <li>Just found a bug in Makefile... Was submitting stuff based on predictions from the first model ensemble...</li> <li>Last two model changes were never submitted - the one with training for 50 epochs and the one with updated sampler weights...</li> <li>Fuck my life...</li> <li>New scores are:<ul> <li>Training for 50 epochs, all weights = 1.0: 0.7197</li> <li>Training for 50 epochs, weights rescaled based on scores on historical LB using min-max scaler: 0.7200</li> <li>New sampler weights: 0.7169</li> </ul> </li> <li>Will build new ensemble of all best models for all splits, regardless of training method</li> <li>New ideas: model weight averaging, weighting probabilities instead of decisions as it is now</li> </ul>"},{"location":"dev-log/#2024-02-07","title":"2024-02-07","text":"<ul> <li>New submissions with a mixture of models - selected best model for each split that was ever produced: 0.7204</li> <li>Best dice was for 8 models in total - fold=1 and fold=9 weights were set to 0.0 as they are the worst on LB individually</li> <li>Add support for soft labels - I can now use weighted average of probabilities instead of hard labels</li> </ul>"},{"location":"dev-log/#2024-02-08","title":"2024-02-08","text":"<ul> <li>Submissions with soft labels - public LB: 0.7206 not much but is something...</li> <li>Need to add some validation for the ensemble scores locally - otherwise the submissions are just wasted</li> <li>New idea: SAHI -&gt; train on 128x128 crops, inference on sliding window with overlaps and padding, then stitch the preds</li> </ul>"},{"location":"dev-log/#2024-02-10","title":"2024-02-10","text":"<ul> <li>Added logic to copy split val files to individual dirs</li> <li>Added eval scripts for evaluating from folders with masks - evaluating ensemble is now easier</li> <li>Tested ensemble with various settings: TTA, decision thresholds, soft labels, tta merge modes, various fold weights</li> <li>Best combination:<ul> <li>no TTA</li> <li>soft labels</li> <li>no decision threshold on fold level</li> <li>decision threshold = 0.48 for final ensemble</li> <li>weights as before -&gt; MinMaxScaler(0.2, 1.0)</li> <li>fold=0 -&gt; 0.0 fold=1 -&gt; 0.0, fold=9 -&gt; 0.0</li> <li>mix of best model per split</li> </ul> </li> <li>Public LB = 0.7208</li> </ul>"},{"location":"dev-log/#2024-02-12","title":"2024-02-12","text":"<ul> <li>Added Stochastic Weight Averaging</li> <li>Running new experiments for 25-50 epochs with SWA kicking in at 75% of epochs</li> </ul>"},{"location":"dev-log/#2024-02-13","title":"2024-02-13","text":"<ul> <li>Generated dataset for SAHI training</li> <li>New submissions - no improvement with SWA</li> </ul>"},{"location":"dev-log/#2024-02-14","title":"2024-02-14","text":"<ul> <li>SAHI training - split=8, image_size=128, no overlap between tiles - locally has DICE=0.83 which is disappointing</li> <li>XEDice loss - no improvement</li> <li>Add option to predict using latest checkpoint instead of the best one</li> <li>Train Unet with reduced encoder depth from 5 to 4. Minor improvements. Will investigate further.</li> </ul>"},{"location":"dev-log/#2024-02-15","title":"2024-02-15","text":"<ul> <li>Add support for providing custom <code>encoder_depth</code> and <code>decoder_channels</code></li> <li>Add support for more losses</li> <li>New submissions - no improvement</li> <li>Test out different <code>decoder_channels</code> configurations <code>512,256,128,64,32</code> seems to bump the performance a bit</li> </ul>"},{"location":"dev-log/#2024-02-16","title":"2024-02-16","text":"<ul> <li>WIP. SAHI inference</li> </ul>"},{"location":"dev-log/#2024-02-17","title":"2024-02-17","text":"<ul> <li>Finished SAHI inference scripts</li> <li>Trained a model on 128x128 crops with resize to 320x320, which had the best local score</li> <li>New submission with SAHI on 8th split =&gt; Public LB=0.68</li> <li>Well, that was a waste of 3 days...</li> </ul>"},{"location":"dev-log/#2024-02-18","title":"2024-02-18","text":"<ul> <li>5-Fold CV</li> <li>Re-train 5 Folds</li> </ul>"},{"location":"dev-log/#2024-02-19","title":"2024-02-19","text":"<ul> <li>New submissions:<ul> <li>Fold-0: 0.7135</li> <li>Fold-1: 0.7114</li> <li>Fold-2: 0.7094</li> <li>Fold-3: 0.7133</li> <li>Fold-4: 0.7106</li> </ul> </li> </ul>"},{"location":"dev-log/#2024-02-20","title":"2024-02-20","text":"<ul> <li>New ensemble: mix of 10-Fold and 5-fold models: 0.7205</li> </ul>"},{"location":"dev-log/#2024-02-21","title":"2024-02-21","text":"<ul> <li>Final submission with fold=2...fold=8 all 1.0 weights public LB = 0.7210</li> <li>No submissions left</li> <li>The competition ends today</li> <li>It was a good run regardless of final ranking</li> <li>But oh boy, do I hope everyone in top 5 over-fitted to public test set :D</li> </ul>"},{"location":"dev-log/#2024-02-22","title":"2024-02-22","text":"<ul> <li>Checked the LB today - #2 place</li> <li>So, yeah... indeed some people over-fitted</li> <li>WIP. docs with training guides and docstrings</li> </ul>"},{"location":"dev-log/#2024-02-23","title":"2024-02-23","text":"<ul> <li>Added more docstrings</li> <li>Updated docs</li> <li>WIP. Reproducibility guide</li> </ul>"},{"location":"dev-log/#2024-02-24","title":"2024-02-24","text":"<ul> <li>Added technical report</li> </ul>"},{"location":"dev-log/#2024-02-25","title":"2024-02-25","text":"<ul> <li>Finishing up the technical report</li> </ul>"},{"location":"dev-log/#2024-02-26","title":"2024-02-26","text":"<ul> <li>Minor adjustments</li> <li>New spectral indices visualization figure</li> </ul>"},{"location":"technical-report/","title":"Kelp Wanted: Segmenting Kelp Forests - 2nd place solution","text":"<p>Username: xultaeculcis</p> <p></p> <p>Overhead drone footage of giant kelp canopy. Image Credit: Tom Bell, All Rights Reserved.</p>"},{"location":"technical-report/#introduction","title":"Introduction","text":"<p>In the realm of environmental conservation, the urgency to deploy innovative solutions for monitoring critical ecosystems has never been more pronounced. Among these ecosystems, kelp forests stand out due to their vital role in marine biodiversity and their substantial contribution to global economic value. These underwater habitats, predominantly formed by giant kelp, are foundational to coastal marine ecosystems, supporting thousands of species while also benefiting human industries significantly. However, the sustainability of kelp forests is under threat from climate change, overfishing, and unsustainable harvesting practices. The pressing need for effective monitoring methods to preserve these ecosystems is evident.</p> <p>In response to this challenge, the Kelp Wanted: Segmenting Kelp Forests competition on the DrivenData platform presented an opportunity for machine learning enthusiasts and experts to contribute to the conservation efforts of these vital marine habitats. The competition aimed to leverage the power of machine learning to analyze coastal satellite imagery, enabling the estimation of kelp forests' extent over large areas and over time. This innovative approach marks a significant advancement in the field of environmental monitoring, offering a cost-effective and scalable solution to understand and protect kelp forest dynamics.</p> <p>The challenge required participants to develop algorithms capable of detecting the presence or absence of kelp canopy using Landsat satellite imagery. This binary semantic segmentation task demanded not only technical expertise in machine learning and image processing but also a deep understanding of the environmental context and the data's geospatial nuances.</p> <p>The competition underscored the importance of applying a combination of advanced machine learning techniques and domain-specific knowledge to address environmental challenges. The solution, which secured the 2nd place, leveraged a comprehensive strategy that included the use of pre-trained weights, data preprocessing methods, and a carefully optimized model architecture. By integrating additional spectral indices, adjusting the learning strategy, and employing a robust model ensemble, it was possible to achieve significant accuracy in segmenting kelp forests from satellite imagery.</p> <p>This report details the methodologies and technologies that underpinned the best submissions. From data preparation and feature engineering to model development and validation, outlined are the steps taken to develop a solution proposed approach demonstrates the potential of machine learning to make a meaningful contribution to environmental conservation efforts, paving the way for further research and application in this critical field.</p> <p>The Author extends his deepest gratitude to the organizers of the \"Kelp Wanted: Segmenting Kelp Forests\" competition, including DrivenData, Kelpwatch.org, UMass Boston, and Woods Hole Oceanographic Institution, for their visionary approach in bridging machine learning with environmental conservation. Their dedication to addressing the urgent need for innovative solutions in monitoring kelp forests has not only spotlighted the critical state of these ecosystems but has also paved the way for groundbreaking research and development. The meticulous organization of the competition, the provision of a rich dataset, and the support offered throughout the challenge were instrumental in fostering a collaborative and inspiring environment for all participants. In Author's opinion, this competition has not only contributed significantly to the field of environmental science but has also empowered the machine learning community to apply their skills for a cause that extends far beyond technological advancement, aiming for a profound and positive impact on our planet's future.</p>"},{"location":"technical-report/#tldl-solution-summary","title":"TL;DL - solution summary","text":"<p>This section contains a TL;DR summary for the 2nd place solution.</p>"},{"location":"technical-report/#what-worked","title":"What worked","text":"<ul> <li>Pre-trained weights</li> <li>Reorder channels into R,G,B,SWIR,NIR,QA,DEM</li> <li>AdamW instead of Adam or SGD</li> <li>Weight decay = 1e-4</li> <li>Learning rate = 3e-4</li> <li>Batch size = 32 &lt;- full utilization of Tesla T4</li> <li><code>16-mixed</code> precision training</li> <li><code>bf16-mixed</code> precision inference</li> <li>AOI grouping (removing leakage between CV folds)</li> <li>Quantile normalization</li> <li>Dice Loss</li> <li>Weighted sampler @10240 samples per epoch<ul> <li><code>has_kelp_importance_factor=3.0</code></li> <li><code>kelp_pixels_pct_importance_factor=0.2</code></li> <li><code>qa_ok_importance_factor=0.0</code></li> <li><code>qa_corrupted_pixels_pct_importance_factor=-1.0</code></li> <li><code>almost_all_water_importance_factor=0.5</code></li> <li><code>dem_nan_pixels_pct_importance_factor=0.25</code></li> <li><code>dem_zero_pixels_pct_importance_factor=-1.0</code></li> </ul> </li> <li>Masking indices with QA and DEM Water Mask</li> <li>Extra spectral indices:<ul> <li>DEMWM,</li> <li>NDVI,</li> <li>ATSAVI,</li> <li>AVI,</li> <li>CI,</li> <li>ClGreen,</li> <li>GBNDVI,</li> <li>GVMI,</li> <li>IPVI,</li> <li>KIVU,</li> <li>MCARI,</li> <li>MVI,</li> <li>NormNIR,</li> <li>PNDVI,</li> <li>SABI,</li> <li>WDRVI,</li> <li>mCRIG</li> </ul> </li> <li>UNet + EfficientNet-B5 &lt;- best combo</li> <li>Decision threshold change to 0.45-0.48</li> <li><code>OneCycleLR</code></li> <li>10-fold CV</li> <li>Training for 50 epochs (best model ensemble)</li> <li>Mixing models with best DICE per split in the ensemble</li> <li>Soft labels (second-best model used them)</li> <li>Mixing model architectures and encoders such as:<ul> <li><code>unet</code> &lt;- was the best</li> <li><code>linknet</code></li> <li><code>unet++</code></li> <li><code>tu-efficeintnet_b5</code> &lt;- was the best</li> <li><code>tu-mobilevitv2_150.cvnets_in22k_ft_in1k_384</code></li> <li><code>tu-maxvit_small_tf_384.in1k</code></li> <li><code>tu-seresnextaa101d_32x8d.sw_in12k_ft_in1k</code></li> <li><code>tu-rexnetr_200.sw_in12k_ft_in1k</code></li> <li><code>tu-seresnext26d_32x4d</code></li> <li><code>tu-gcresnet33ts.ra2_in1k</code></li> <li><code>tu-seresnext101_32x4d</code></li> </ul> </li> </ul>"},{"location":"technical-report/#what-did-not-work","title":"What did not work","text":"<ul> <li>Training from scratch</li> <li>Larger or smaller <code>weight_decay</code></li> <li>Larger or smaller <code>lr</code></li> <li><code>decoder_attention_type=\"scse\"</code></li> <li>Losses other than DICE (CE with weighting was close)</li> <li>Compiling the model and <code>torch-ort</code></li> <li>Normalization strategies other than <code>quantile</code> / <code>z-score</code></li> <li>Bunch of different index combinations</li> <li>TTA (for leaderboard)</li> <li>LR Schedulers other than <code>OneCycleLR</code></li> <li>Random split</li> <li>XGBoost and other tree based models predicting on pixel level</li> <li>More decoder channels</li> <li>SAHI</li> <li>Resize strategy different than <code>pad</code></li> <li>Training with larger images</li> <li>Bigger batches</li> <li>More frequent val checks</li> <li>Smaller batch sizes and <code>accumulate_grad_batches</code> &gt; 1</li> <li>Stochastic Weights Averaging (SWA)</li> </ul>"},{"location":"technical-report/#what-was-not-tested","title":"What was not tested","text":"<ul> <li>Prithvi-100M - did not have the time to verify it, also   it was trained on Harmonised Landsat Sentinel 2 (HLS) data - additional data transforms would need to be implemented.</li> <li>The Pretrained Remote Sensing Transformer (Presto) - again no time,   also - it requires additional data and was not directly trained on Landsat. Sentinel 2 was the main data source for   it.</li> <li>Transformer based models such as Swin transformer   or SegFormer - the dev env was already bloated. Including those models would   mean adding MMSegmentation package which has completely different   training configuration than <code>pytorch-lightning</code>.</li> </ul>"},{"location":"technical-report/#most-impactful-decisions","title":"Most impactful decisions","text":"<p>Some of the most impactful decisions are listed in the table below. Please note that this is not a full list. Along the way there were other methods that are not mentioned here.</p> Method Public LB score Score increase Baseline UNet + ResNet-50 encoder 0.6569 N/A + AOI grouping with cosine similarity and more robust CV strategy 0.6648 0.0079 + Dice loss instead of Cross Entropy 0.6824 0.0176 + Weighted sampler 0.6940 0.0116 + Appending 17 extra spectral indices + using water mask to mask land 0.7083 0.0143 + EfficientNet-B5 instead of ResNet-50 0.7101 0.0018 + Decision threshold optimization + bf16-mixed inference 0.7132 0.0031 + 10-fold model ensemble 0.7169 0.0037 + Training for 50 epochs + weighted average + soft labels 0.7210 0.0041"},{"location":"technical-report/#best-single-model","title":"Best single model","text":"<p>The best single model had private LB score of 0.7264 which would result in 5th place in the final ranking.</p> <p>NOTE: All submissions were created using a Linux machine with 8 core AMD CPU, 32 GB RAM, 2TB SSD disk and RTX 3090 GPU.</p> <p>The model was a UNet with EfficientNet-B5 encoder pretrained on ImageNet trained in a following fashion:</p> <ul> <li>Trained with Azure ML using <code>Standard_NC4as_T4_v3</code> spot instances (4 cores, 28 GB RAM, 176 GB disk, Tesla T4 GPU with   16 GB VRAM)</li> <li>CV-Fold: #7</li> <li>Channel order: R,G,B,SWIR,NIR,QA,DEM</li> <li>Additional spectral indices: DEMWM, NDVI, ATSAVI, AVI, CI, ClGreen, GBNDVI, GVMI, IPVI, KIVU, MCARI, MVI, NormNIR,   PNDVI, SABI, WDRVI, mCRIG</li> <li>Default decoder channel numbers</li> <li>Batch size: 32</li> <li>Accumulate Grad Batches: 1</li> <li>Val Check Interval: 1</li> <li>Loss: Dice</li> <li>Weighted Sampler: @10240 samples / epoch</li> <li>Per image weights based on following config:<ul> <li><code>almost_all_water_importance_factor</code>: 0.5</li> <li><code>dem_nan_pixels_pct_importance_factor</code>: 0.25</li> <li><code>dem_zero_pixels_pct_importance_factor</code>: -1.0</li> <li><code>has_kelp_importance_factor</code>: 3.0</li> <li><code>kelp_pixels_pct_importance_factor</code>: 0.2</li> <li><code>qa_corrupted_pixels_pct_importance_factor</code>: -1.0</li> <li><code>qa_ok_importance_factor</code>: 0.0</li> </ul> </li> <li>Image size: 352 (resized using padding)</li> <li>Image normalization strategy: <code>quantile</code></li> <li>Masking spectral indices with DEMWM and QA</li> <li>Replacing missing pixels with <code>torch.nan</code></li> <li>Epochs: 10</li> <li>Optimizer: AdamW</li> <li>Weight decay: 1e-4</li> <li>Max learning rate: 3e-4</li> <li>Learning rate scheduler: <code>OneCycleLR</code>:<ul> <li><code>onecycle_div_factor</code>: 2.0</li> <li><code>onecycle_final_div_factor</code>: 100.0</li> <li><code>onecycle_pct_start</code>: 0.1</li> </ul> </li> <li>Precision: <code>16-mixed</code></li> <li>No <code>torch-ort</code> or <code>torch.compile</code></li> <li>No <code>benchmark</code></li> <li>Seed: 42</li> <li>Training single model took: ~1:20h</li> </ul> <p>The submission used:</p> <ul> <li>No TTA</li> <li><code>bf16-mixed</code> precision</li> <li>Decision threshold = 0.45</li> <li>Inference took: ~30s</li> </ul>"},{"location":"technical-report/#best-submissions","title":"Best submissions","text":"<p>Two submissions with the same private LB score of 0.7318 were produced. Both of those were an ensemble of 10 models, trained on all 10-fold CV splits. Both submission used the best checkpoints not the last ones.</p>"},{"location":"technical-report/#submission-1","title":"Submission #1","text":"<p>Trained in the same way as the single model submission with following exceptions:</p> <ul> <li>Epochs increased to 50</li> <li>Mixture of all 10 CV-Folds</li> <li>Training single model took: ~6-7h</li> </ul> <p>NOTE: Model trained on fold=5 by mistake was trained for 10 epochs not 50</p> <p>Inference:</p> <ul> <li>No soft labels</li> <li>No TTA</li> <li>Decision threshold for a single model: 0.48</li> <li>Ensemble model weights:<ul> <li>fold=0: 1.0</li> <li>fold=1: 1.0</li> <li>fold=2: 1.0</li> <li>fold=3: 1.0</li> <li>fold=4: 1.0</li> <li>fold=5: 1.0</li> <li>fold=6: 1.0</li> <li>fold=7: 1.0</li> <li>fold=8: 1.0</li> <li>fold=9: 1.0</li> </ul> </li> <li>Ensemble decision threshold: 0.48</li> <li>Inference took: ~4:10 min</li> </ul>"},{"location":"technical-report/#submission-2","title":"Submission #2","text":"<p>Trained in the same way as the single model submission with following exceptions:</p> <ul> <li>Trained with Azure ML using <code>Standard_NC24ads_A100_v4</code> spot instances (24 cores, 220 GB RAM, 64 GB disk, A100 GPU with   80 GB VRAM)</li> <li>50 epochs</li> <li>Loss: Jaccard</li> <li>Decoder channels: 512, 256, 128, 64, 32</li> <li><code>bf16-mixed</code> precision</li> <li>Mixture of all 10 CV-Folds</li> <li>Training single model took: ~2h</li> </ul> <p>Inference:</p> <ul> <li>Soft labels</li> <li>No TTA</li> <li><code>bf16-mixed</code> precision</li> <li>Ensemble model weights:<ul> <li>fold=0: 0.666</li> <li>fold=1: 0.5</li> <li>fold=2: 0.666</li> <li>fold=3: 0.88</li> <li>fold=4: 0.637</li> <li>fold=5: 0.59</li> <li>fold=6: 0.733</li> <li>fold=7: 0.63</li> <li>fold=8: 1.0</li> <li>fold=9: 0.2</li> </ul> </li> <li>Ensemble decision threshold: 0.45</li> <li>Inference took: ~4:10 min</li> </ul>"},{"location":"technical-report/#private-leaderboard-scores","title":"Private leaderboard scores","text":"<p>In the end 11 submissions had the score that would qualify them as 2nd place solutions - 8 of those had score of over 0.73. Furthermore, additional 5 submissions had the same score as the 3rd place solution.</p> <p></p> <p>All of those submissions used model ensembles. The best single model was #41 in Author's submissions with a score of 0.7264. That would have been 5th place solution.</p>"},{"location":"technical-report/#code","title":"Code","text":"<p>The code is available on GitHub: https://github.com/xultaeculcis/kelp-wanted-competition</p>"},{"location":"technical-report/#project-documentation","title":"Project documentation","text":"<p>Interactive documentation page is available here: https://xultaeculcis.github.io/kelp-wanted-competition/</p>"},{"location":"technical-report/#dev-log","title":"Dev Log","text":"<p>A detailed development log has been kept during the competition. You can review what was done to train the model in great detail here.</p>"},{"location":"technical-report/#how-to-guides","title":"How-to guides","text":"<p>The best place to start with the solution is to review the How-To guides:</p> <ul> <li>Setting up dev environment</li> <li>Contributing</li> <li>Running tests</li> <li>Using Makefile commands</li> <li>Reproducibility of results</li> <li>Preparing data</li> <li>Training models</li> <li>MLFlow artifacts</li> <li>Evaluating models</li> <li>Running inference</li> <li>Making submissions</li> <li>XGBoost</li> <li>SAHI</li> </ul>"},{"location":"technical-report/#api-docs","title":"API docs","text":"<p>To learn more about how the code is structured and what are its functionalities go to: Code docs.</p>"},{"location":"technical-report/#methodology","title":"Methodology","text":"<p>This section aims to provide a detailed exposition of the methods employed to tackle the complex task of detecting kelp canopy presence using satellite imagery. Overall approach was multifaceted, integrating advanced machine learning techniques with domain-specific insights to develop a robust and effective model.</p>"},{"location":"technical-report/#issues-during-training","title":"Issues during training","text":"<p>Before discussing the methods and techniques used in this solution, the Author would like to acknowledge a few issues with the presented approach.</p> <ol> <li>Misaligned DEM and spectral layers - the misalignment became apparent in the last weeks of the competition,    that being said, aligning the images was not performed since as per competition rules: \"Eligible solutions need    to be able to run on test samples automatically using the test data as provided\". Test set modification was    prohibited.    Designing an automated band alignment although possible would not be ideal if additional verification and checks    for the final models were needed. Running alignment on the fly would be too expensive to run for each submission.</li> <li>DICE score discrepancy between local runs and LB - <code>torchmetrics</code> package was used for fast on-GPU metrics    calculation. While local DICE scores were in the range 0.84-0.86 the public LB scores were in range 0.70-0.72.    This discrepancy was attributed to the test set having different distribution and possibly higher number of false    positive kelp labels than the training set. Since the scores were consistent over time and increase in local DICE    usually resulted in higher LB scores a decision was made to keep using <code>torchmetrics</code> DICE implementation.    After seeing some scores on the competition forum, perhaps further investigation into score alignment issues would    yield better results in the end.</li> <li>The fold number used to compare the locally trained models changed from fold=0 to fold=6 and finally to fold=9.    Fold #0 was used mainly during the baseline training phase, where different trainer and dataloader hyperparameters    were tested. Once all folds were evaluated against public LB - fold #6 was used as it showed the best performance    on public LB. Fold #8 was only used after training larger more capable models with larger amount of samples / epoch    and with larger amount of input channels. Latest submissions were always evaluated against fold #8.</li> <li>Spot instances on Azure ML can be evicted, leading to failed Jobs - in such cases manual Job re-submission is needed.</li> </ol>"},{"location":"technical-report/#software-setup","title":"Software setup","text":"<p>All experiments were conducted on Linux based machines. Azure ML was used to scale the training jobs to the cloud. Local PC used Ubuntu 22.04 while jobs running on Azure ML used Ubuntu 20.04 based Docker images. The code is developed using Python. The environment management is performed using <code>conda</code> and <code>conda-lock</code>. Please refer to the Local env setup guide for setting up the local development environment.</p> <p>NOTE: The dev environment and code was not tested on Windows or macOS.</p> <p>A short guide on setting up Azure ML Workspace and Azure DevOps organization is available in Azure ML reproducibility guide. Scheduling AML Jobs is described in Training on Azure ML guide.</p> <p>Full list of packages with their versions can be found in the conda-lock.yml file. Specs for Azure ML Docker based Environment are here: acpt_train_env</p> <p>Some packages that were used for this competition include:</p> <ul> <li>pytorch</li> <li>torchvision</li> <li>pytorch-lightning</li> <li>kornia</li> <li>rasterio</li> <li>scikit-learn</li> <li>pandas</li> <li>ttach</li> <li>segmentation-models-pytorch</li> <li>timm</li> <li>dask</li> <li>matplotlib</li> </ul>"},{"location":"technical-report/#hardware-setup","title":"Hardware setup","text":"<p>For running quick experiments, data preparation, training models and making submissions a local PC with Ubuntu 22.04 was used. This machine was the only one used to make submission files.</p> <p>Specs:</p> <ul> <li>OS: Ubuntu 22.04</li> <li>Python 3.10 environment with PyTorch 2.1.2 and CUDA 12.1</li> <li>CPU: 8-Core AMD Ryzen 7 2700X</li> <li>RAM: 32 GB RAM</li> <li>Disk: 2 TB SSD</li> <li>GPU: RTX 3090 with 24 GB VRAM</li> </ul> <p>Larger training jobs and hyperparameter optimization sweeps were performed on Azure ML using following spot instances:</p> <ul> <li>Standard_NC4as_T4_v3:<ul> <li>Ubuntu 20.04 docker image</li> <li>Python 3.8 environment with PyTorch 2.1.2 and CUDA 12.1 (the Azure base image forces py38 usage)</li> <li>4 cores</li> <li>28 GB RAM</li> <li>176 GB disk</li> <li>Tesla T4 GPU with 16 GB VRAM</li> </ul> </li> <li>Standard_NC24ads_A100_v4:<ul> <li>Ubuntu 20.04 docker image</li> <li>Python 3.8 environment with PyTorch 2.1.2 and CUDA 12.1 (the Azure base image forces py38 usage)</li> <li>24 cores</li> <li>220 GB RAM</li> <li>64 GB disk</li> <li>A100 GPU with 80 GB VRAM</li> </ul> </li> </ul>"},{"location":"technical-report/#initial-data-processing","title":"Initial Data processing","text":"<p>Before the baseline submission was made, Exploratory Data Analysis (EDA) was performed. Samples were plotted in various composite configurations together with QA, DEM, NDVI and the Kelp Mask in a single figures. Samples were also visualized on the map using QGIS. The organizations choose to not include CRS information - probably to eliminate cheating, where one could use overlapping image extents in order to use leakage for model training. This however hindered robust train-val-test split strategies.</p>"},{"location":"technical-report/#baseline","title":"Baseline","text":"<p>For baseline a combo of UNet architecture from Segmentation Model PyTorch and ResNet-50 was used. Initially a 10-Fold Stratified Cross Validation was used. Stratification was done using following per image flag combination: <code>qa_ok</code>, <code>has_kelp</code>, <code>dem_has_nans</code>, <code>high_corrupted_pixels_pct</code>. Band statistics were calculated and <code>z-score</code> normalization strategy was used. NDVI was appended to input Tensor. Pixels with -32k values were filled using <code>0.0</code>. Bands used in this model were: SWIR, NIR, R, G, B, QA, DEM and NDVI. Batch size of 32, no LR Scheduler, training for 10 epochs, without any form of weighted sampler using <code>torch.nn.CrossEntropy</code> loss. The optimizer was set to Adam with 3e-4 learning rate. Since UNet expects the input tensor W and H dimension to be divisible by 32 the input images were padded to 352x352. This model (after fixing MLFlow checkpoint loading) achieved 0.6569 on public LB.</p>"},{"location":"technical-report/#data-fiddling","title":"Data fiddling","text":"<p>After it became apparent that there are \"duplicates\" in the validation datasets - images captured from different orbits, different platforms or at different times a need for more robust train-test split arose. You can view sample of \"duplicated\" images below (DEM layer).</p> <p></p> <p>You can see that the same Area of Interest is presented on those images. In order to create robust CV split strategy each DEM layer was used for embedding generation. DEM layer was chosen for AOI grouping since it does not contain any striping artifacts, corrupted or saturated pixels or clouds. Missing values in this layer mark the extent of the DEM layer i.e. DEM only goes couple hundred of meters into the sea - there is no point in calculating DEM over the water. Missing values were replaced with zeroes. Every DEM image was passed through a pre-trained ResNet network. The resulting embeddings were then compared with each other - if cosine similarity between two images was over 0.97 they were placed into a single group. The de-duplication resulted in 3313 unique AOI groups. Those groups where then used to perform 10-Fold Stratified CV Split.</p> <p>What's more a few images had been mislabelled as kelp. Those images were filtered out from the training dataset.</p> <p></p> <p>Input channels were also re-ordered so that the first channels match those used by pre-trained networks trained on ImageNet and other natural-image datasets. R, G, B, SWIR, NIR, QA, DEM band order consistently outperformed the default SWIR, NIR, R, G, B, QA, DEM by a small margin - val/dice 0.760 vs 0.762</p>"},{"location":"technical-report/#extra-channels-spectral-indices","title":"Extra channels - spectral indices","text":"<p>In Author's other projects utilization of additional input channels such as NDVI, EVI, Water Masks and other proved to greatly bump model predictive capabilities. To see a list of all implemented spectral indices see the indices page.</p> <p>The best models all used 17 extra spectral indices appended to the input tensor of R, G, B, SWIR, NIR, QA, DEM bands: DEMWM, NDVI, ATSAVI, AVI, CI, ClGreen, GBNDVI, GVMI, IPVI, KIVU, MCARI, MVI, NormNIR, PNDVI, SABI, WDRVI, mCRIG.</p> <p></p> <p>To see more visualization of those indices refer to MLFlow spectral indices artifacts guide.</p>"},{"location":"technical-report/#normalization-strategy","title":"Normalization strategy","text":"<p>Following normalization strategies were evaluated early on using split=6:</p> <ul> <li><code>z-score</code>: 0.834168</li> <li><code>quantile</code>: 0.834134</li> <li><code>min-max</code>: 0.831865</li> <li><code>per-sample-quantile</code>: 0.806227</li> <li><code>per-sample-min-max</code>: 0.801893</li> </ul> <p>In the end <code>quantile</code> normalization was used since it produces the most appealing visual samples and was more robust against outliers, the learning curve also seemed to converge faster. The idea behind quantile normalization is rather simple instead of using global min-max we calculate per-channel quantile values for q01 and q99. During training Min-Max normalization is used with q01 and q99 as min and max values respectively.</p> <p>You can view the effects of normalization strategies in the figures below.</p> <ul> <li>Z-Score:</li> </ul> <p></p> <ul> <li>Quantile:</li> </ul> <p></p> <ul> <li>Min-Max:</li> </ul> <p></p> <ul> <li>Per-Sample Min-Max:</li> </ul> <p></p> <ul> <li>Per-Sample Quantile:</li> </ul> <p></p> <p>Additionally, replacing corrupted, missing and land pixels with <code>0.0</code> and <code>torch.nan</code> was tested. Masking with <code>torch.nan</code> was better than using zeroes (Public LB  0.7062 vs 0.7083). A rationale here is that using zeroes can lead to suboptimal normalization. Masking with <code>torch.nan</code> is performed instead. After the spectral indices are calculated and appended to the input tensor, for each channel the <code>nan</code> and <code>-inf</code> pixels are replaced with minimal value for each channel, while <code>inf</code> pixels are replaced with maximal value per channel. Masking land and corrupted pixels in indices bumps the performance by over 1-2%</p>"},{"location":"technical-report/#loss-functions","title":"Loss functions","text":"<p>Various loss functions were evaluated:</p> <ul> <li><code>torch.nn.CrossEntropyLoss</code> (<code>weight=[0.4,0.6]</code>)</li> <li><code>smp.losses.DiceLoss</code></li> <li><code>smp.losses.JaccardLoss</code></li> <li><code>smp.losses.TverskyLoss</code></li> <li><code>torch.nn.CrossEntropyLoss</code> (<code>weight=[0.3,0.7]</code>)</li> <li><code>torch.nn.CrossEntropyLoss</code> (<code>weight=None</code>)</li> <li><code>smp.losses.SoftCrossEntropyLoss</code> (<code>smooth_factor=0.1</code>))</li> <li><code>smp.losses.FocalLoss</code></li> <li><code>smp.losses.LovaszLoss</code></li> <li><code>smp.losses.SoftCrossEntropyLoss</code> (<code>smooth_factor=0.2</code>)</li> <li><code>smp.losses.SoftCrossEntropyLoss</code> (<code>smooth_factor=0.3</code>)</li> <li><code>torch.nn.CrossEntropyLoss</code> (<code>weight=[0.1,0.9]</code>)</li> <li><code>kelp.nn.models.losses.XEDiceLoss</code></li> <li><code>kelp.nn.models.losses.ComboLoss</code></li> <li><code>kelp.nn.models.losses.LogCoshDiceLoss</code></li> <li><code>kelp.nn.models.losses.HausdorffLoss</code></li> <li><code>kelp.nn.models.losses.TLoss</code></li> <li><code>kelp.nn.models.losses.ExponentialLogarithmicLoss</code></li> <li><code>kelp.nn.models.losses.SoftDiceLoss</code></li> <li><code>kelp.nn.models.losses.BatchSoftDice</code></li> </ul> <p>In the end Dice loss was used, since it performed best on public leaderboard. The best two model ensembles with the same private LB score of 0.7318 used DICE and Jaccard losses respectively.</p>"},{"location":"technical-report/#weighted-sampler","title":"Weighted sampler","text":"<p>Since over 2000 images do not have any kelp pixels in them. It was apparent that those images will not contribute to the training very much. Weighted Sampler was then used and adjusted via hparam search on Azure ML.</p> <p></p> <p>Following metadata stats and flags were used to determine the optimal per-image weight:</p> <ul> <li><code>has_kelp</code> - a flag indicating if the image has kelp in it</li> <li><code>kelp_pixels_pct</code> - percentage of all pixels marked as kelp</li> <li><code>dem_nan_pixels_pct</code> - percentage of all DEM pixels marked as NaN</li> <li><code>dem_zero_pixels_pct</code> - percentage of all DEM pixels with value=zero</li> <li><code>almost_all_water</code> - a flag indicating that over 98% of the DEM layer pixels are water</li> <li><code>qa_ok</code> - a flag indicating that no pixels are corrupted in the QA band</li> <li> <p><code>qa_corrupted_pixels_pct</code> - percentage of corrupted pixels in the QA band</p> </li> <li> <p>Some results after whole night of training:</p> </li> </ul> samples_per_epoch has_kelp kelp_pixels_pct qa_ok qa_corrupted_pixels_pct almost_all_water dem_nan_pixels_pct dem_zero_pixels_pct val/dice 5120 2 0.5 0.5 -0.5 -1 0 -0.25 0.84405 5120 0.2 0 -0.5 -0.5 -0.5 0.25 0.5 0.84402 5120 3 0.5 -1 0 -1 0.25 0 0.84396 5120 3 0.2 0 -1 0.5 0.25 -1 0.84396 5120 2 0 0.5 0 0.25 0 -0.5 0.84391 5120 0.5 0.2 -0.5 0.75 0.5 -1 -0.25 0.84390 5120 3 0.2 0.5 -0.25 0.75 -0.25 0.5 0.84382 5120 3 0.5 1 -0.25 0.5 -0.5 0 0.84382 5120 3 2 -0.25 -0.5 -0.5 -1 0.75 0.84380 5120 2 1 0.25 -1 0.75 0.75 1 0.84377 5120 2 0 -0.5 0 -1 -1 -1 0.84374 5120 0.5 0 -1 -0.25 0.25 -0.25 -0.5 0.84373 5120 0.2 0 0 0.25 -1 0.25 0.5 0.84370 5120 2 0.5 0.25 -0.5 0.25 0.5 -0.5 0.84369 <ul> <li>After retraining using 10240 samples per epoch:</li> </ul> samples_per_epoch has_kelp kelp_pixels_pct qa_ok qa_corrupted_pixels_pct almost_all_water dem_nan_pixels_pct dem_zero_pixels_pct val/dice 10240 2 0.5 0.5 -0.5 -1 0 -0.25 0.84459 10240 0.2 0 -0.5 -0.5 -0.5 0.25 0.5 0.84456 10240 3 0.5 -1 0 -1 0.25 0 0.84501 10240 3 0.2 0 -1 0.5 0.25 -1 0.84801 10240 2 0 0.5 0 0.25 0 -0.5 0.84641 10240 0.5 0.2 -0.5 0.75 0.5 -1 -0.25 0.84622 10240 3 0.2 0.5 -0.25 0.75 -0.25 0.5 0.84546 10240 3 0.5 1 -0.25 0.5 -0.5 0 0.84619 10240 3 2 -0.25 -0.5 -0.5 -1 0.75 0.84500 10240 2 1 0.25 -1 0.75 0.75 1 0.84508 10240 2 0 -0.5 0 -1 -1 -1 0.84430 10240 0.5 0 -1 -0.25 0.25 -0.25 -0.5 0.84496 10240 0.2 0 0 0.25 -1 0.25 0.5 0.84522 10240 2 0.5 0.25 -0.5 0.25 0.5 -0.5 0.84538 <p>Training for 10240 samples / epoch yielded better results. Assigning higher importance to <code>has_kelp</code> flag while keeping the <code>almost_all_water</code> flag low and even negative was the best combination. Zeroes and NaN values in the DEM layer seemed to not be important as much - weights for those stats usually ranged from -0.25 - 0.25. The QA band stats seemed also to be only slightly important with weights being in rage -1.0 - 1.0.</p> <p>For public LB following configuration worked the best:</p> <ul> <li>10240 samples / epoch</li> <li><code>has_kelp_importance_factor=3.0</code></li> <li><code>kelp_pixels_pct_importance_factor=0.2</code></li> <li><code>qa_ok_importance_factor=0.0</code></li> <li><code>qa_corrupted_pixels_pct_importance_factor=-1.0</code></li> <li><code>almost_all_water_importance_factor=0.5</code></li> <li><code>dem_nan_pixels_pct_importance_factor=0.25</code></li> <li><code>dem_zero_pixels_pct_importance_factor=-1.0</code></li> </ul>"},{"location":"technical-report/#training-configuration","title":"Training configuration","text":"<p>Multiple training configurations were tested out including:</p> <ul> <li>Training from scratch - very slow convergence, training using pre-trained model is a must in this case.   Final DICE after 10 epochs=0.736, compared to 0.760 with <code>imagenet</code> weights</li> <li>Optimizers: Adam, SGD, AdamW - in the end AdamW was the best on local validation set -&gt; +0.02 for AdamW</li> <li>OneCycleLR vs no LR scheduler: 0.76640 vs 0.764593 but overall stability is better with 1Cycle</li> <li><code>weight_decay</code>:<ul> <li>1e-2: 0.759</li> <li>1e-3: 0.763</li> <li>1e-4: 0.765</li> <li>1e-5: 0.762</li> </ul> </li> <li>Tried to add <code>decoder_attention_type=\"scse\"</code> but it gives worse performance (val/dice=0.755)</li> <li>Learning rate set to 3e-4 worked best locally</li> <li>UNet required images to be divisible by 32 - resized the input images to 352x352 using zero-padding - during the   inference the padding is removed</li> <li>Using other resize strategies and image sizes did not generate better results</li> <li>Batch size of 32 was selected mainly to keep Tesla T4 GPUs fully saturated, almost at the edge of running into OOM   errors</li> <li>Training with <code>batch_size=8</code> and <code>accumulate_grad_batches=4</code> resulted in better local eval scores,   but did not improve leaderboard scores</li> <li>The train augmentations were rather simple:<ul> <li>Random Horizontal Flip</li> <li>Random Vertical Flip</li> <li>Random Rotation 0-90 deg.</li> </ul> </li> <li>Using Random Resized Crops did not work better than the base augmentations mentioned above</li> <li>Transforms (including appending spectral indices) were performed on batch of images on GPU using   kornia library</li> <li>Compiling the model using <code>torch-ort</code> or <code>torch.compile</code> did not yield much of speedups, in some configurations   the compilation phase took 50% of training time - in this case it is better suited for large multi-day and multi-GPU   training runs where those few % of speedup can really be beneficial.</li> </ul>"},{"location":"technical-report/#tta","title":"TTA","text":"<p>Test time augmentations were also tested. ttach library was used for it. Following augmentations were used during testing:</p> <ul> <li>Vertical flip</li> <li>Horizontal flip</li> <li>Rotation: 0, 90, 180, 270</li> </ul> <p>Results with different <code>tta_merge_mode</code>:</p> <ul> <li>Baseline (no TTA) val/dice=0.85339:</li> <li>max: 0.85490</li> <li>mean: 0.85458</li> <li>sum: 0.85458</li> <li>min: 0.85403</li> <li>gmean: 0.15955</li> <li>tsharpen: 0.00468 - loss was nan</li> </ul> <p>On public LB:</p> <ul> <li>no-tta: 0.7083</li> <li>max: 0.7076</li> <li>mean: 0.7073</li> </ul> <p>Decided that TTA was not working and never used it again...</p>"},{"location":"technical-report/#thresholding","title":"Thresholding","text":"<p>Multiple decision threshold were verified on local validation set.</p> threshold val/dice 0.3 0.85301 0.31 0.85305 0.32 0.85306 0.33 0.85307 0.34 0.85309 0.35 0.85314 0.36 0.85314 0.37 0.85313 0.38 0.85315 0.39 0.85317 0.4 0.85316 0.41 0.85317 0.42 0.85317 0.43 0.85320 0.44 0.85319 0.45 0.85320 0.46 0.85318 0.47 0.85314 0.48 0.85316 0.49 0.85317 0.5 0.85316 0.51 0.85315 0.52 0.85314 0.53 0.85313 0.54 0.85311 0.55 0.85309 0.56 0.85305 0.57 0.85303 0.58 0.85300 0.59 0.85300 0.6 0.85296 0.61 0.85296 0.62 0.85290 0.63 0.85287 0.64 0.85285 0.65 0.85281 0.66 0.85278 0.67 0.85274 0.68 0.85267 0.69 0.85259 0.7 0.85253 <ul> <li>Leaderboard using dt=0.45: 0.7077</li> </ul>"},{"location":"technical-report/#model-architectures-and-encoders","title":"Model Architectures and Encoders","text":"<p>Various Model architectures and encoders were tested to find the best performing ones. Azure ML Sweep Job was used to verify encoder + architecture pairs. The encoders came from both segmentation-models-pytorch and timm libraries. A sample job results are visible below.</p> <p></p> <p>General observations:</p> <ul> <li>ResUnet++ often results in <code>NaN</code> loss.</li> <li>Unet and Unet++ often were the best.</li> <li>Unet++ training was not deterministic even though <code>pl.seed_everything(42)</code> was used</li> <li>Bigger models often resulted in OOM errors during training - had to reduce batch size and apply gradient accumulation</li> <li>Some models expected the input image to be divisible by 8, 24, 128 etc. - the training config class had to be   adjusted to change the input <code>image_size</code> parameter to allow for training those models.</li> <li>In general bigger models worked better</li> <li>FCN - model collapses</li> <li>ConvNext and SWIN Transformer models not supported</li> <li>The best combo was UNet + EfficientNet-B5</li> </ul> <p>Results after overnight training on split=6 (Top 5 runs only):</p> encoder architecture val/dice tu-efficientnet_b5 unet 0.85854 tu-seresnext101_32x4d unet 0.85807 tu-resnest50d_4s2x40d unet 0.85787 tu-rexnetr_300 unet 0.85749 tu-seresnext26d_32x4d unet 0.85728 <p>As you might have noticed in the screenshot in the Private leaderboard subsection some of the top performing model ensembles used a mixture of EfficientNet, SE-ResNext, ReXNet-R and other encoders trained as part of LinkNet, UNet and UNet++ architectures. Although, best two submissions used an ensemble of just UNet + EfficientNet-B5 models.</p>"},{"location":"technical-report/#stochastic-weights-averaging-swa","title":"Stochastic Weights Averaging (SWA)","text":"<p>SWA was one of the last things that were tested during the competition. In short, SWA performs an equal average of the weights traversed by SGD with a modified learning rate schedule (see the left panel of figure below). SWA solutions end up in the center of a wide flat region of loss, while SGD tends to converge to the boundary of the low-loss region, making it susceptible to the shift between train and test error surfaces (see the middle and right panels of figure below).</p> <p></p> <p>Source: Stochastic Weight Averaging in PyTorch Illustrations of SWA and SGD with a Preactivation ResNet-164 on CIFAR-100 [1]. Left: test error surface for three FGE samples and the corresponding SWA solution (averaging in weight space). Middle and Right: test error and train loss surfaces showing the weights proposed by SGD (at convergence) and SWA, starting from the same initialization of SGD after 125 training epochs. Please see [1] for details on how these figures were constructed.</p> <p>The figure below presents and illustration of the learning rate schedule adopted by SWA. Standard decaying schedule is used for the first 75% of the training and then a high constant value is used for the remaining 25%. The SWA averages are formed during the last 25% of training.</p> <p></p> <p>The best model ensemble trained with SWA had public and private LB score of 0.7159 and 0.7285 respectively - thus, was not the best. Best single model with SWA had public and private LB score of 0.7147 and 0.7226 respectively.</p> <p>[1] Averaging Weights Leads to Wider Optima and Better Generalization; Pavel Izmailov, Dmitry Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson; Uncertainty in Artificial Intelligence (UAI), 2018</p>"},{"location":"technical-report/#xgboost","title":"XGBoost","text":"<p>Another idea that did not work was training XGBoost Classifier on all channels and spectral indices. The goal was not to use XGBoost to establish channel and spectral index importance, but rather completely replace the deep learning approach with classical tree-based model. Unfortunately best models on public LB had score of 0.5125. Which was way too low to try to optimize. Since predictions are done per pixel, TTA cannot be usd. Optimizing decision threshold did not improve the scores much. Even applying post-processing operations such as erosion or dilution would not bump the performance by 20 p.p.</p> <p>Sample predictions:</p> <p></p> <p>If you want to learn more about XGBoost approach and to reproduce the results, please see: XGBoost guide.</p> <p>The XGB feature importance at least gave the Author confidence that the original approach with appending spectral indices to the input tensor was somewhat justified.</p> <p></p>"},{"location":"technical-report/#sahi","title":"SAHI","text":"<p>Slicing Aided Hyper Inference or SAHI is a technique that helps overcome the problem with detecting and segmenting small objects in large images by utilizing inference on image slices and prediction merging. Because of this it is slower than running inference on full image but at the same time usually ends up having better performance, especially for smaller features.</p> <p>This was the last idea that was tested.</p> <p> Source: https://github.com/obss/sahi</p> <p>The idea was simple:</p> <ol> <li>Generate sliced dataset of small 128x128 non-overlapping tiles from the bigger 350x350 input images</li> <li>Use this dataset to train new model</li> <li>During training resize the crops to e.g. 320x320 resolution and train on those</li> <li>When running inference generate overlapping tiles, inference on those tiles, and merge the predicted masks    by averaging the predictions in the overlapping areas</li> <li>Profit?</li> </ol> <p>A sample of small tiles used for training are visualized below:</p> <p></p> <p>Best model trained on 128x128 crops with 320x320 resize and nearest interpolation resulted in public LB score of: 0.6848. As it turns out, the input time size of 350x350 is already too small for SAHI to shine. Sliced inference did not result in any meaningful bump in performance.</p> <p>If you want to learn more about SAHI approach and to reproduce the results, please see: SAHI guide.</p>"},{"location":"technical-report/#ensemble","title":"Ensemble","text":"<p>Averaging model predictions was a natural step to take in the later stages of the competition. The prediction scripts were written in a way to allow to optimize both decision threshold and usage of TTA. The final prediction averaging was done using per-model weights. The weights were obtained by using <code>MinMaxScaler</code> with feature range of <code>(0.2, 1.0)</code> on public LB scores obtained for each model. This resulted with following weights:</p> Fold # Public LB Score Calculated Weight 0 0.7110 0.666 1 0.7086 0.5 2 0.7110 0.666 3 0.7139 0.88 4 0.7106 0.637 5 0.7100 0.59 6 0.7119 0.733 7 0.7105 0.63 8 0.7155 1.0 9 0.7047 0.2 <p>2nd best submission used those weights. The best submission used following weights:</p> Fold # Public LB Score Calculated Weight 0 0.7110 1.0 1 0.7086 1.0 2 0.7110 1.0 3 0.7139 1.0 4 0.7106 1.0 5 0.7100 1.0 6 0.7119 1.0 7 0.7105 1.0 8 0.7155 1.0 9 0.7047 1.0 <p>Model stacking was not performed.</p>"},{"location":"technical-report/#soft-labels","title":"Soft labels","text":"<p>Once the ensemble prediction pipeline was in place soft label prediction was implemented. Using soft labels is rather simple - instead of using hard labels i.e. 0 - for <code>no kelp</code> and 1 for <code>kelp</code> we use probability of certain pixel belonging to <code>kelp</code> class. We can then use weighted average and decision threshold adjustment to make final prediction.</p> <p>As seen in the LB screenshot in the Private LB scores subsection, most of the top performing submissions used soft labels - the 2nd best submission included.</p>"},{"location":"technical-report/#conclusion","title":"Conclusion","text":"<p>In this highly competitive and challenging contest, approach to segmenting kelp forests using Landsat satellite imagery and deep learning has demonstrated the power of combining advanced machine learning techniques with domain-specific knowledge. Used methodology was propelled to the second place, showcasing a robust and effective solution for the critical environmental task of mapping and monitoring kelp canopy presence.</p> <p>The utilization of pre-trained weights laid a strong foundation, allowing the models to learn from relevant, pre-existing patterns, significantly accelerating the learning process. The introduction of the Normalized Difference Vegetation Index (NDVI) and other spectral indices such as ATSAVI, AVI, CI, and more, provided the models with enhanced capabilities to distinguish between kelp and non-kelp regions effectively. These indices, especially when coupled with strategy of reordering channels to align more closely with natural and scientific observations, significantly improved model performance.</p> <p>Decision to employ AdamW optimizer, accompanied by a carefully chosen weight decay and learning rate scheduler, further optimized the training process, striking a fine balance between fast convergence and avoiding overfitting. The choice of a 32 batch size fully leveraged the computational capacity of the Tesla T4 GPUs, ensuring efficient use of resources.</p> <p>The implementation of mixed-precision training and inference not only reduced model's memory footprint but also accelerated its computational speed, making the solution both effective and efficient. By adopting a weighted sampler with a tailored importance factor for each type of pixel, the models could focus more on significant areas, reducing the bias towards dominant classes.</p> <p>The UNet architecture, augmented with an EfficientNet-B5 encoder, proved to be the best combination for the task, striking a balance between accuracy and computational efficiency. Adjusting the decision threshold allowed for fine-tuning the model's sensitivity to kelp presence, which was crucial for achieving high evaluation scores.</p> <p>Comprehensive approach, which included ten-fold cross-validation, training models for 50 epochs, and creating an ensemble of the best-performing models, ensured that the solution was not only robust across different data splits but also generalized well to unseen data.</p> <p>To conclude, this work underscores the potential of machine learning to contribute significantly to environmental conservation efforts. By employing a mix of techniques, from spectral analysis to advanced deep learning models, a scalable and accurate method for monitoring kelp forests was developed. This solution not offers a practical tool for ecosystem management and conservation. Success of Deep Learning approach in this challenge reaffirms the importance of machine learning in addressing some of the most pressing environmental issues of our time.</p>"},{"location":"api_ref/","title":"About","text":"<p>Here you can find detailed documentation about the API reference including public functions and classes that can be imported and used in your own scripts.</p> <p>The diagram below shows the source code structure for the <code>kelp</code> package.</p> <pre><code>kelp\n\u251c\u2500\u2500 consts                               &lt;- Constants\n\u2502   \u251c\u2500\u2500 data.py\n\u2502   \u251c\u2500\u2500 directories.py\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 logging.py\n\u2502   \u2514\u2500\u2500 reproducibility.py\n\u251c\u2500\u2500 core                                 &lt;- Common classes and functions shared across modules\n\u2502   \u251c\u2500\u2500 configs\n\u2502   \u2502   \u251c\u2500\u2500 argument_parsing.py\n\u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u2514\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 device.py\n\u2502   \u251c\u2500\u2500 indices.py\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 settings.py\n\u2502   \u2514\u2500\u2500 submission.py\n\u251c\u2500\u2500 data_prep                            &lt;- Data preparation scripts\n\u2502   \u251c\u2500\u2500 aoi_grouping.py\n\u2502   \u251c\u2500\u2500 calculate_band_stats.py\n\u2502   \u251c\u2500\u2500 dataset_prep.py\n\u2502   \u251c\u2500\u2500 eda.py\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 move_split_files.py\n\u2502   \u251c\u2500\u2500 plot_samples.py\n\u2502   \u251c\u2500\u2500 sahi_dataset_prep.py\n\u2502   \u2514\u2500\u2500 train_val_test_split.py\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 nn                                   &lt;- Segmentation Neural Network stuff\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2502   \u251c\u2500\u2500 band_stats.py\n\u2502   \u2502   \u251c\u2500\u2500 datamodule.py\n\u2502   \u2502   \u251c\u2500\u2500 dataset.py\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 transforms.py\n\u2502   \u2502   \u2514\u2500\u2500 utils.py\n\u2502   \u251c\u2500\u2500 inference\n\u2502   \u2502   \u251c\u2500\u2500 average_predictions.py\n\u2502   \u2502   \u251c\u2500\u2500 fold_weights.py\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 predict_and_submit.py\n\u2502   \u2502   \u251c\u2500\u2500 predict.py\n\u2502   \u2502   \u251c\u2500\u2500 preview_submission.py\n\u2502   \u2502   \u2514\u2500\u2500 sahi.py\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u251c\u2500\u2500 efficientunetplusplus\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 decoder.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 model.py\n\u2502   \u2502   \u251c\u2500\u2500 factories.py\n\u2502   \u2502   \u251c\u2500\u2500 fcn\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 model.py\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 losses.py\n\u2502   \u2502   \u251c\u2500\u2500 modules.py\n\u2502   \u2502   \u251c\u2500\u2500 resunet\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 decoder.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500model.py\n\u2502   \u2502   \u251c\u2500\u2500 resunetplusplus\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 decoder.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 model.py\n\u2502   \u2502   \u2514\u2500\u2500 segmentation.py\n\u2502   \u2514\u2500\u2500 training\n\u2502       \u251c\u2500\u2500 config.py\n\u2502       \u251c\u2500\u2500 eval_from_folders.py\n\u2502       \u251c\u2500\u2500 eval.py\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 options.py\n\u2502       \u2514\u2500\u2500 train.py\n\u251c\u2500\u2500 py.typed\n\u251c\u2500\u2500 utils                                &lt;- Utils\n\u2502   \u251c\u2500\u2500 gpu.py\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 logging.py\n\u2502   \u251c\u2500\u2500 mlflow.py\n\u2502   \u251c\u2500\u2500 plotting.py\n\u2502   \u2514\u2500\u2500 serialization.py\n\u2514\u2500\u2500 xgb                                  &lt;- XGBoost stuff\n   \u251c\u2500\u2500 inference\n   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u251c\u2500\u2500 predict_and_submit.py\n   \u2502   \u2514\u2500\u2500 predict.py\n   \u251c\u2500\u2500 __init__.py\n   \u2514\u2500\u2500 training\n      \u251c\u2500\u2500 cfg.py\n      \u251c\u2500\u2500 eval.py\n      \u251c\u2500\u2500 __init__.py\n      \u251c\u2500\u2500 options.py\n      \u2514\u2500\u2500 train.py\n</code></pre>"},{"location":"api_ref/consts/","title":"kelp.consts","text":"<p>The constants to be used across the project.</p>"},{"location":"api_ref/utils/","title":"kelp.utils","text":""},{"location":"api_ref/utils/#gpu","title":"GPU","text":"<p>The GPU utilities.</p>"},{"location":"api_ref/utils/#kelp.utils.gpu.set_gpu_power_limit_if_needed","title":"<code>kelp.utils.gpu.set_gpu_power_limit_if_needed</code>","text":"<p>Helper function, that sets GPU power limit if RTX 3090 is used</p> <p>Parameters:</p> Name Type Description Default <code>pw</code> <code>int</code> <p>The new power limit to set. Defaults to 250W.</p> <code>250</code> Source code in <code>kelp/utils/gpu.py</code> <pre><code>def set_gpu_power_limit_if_needed(pw: int = 250) -&gt; None:\n    \"\"\"\n    Helper function, that sets GPU power limit if RTX 3090 is used\n\n    Args:\n        pw: The new power limit to set. Defaults to 250W.\n\n    \"\"\"\n\n    stream = os.popen(\"nvidia-smi --query-gpu=gpu_name --format=csv\")\n    gpu_list = stream.read()\n    if \"NVIDIA GeForce RTX 3090\" in gpu_list:\n        os.system(\"sudo nvidia-smi -pm 1\")\n        os.system(f\"sudo nvidia-smi -pl {pw}\")\n</code></pre>"},{"location":"api_ref/utils/#logging","title":"Logging","text":"<p>The logging utilities.</p>"},{"location":"api_ref/utils/#kelp.utils.logging.get_logger","title":"<code>kelp.utils.logging.get_logger</code>","text":"<p>Builds a <code>Logger</code> instance with provided name and log level.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name for the logger.</p> required <code>log_level</code> <code>Union[int, str]</code> <p>The default log level.</p> <code>INFO</code> <p>Returns:</p> Type Description <code>Logger</code> <p>The logger.</p> Source code in <code>kelp/utils/logging.py</code> <pre><code>def get_logger(name: str, log_level: Union[int, str] = logging.INFO) -&gt; logging.Logger:\n    \"\"\"\n    Builds a `Logger` instance with provided name and log level.\n\n    Args:\n        name: The name for the logger.\n        log_level: The default log level.\n\n    Returns:\n        The logger.\n\n    \"\"\"\n\n    logger = logging.getLogger(name=name)\n    logger.setLevel(log_level)\n\n    # Prevent log messages from propagating to the parent logger\n    logger.propagate = False\n\n    # Check if handlers are already set to avoid duplication\n    if not logger.handlers:\n        stream_handler = logging.StreamHandler()\n        formatter = logging.Formatter(fmt=consts.logging.FORMAT)\n        stream_handler.setFormatter(fmt=formatter)\n        logger.addHandler(stream_handler)\n\n    return logger\n</code></pre>"},{"location":"api_ref/utils/#kelp.utils.logging.timed","title":"<code>kelp.utils.logging.timed</code>","text":"<p>This decorator prints the execution time for the decorated function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[P, T]</code> <p>The function to wrap.</p> required <p>Returns:</p> Type Description <code>Callable[P, T]</code> <p>Wrapper around the function.</p> Source code in <code>kelp/utils/logging.py</code> <pre><code>def timed(func: Callable[P, T]) -&gt; Callable[P, T]:\n    \"\"\"\n    This decorator prints the execution time for the decorated function.\n\n    Args:\n        func: The function to wrap.\n\n    Returns:\n        Wrapper around the function.\n\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -&gt; T:\n        _timed_logger.info(f\"{func.__qualname__} is running...\")\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        _timed_logger.info(f\"{func.__qualname__} ran in {(end - start):.4f}s\")\n        return result\n\n    return wrapper\n</code></pre>"},{"location":"api_ref/utils/#mlflow","title":"MLFlow","text":"<p>The MLFlow utilities.</p>"},{"location":"api_ref/utils/#kelp.utils.mlflow.get_mlflow_run_dir","title":"<code>kelp.utils.mlflow.get_mlflow_run_dir</code>","text":"<p>Gets MLFlow run directory given the active run and output directory. Args:     current_run: The current active run.     output_dir: The output directory.</p> <p>Returns: A path to the MLFlow run directory.</p> Source code in <code>kelp/utils/mlflow.py</code> <pre><code>def get_mlflow_run_dir(current_run: ActiveRun, output_dir: Path) -&gt; Path:\n    \"\"\"\n    Gets MLFlow run directory given the active run and output directory.\n    Args:\n        current_run: The current active run.\n        output_dir: The output directory.\n\n    Returns: A path to the MLFlow run directory.\n\n    \"\"\"\n    return Path(output_dir / str(current_run.info.experiment_id) / current_run.info.run_id)\n</code></pre>"},{"location":"api_ref/utils/#plotting","title":"Plotting","text":"<p>The sample plotting utilities.</p>"},{"location":"api_ref/utils/#kelp.utils.plotting.plot_sample","title":"<code>kelp.utils.plotting.plot_sample</code>","text":"<p>Plot a single sample of the satellite image.</p> <p>Parameters:</p> Name Type Description Default <code>input_arr</code> <code>ndarray</code> <p>The input image array. Expects all image bands to be provided.</p> required <code>target_arr</code> <code>Optional[ndarray]</code> <p>An optional kelp mask array.</p> <code>None</code> <code>predictions_arr</code> <code>Optional[ndarray]</code> <p>An optional kelp prediction array.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The figure size.</p> <code>(20, 4)</code> <code>ndvi_cmap</code> <code>str</code> <p>The colormap to use for the NDVI.</p> <code>'RdYlGn'</code> <code>dem_cmap</code> <code>str</code> <p>The colormap to use for the DEM band.</p> <code>'viridis'</code> <code>qa_mask_cmap</code> <code>str</code> <p>The colormap to use for the QA band.</p> <code>'gray'</code> <code>mask_cmap</code> <code>str</code> <p>The colormap to use for the kelp mask.</p> <code>CMAP</code> <code>show_titles</code> <code>bool</code> <p>A flag indicating whether the titles should be visible.</p> <code>True</code> <code>suptitle</code> <code>Optional[str]</code> <p>The title for the figure.</p> <code>None</code> Source code in <code>kelp/utils/plotting.py</code> <pre><code>def plot_sample(\n    input_arr: np.ndarray,  # type: ignore[type-arg]\n    target_arr: Optional[np.ndarray] = None,  # type: ignore[type-arg]\n    predictions_arr: Optional[np.ndarray] = None,  # type: ignore[type-arg]\n    figsize: Tuple[int, int] = (20, 4),\n    ndvi_cmap: str = \"RdYlGn\",\n    dem_cmap: str = \"viridis\",\n    qa_mask_cmap: str = \"gray\",\n    mask_cmap: str = consts.data.CMAP,\n    show_titles: bool = True,\n    suptitle: Optional[str] = None,\n) -&gt; plt.Figure:\n    \"\"\"\n    Plot a single sample of the satellite image.\n\n    Args:\n        input_arr: The input image array. Expects all image bands to be provided.\n        target_arr: An optional kelp mask array.\n        predictions_arr: An optional kelp prediction array.\n        figsize: The figure size.\n        ndvi_cmap: The colormap to use for the NDVI.\n        dem_cmap: The colormap to use for the DEM band.\n        qa_mask_cmap: The colormap to use for the QA band.\n        mask_cmap: The colormap to use for the kelp mask.\n        show_titles: A flag indicating whether the titles should be visible.\n        suptitle: The title for the figure.\n\n    Returns: A figure with plotted sample.\n\n    \"\"\"\n    num_panels = 6\n\n    if target_arr is not None:\n        num_panels = num_panels + 1\n\n    if predictions_arr is not None:\n        num_panels = num_panels + 1\n\n    tci = np.rollaxis(input_arr[[2, 3, 4]], 0, 3)\n    tci = min_max_normalize(tci)\n    false_color = np.rollaxis(input_arr[[1, 2, 3]], 0, 3)\n    false_color = min_max_normalize(false_color)\n    agriculture = np.rollaxis(input_arr[[0, 1, 2]], 0, 3)\n    agriculture = min_max_normalize(agriculture)\n    qa_mask = input_arr[5]\n    dem = input_arr[6]\n    ndvi = (input_arr[1] - input_arr[2]) / (input_arr[1] + input_arr[2] + consts.data.EPS)\n    dem = min_max_normalize(dem)\n\n    fig, axes = plt.subplots(nrows=1, ncols=num_panels, figsize=figsize, sharey=True)\n\n    axes[0].imshow(tci)\n    axes[1].imshow(false_color)\n    axes[2].imshow(agriculture)\n    axes[3].imshow(ndvi, cmap=ndvi_cmap, vmin=-1, vmax=1)\n    axes[4].imshow(dem, cmap=dem_cmap)\n    axes[5].imshow(qa_mask, cmap=qa_mask_cmap, interpolation=None)\n\n    if target_arr is not None:\n        axes[6].imshow(target_arr, cmap=mask_cmap, interpolation=None)\n\n    if predictions_arr is not None:\n        axes[7 if target_arr is not None else 6].imshow(predictions_arr, cmap=mask_cmap, interpolation=None)\n\n    if show_titles:\n        axes[0].set_xlabel(\"Natural Color (R, G, B)\")\n        axes[1].set_xlabel(\"Color Infrared (NIR, R, B)\")\n        axes[2].set_xlabel(\"Short Wave Infrared (SWIR, NIR, R)\")\n        axes[3].set_xlabel(\"NDVI\")\n        axes[4].set_xlabel(\"DEM\")\n        axes[5].set_xlabel(\"QA Mask\")\n\n        if target_arr is not None:\n            axes[6].set_xlabel(\"Kelp Mask GT\")\n\n        if predictions_arr is not None:\n            axes[7 if target_arr is not None else 6].set_xlabel(\"Prediction\")\n\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api_ref/utils/#serialization","title":"Serialization","text":"<p>The serialization utils.</p>"},{"location":"api_ref/utils/#kelp.utils.serialization.JsonEncoder","title":"<code>kelp.utils.serialization.JsonEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>Custom JSON encoder that handles datatypes that are not out-of-the-box supported by the <code>json</code> package.</p> Source code in <code>kelp/utils/serialization.py</code> <pre><code>class JsonEncoder(JSONEncoder):\n    \"\"\"\n    Custom JSON encoder that handles datatypes that are not out-of-the-box supported by the `json` package.\n    \"\"\"\n\n    def default(self, o: Any) -&gt; str:\n        if isinstance(o, datetime) or isinstance(o, date):\n            return o.isoformat()\n\n        if isinstance(o, Path):\n            return o.as_posix()\n\n        return super().default(o)  # type: ignore\n</code></pre>"},{"location":"api_ref/core/configs/","title":"configs","text":""},{"location":"api_ref/core/configs/#configs","title":"Configs","text":"<p>The base classes for step entrypoint configs.</p>"},{"location":"api_ref/core/configs/#base-classes","title":"Base classes","text":"<p>The base classes for step entrypoint configs.</p>"},{"location":"api_ref/core/configs/#kelp.core.configs.base.ConfigBase","title":"<code>kelp.core.configs.base.ConfigBase</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A base class for all entrypoint config classes.</p> Source code in <code>kelp/core/configs/base.py</code> <pre><code>class ConfigBase(BaseModel):\n    \"\"\"A base class for all entrypoint config classes.\"\"\"\n\n    def __str__(self) -&gt; str:\n        return json.dumps(self.model_dump(), cls=JsonEncoder, indent=4)\n\n    def log_self(self) -&gt; None:\n        \"\"\"\n        Logs a short info with INFO logging level about what parameters is the script being run with.\n        \"\"\"\n        _logger.info(f\"Running with following config: {self}\")\n</code></pre>"},{"location":"api_ref/core/configs/#kelp.core.configs.base.ConfigBase.log_self","title":"<code>kelp.core.configs.base.ConfigBase.log_self</code>","text":"<p>Logs a short info with INFO logging level about what parameters is the script being run with.</p> Source code in <code>kelp/core/configs/base.py</code> <pre><code>def log_self(self) -&gt; None:\n    \"\"\"\n    Logs a short info with INFO logging level about what parameters is the script being run with.\n    \"\"\"\n    _logger.info(f\"Running with following config: {self}\")\n</code></pre>"},{"location":"api_ref/core/configs/#argument-parsing","title":"Argument parsing","text":"<p>The argument parsing helper functions for the script entrypoint arguments.</p>"},{"location":"api_ref/core/configs/#kelp.core.configs.argument_parsing.parse_args","title":"<code>kelp.core.configs.argument_parsing.parse_args</code>","text":"<p>Parses Command Line arguments and returns the script config model.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>The instance of <code>argparse.ArgumentParser</code> to use.</p> required <code>cfg_cls</code> <code>Type[T]</code> <p>The class of the config to use. Anything inheriting from <code>ConfigBase</code> will work.</p> required <p>Returns:</p> Type Description <code>T</code> <p>A config instance of type given by <code>cfg_cls</code>.</p> Source code in <code>kelp/core/configs/argument_parsing.py</code> <pre><code>def parse_args(parser: argparse.ArgumentParser, cfg_cls: Type[T]) -&gt; T:\n    \"\"\"\n    Parses Command Line arguments and returns the script config model.\n\n    Args:\n        parser: The instance of `argparse.ArgumentParser` to use.\n        cfg_cls: The class of the config to use. Anything inheriting from `ConfigBase` will work.\n\n    Returns:\n         A config instance of type given by `cfg_cls`.\n\n    \"\"\"\n    known_args, unknown_args = parser.parse_known_args()\n    _logger.info(f\"Unknown args: {unknown_args}\")\n    cfg = cfg_cls(**vars(known_args))\n    if cfg.log_config:\n        _logger.info(f\"Running with following config: {cfg}\")\n\n    return cfg\n</code></pre>"},{"location":"api_ref/core/indices/","title":"indices","text":"<p>The spectral indices transform classes.</p>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendAFRI1600","title":"<code>kelp.core.indices.AppendAFRI1600</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Aerosol free vegetation index 1600</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendAFRI1600(AppendIndex):\n    \"\"\"\n    Aerosol free vegetation index 1600\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_swir: int,\n        index_nir: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_swir=index_swir,\n            index_nir=index_nir,\n        )\n\n    def _compute_index(self, swir: Tensor, nir: Tensor) -&gt; Tensor:\n        return nir - 0.66 * (swir / (nir + 0.66 * swir + consts.data.EPS))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendARVI","title":"<code>kelp.core.indices.AppendARVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Atmospherically Resistant Vegetation Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendARVI(AppendIndex):\n    \"\"\"\n    Atmospherically Resistant Vegetation Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return -0.18 + 1.17 * ((nir - red) / (nir + red + consts.data.EPS))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendATSAVI","title":"<code>kelp.core.indices.AppendATSAVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Adjusted transformed soil-adjusted VI</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendATSAVI(AppendIndex):\n    \"\"\"\n    Adjusted transformed soil-adjusted VI\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return (\n            1.22\n            * (nir - 1.22 * red - 0.03)\n            / (1.22 * nir + red - 1.22 * 0.03 + 0.08 * (1 + 1.22**2) + consts.data.EPS)\n        )\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendAVI","title":"<code>kelp.core.indices.AppendAVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Ashburn Vegetation Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendAVI(AppendIndex):\n    \"\"\"\n    Ashburn Vegetation Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return 2 * nir - red\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendBWDRVI","title":"<code>kelp.core.indices.AppendBWDRVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Blue-wide dynamic range vegetation index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendBWDRVI(AppendIndex):\n    \"\"\"\n    Blue-wide dynamic range vegetation index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, blue: Tensor) -&gt; Tensor:\n        return (0.1 * nir - blue) / (0.1 * nir + blue + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendCDOM","title":"<code>kelp.core.indices.AppendCDOM</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Colored Dissolved Organic Matter</p> References <p>Se2WaQ - Sentinel-2 Water Quality</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendCDOM(AppendIndex):\n    \"\"\"\n    Colored Dissolved Organic Matter\n\n    References:\n       [Se2WaQ - Sentinel-2 Water Quality](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/se2waq/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor) -&gt; Tensor:\n        return 537 * torch.exp(-2.93 * green / (red + consts.data.EPS))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendCHLA","title":"<code>kelp.core.indices.AppendCHLA</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Chlorophyll-a</p> References <p>Se2WaQ - Sentinel-2 Water Quality</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendCHLA(AppendIndex):\n    \"\"\"\n    Chlorophyll-a\n\n    References:\n        [Se2WaQ - Sentinel-2 Water Quality](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/se2waq/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, blue: Tensor, green: Tensor) -&gt; Tensor:\n        # Yes, I know we should use coastal band here instead of blue, but we don't get to have coastal band\n        blue = blue / 65_535\n        green = green / 65_535\n        return 4.23 * torch.pow((green / (blue + consts.data.EPS)), 3.94)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendCI","title":"<code>kelp.core.indices.AppendCI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Coloration Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendCI(AppendIndex):\n    \"\"\"\n    Coloration Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, red: Tensor, blue: Tensor) -&gt; Tensor:\n        return (red - blue) / (red + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendCVI","title":"<code>kelp.core.indices.AppendCVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Chlorophyll vegetation index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendCVI(AppendIndex):\n    \"\"\"\n    Chlorophyll vegetation index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor) -&gt; Tensor:\n        return nir * (red / (green**2 + consts.data.EPS))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendCYA","title":"<code>kelp.core.indices.AppendCYA</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Cyanobacteria density</p> References <p>Se2WaQ - Sentinel-2 Water Quality</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendCYA(AppendIndex):\n    \"\"\"\n    Cyanobacteria density\n\n    References:\n       [Se2WaQ - Sentinel-2 Water Quality](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/se2waq/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        red = red / 65_535\n        green = green / 65_535\n        blue = blue / 65_535\n        return torch.pow(((green * red + consts.data.EPS) / (blue + consts.data.EPS)), 2.38)  # * 115_530.31\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendClGreen","title":"<code>kelp.core.indices.AppendClGreen</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Chlorophyll Index Green</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendClGreen(AppendIndex):\n    \"\"\"\n    Chlorophyll Index Green\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, green: Tensor) -&gt; Tensor:\n        return nir / (green + consts.data.EPS) - 1\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendDEMWM","title":"<code>kelp.core.indices.AppendDEMWM</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>DEM Water Mask</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendDEMWM(AppendIndex):\n    \"\"\"\n    DEM Water Mask\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_dem: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_dem=index_dem,\n        )\n\n    def _compute_index(self, dem: Tensor) -&gt; Tensor:\n        return torch.maximum(dem, torch.zeros_like(dem))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendDOC","title":"<code>kelp.core.indices.AppendDOC</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Dissolved Organic Carbon index</p> References <p>Se2WaQ - Sentinel-2 Water Quality</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendDOC(AppendIndex):\n    \"\"\"\n    Dissolved Organic Carbon index\n\n    References:\n       [Se2WaQ - Sentinel-2 Water Quality](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/se2waq/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor) -&gt; Tensor:\n        return 432 * torch.exp(-2.24 * green / (red + consts.data.EPS))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendDVIMSS","title":"<code>kelp.core.indices.AppendDVIMSS</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Differenced Vegetation Index MSS</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendDVIMSS(AppendIndex):\n    \"\"\"\n    Differenced Vegetation Index MSS\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return 2.4 * nir - red\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendEVI","title":"<code>kelp.core.indices.AppendEVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Enhanced Vegetation Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendEVI(AppendIndex):\n    \"\"\"\n    Enhanced Vegetation Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, blue: Tensor) -&gt; Tensor:\n        return torch.clamp(\n            2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1 + consts.data.EPS)),\n            min=-20_000,\n            max=20_000,\n        )\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendEVI2","title":"<code>kelp.core.indices.AppendEVI2</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Enhanced Vegetation Index 2</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendEVI2(AppendIndex):\n    \"\"\"\n    Enhanced Vegetation Index 2\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return 2.4 * (nir - red) / (nir + red + 1 + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendEVI22","title":"<code>kelp.core.indices.AppendEVI22</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Enhanced Vegetation Index 2 -2</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendEVI22(AppendIndex):\n    \"\"\"\n    Enhanced Vegetation Index 2 -2\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return 2.5 * (nir - red) / (nir + 2.4 * red + 1 + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendGARI","title":"<code>kelp.core.indices.AppendGARI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Green atmospherically resistant vegetation index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendGARI(AppendIndex):\n    \"\"\"\n    Green atmospherically resistant vegetation index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return (nir - (green - (blue - red))) / (nir - (green + (blue - red)) + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendGBNDVI","title":"<code>kelp.core.indices.AppendGBNDVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Green-Blue NDVI</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendGBNDVI(AppendIndex):\n    \"\"\"\n    Green-Blue NDVI\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return (nir - (green + blue)) / (nir + green + blue + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendGDVI","title":"<code>kelp.core.indices.AppendGDVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Difference NIR/Green Green Difference Vegetation Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendGDVI(AppendIndex):\n    \"\"\"\n    Difference NIR/Green Green Difference Vegetation Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, green: Tensor) -&gt; Tensor:\n        return nir - green\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendGNDVI","title":"<code>kelp.core.indices.AppendGNDVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Green Normalized Difference Vegetation Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendGNDVI(AppendIndex):\n    \"\"\"\n    Green Normalized Difference Vegetation Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, green: Tensor) -&gt; Tensor:\n        return (nir - green) / (nir + green + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendGRNDVI","title":"<code>kelp.core.indices.AppendGRNDVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Green-Red NDVI</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendGRNDVI(AppendIndex):\n    \"\"\"\n    Green-Red NDVI\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor) -&gt; Tensor:\n        return (nir - (red + green)) / (nir + red + green + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendGVMI","title":"<code>kelp.core.indices.AppendGVMI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Global Vegetation Moisture Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendGVMI(AppendIndex):\n    \"\"\"\n    Global Vegetation Moisture Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_swir: int,\n        index_nir: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_swir=index_swir,\n            index_nir=index_nir,\n        )\n\n    def _compute_index(self, swir: Tensor, nir: Tensor) -&gt; Tensor:\n        return ((nir + 0.1) - (swir + 0.02)) / ((nir + 0.1) + (swir + 0.02) + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendH","title":"<code>kelp.core.indices.AppendH</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Hue</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendH(AppendIndex):\n    \"\"\"\n    Hue\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return torch.arctan(((2 * red - green - blue) / 30.5) * (green - blue))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendI","title":"<code>kelp.core.indices.AppendI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Intensity</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendI(AppendIndex):\n    \"\"\"\n    Intensity\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return (1 / 30.5) * (red + green + blue + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendIPVI","title":"<code>kelp.core.indices.AppendIPVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Infrared percentage vegetation index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendIPVI(AppendIndex):\n    \"\"\"\n    Infrared percentage vegetation index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor) -&gt; Tensor:\n        return (nir / (nir + red + consts.data.EPS) / 2) * (\n            ((red - green) / (red + consts.data.EPS) + green) + 1 + consts.data.EPS\n        )\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendIndex","title":"<code>kelp.core.indices.AppendIndex</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Base class for appending spectral indices to the input Tensor.</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendIndex(nn.Module, abc.ABC):\n    \"\"\"\n    Base class for appending spectral indices to the input Tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        index_qa: int = 5,\n        index_water_mask: int = 7,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **band_kwargs: Any,\n    ) -&gt; None:\n        super().__init__()\n        assert all(k.startswith(\"index_\") for k in band_kwargs), (\n            \"Passed keyword arguments must start with 'index_' followed by band name! \"\n            f\"Found following keys: {list(band_kwargs.keys())}\"\n        )\n        self.dim = -3\n        self.index_qa = index_qa\n        self.index_water_mask = index_water_mask\n        self.mask_using_qa = mask_using_qa\n        self.mask_using_water_mask = mask_using_water_mask\n        self.fill_val = fill_val\n        self.band_kwargs = band_kwargs\n\n    def _append_index(self, sample: Tensor, index: Tensor) -&gt; Tensor:\n        index = index.unsqueeze(self.dim)\n        sample = torch.cat([sample, index], dim=self.dim)\n        return sample\n\n    def _mask_index(self, index: Tensor, sample: Tensor, masking_band_index: int, apply_mask: bool) -&gt; Tensor:\n        if not apply_mask:\n            return index\n        mask = sample[..., masking_band_index, :, :]\n        index = torch.where(mask == 0, index, self.fill_val)\n        return index\n\n    @abc.abstractmethod\n    def _compute_index(self, **kwargs: Any) -&gt; Tensor:\n        pass\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # Compute the index\n        compute_kwargs: Dict[str, Tensor] = {\n            k.replace(\"index_\", \"\"): x[..., v, :, :] for k, v in self.band_kwargs.items()\n        }\n        index = self._compute_index(**compute_kwargs)\n\n        # Mask using QA band if requested\n        index = self._mask_index(\n            index=index,\n            sample=x,\n            masking_band_index=self.index_qa,\n            apply_mask=self.mask_using_qa,\n        )\n\n        # Mask using Water Mask if requested\n        index = self._mask_index(\n            index=index,\n            sample=x,\n            masking_band_index=self.index_water_mask,\n            apply_mask=self.mask_using_water_mask,\n        )\n\n        # Append to the input tensor\n        x = self._append_index(sample=x, index=index)\n\n        return x\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendKIVU","title":"<code>kelp.core.indices.AppendKIVU</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>KIVU</p> References <p>Boucher et al. 2018</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendKIVU(AppendIndex):\n    \"\"\"\n    KIVU\n\n    References:\n        [Boucher et al. 2018](https://doi.org/10.1002/eap.1708)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return (blue - red) / (green + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendKab1","title":"<code>kelp.core.indices.AppendKab1</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Kabbara Index 1</p> References <p>Boucher et al. 2018</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendKab1(AppendIndex):\n    \"\"\"\n    Kabbara Index 1\n\n    References:\n        [Boucher et al. 2018](https://doi.org/10.1002/eap.1708)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return 1.67 - 3.94 * torch.log(blue + consts.data.EPS) + 3.78 * torch.log(green + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendLogR","title":"<code>kelp.core.indices.AppendLogR</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Log Ratio</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendLogR(AppendIndex):\n    \"\"\"\n    Log Ratio\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return torch.log(nir / (red + consts.data.EPS) + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendMCARI","title":"<code>kelp.core.indices.AppendMCARI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Modified Chlorophyll Absorption in Reflectance Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendMCARI(AppendIndex):\n    \"\"\"\n    Modified Chlorophyll Absorption in Reflectance Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor) -&gt; Tensor:\n        return ((nir - red) - 0.2 * (nir - green)) * (nir / (red + consts.data.EPS))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendMCRIG","title":"<code>kelp.core.indices.AppendMCRIG</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>mCRIG</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendMCRIG(AppendIndex):\n    \"\"\"\n    mCRIG\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return (blue**-1 - green**-1) * nir\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendMSAVI","title":"<code>kelp.core.indices.AppendMSAVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Modified Soil Adjusted Vegetation Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendMSAVI(AppendIndex):\n    \"\"\"\n    Modified Soil Adjusted Vegetation Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return (2 * nir + 1 - torch.sqrt((2 * nir + 1) ** 2 - 8 * (nir - red))) / 2\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendMSRNirRed","title":"<code>kelp.core.indices.AppendMSRNirRed</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Modified Simple Ratio NIR/RED</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendMSRNirRed(AppendIndex):\n    \"\"\"\n    Modified Simple Ratio NIR/RED\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return ((nir / red + consts.data.EPS) - 1) / torch.sqrt((nir / (red + consts.data.EPS)) + 1)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendMVI","title":"<code>kelp.core.indices.AppendMVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Mid-infrared vegetation index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendMVI(AppendIndex):\n    \"\"\"\n    Mid-infrared vegetation index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_swir: int,\n        index_nir: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_swir=index_swir,\n            index_nir=index_nir,\n        )\n\n    def _compute_index(self, swir: Tensor, nir: Tensor) -&gt; Tensor:\n        return nir / (swir + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendNDAVI","title":"<code>kelp.core.indices.AppendNDAVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Normalized Difference Aquatic Vegetation Index</p> References <p>Villa et al. 2014</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendNDAVI(AppendIndex):\n    \"\"\"\n    Normalized Difference Aquatic Vegetation Index\n\n    References:\n        [Villa et al. 2014](https://doi.org/10.1016/j.jag.2014.01.017)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, blue: Tensor) -&gt; Tensor:\n        return (nir - blue) / (nir + blue + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendNDVI","title":"<code>kelp.core.indices.AppendNDVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Normalized Difference Vegetation Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendNDVI(AppendIndex):\n    \"\"\"\n    Normalized Difference Vegetation Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return (nir - red) / (nir + red + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendNDVIWM","title":"<code>kelp.core.indices.AppendNDVIWM</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>NDVI Water Mask</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendNDVIWM(AppendIndex):\n    \"\"\"\n    NDVI Water Mask\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return torch.maximum((nir - red) / (nir + red + consts.data.EPS), torch.zeros_like(nir))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendNDWI","title":"<code>kelp.core.indices.AppendNDWI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Normalized Difference Water Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendNDWI(AppendIndex):\n    \"\"\"\n    Normalized Difference Water Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, green: Tensor) -&gt; Tensor:\n        return (nir - green) / (nir + green + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendNDWIWM","title":"<code>kelp.core.indices.AppendNDWIWM</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>NDWI Water Mask</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendNDWIWM(AppendIndex):\n    \"\"\"\n    NDWI Water Mask\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, green: Tensor) -&gt; Tensor:\n        return torch.maximum((nir - green) / (nir + green + consts.data.EPS), torch.zeros_like(nir))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendNLI","title":"<code>kelp.core.indices.AppendNLI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Nonlinear vegetation index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendNLI(AppendIndex):\n    \"\"\"\n    Nonlinear vegetation index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return (nir**2 - red) / (nir**2 + red + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendNormG","title":"<code>kelp.core.indices.AppendNormG</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Norm Green</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendNormG(AppendIndex):\n    \"\"\"\n    Norm Green\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor) -&gt; Tensor:\n        return green / (nir + red + green + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendNormNIR","title":"<code>kelp.core.indices.AppendNormNIR</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Norm NIR</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendNormNIR(AppendIndex):\n    \"\"\"\n    Norm NIR\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor) -&gt; Tensor:\n        return nir / (nir + red + green + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendNormR","title":"<code>kelp.core.indices.AppendNormR</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Norm R</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendNormR(AppendIndex):\n    \"\"\"\n    Norm R\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor) -&gt; Tensor:\n        return red / (nir + red + green + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendPNDVI","title":"<code>kelp.core.indices.AppendPNDVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <pre><code>Pan NDVI\n</code></pre> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendPNDVI(AppendIndex):\n    \"\"\"\n        Pan NDVI\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return (nir - (red + green + blue)) / (nir + red + green + blue + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendRBNDVI","title":"<code>kelp.core.indices.AppendRBNDVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Red-Blue NDVI</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendRBNDVI(AppendIndex):\n    \"\"\"\n    Red-Blue NDVI\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, blue: Tensor) -&gt; Tensor:\n        return (nir - (red + blue)) / (nir + red + blue + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendSABI","title":"<code>kelp.core.indices.AppendSABI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Surface Algal Bloom Index</p> References <p>Boucher et al. 2018</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendSABI(AppendIndex):\n    \"\"\"\n    Surface Algal Bloom Index\n\n    References:\n        [Boucher et al. 2018](https://doi.org/10.1002/eap.1708)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return (nir - red) / (green + blue + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendSQRTNIRR","title":"<code>kelp.core.indices.AppendSQRTNIRR</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>SQRT(IR/R)</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendSQRTNIRR(AppendIndex):\n    \"\"\"\n    SQRT(IR/R)\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return torch.sqrt(nir / (red + consts.data.EPS))\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendSRGR","title":"<code>kelp.core.indices.AppendSRGR</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Simple Ratio NIR/Blue</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendSRGR(AppendIndex):\n    \"\"\"\n    Simple Ratio NIR/Blue\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor) -&gt; Tensor:\n        return green / (red + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendSRNIRG","title":"<code>kelp.core.indices.AppendSRNIRG</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Simple Ratio NIR/Green</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendSRNIRG(AppendIndex):\n    \"\"\"\n    Simple Ratio NIR/Green\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, nir: Tensor, green: Tensor) -&gt; Tensor:\n        return nir / (green + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendSRNIRR","title":"<code>kelp.core.indices.AppendSRNIRR</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Simple Ratio NIR/Red</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendSRNIRR(AppendIndex):\n    \"\"\"\n    Simple Ratio NIR/Red\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return nir / (red + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendSRNIRSWIR","title":"<code>kelp.core.indices.AppendSRNIRSWIR</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Simple Ratio NIR/SWIR</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendSRNIRSWIR(AppendIndex):\n    \"\"\"\n    Simple Ratio NIR/SWIR\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_swir: int,\n        index_nir: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_swir=index_swir,\n            index_nir=index_nir,\n        )\n\n    def _compute_index(self, swir: Tensor, nir: Tensor) -&gt; Tensor:\n        return nir / (swir + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendSRSWIRNIR","title":"<code>kelp.core.indices.AppendSRSWIRNIR</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Simple Ratio SWIR/NIR</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendSRSWIRNIR(AppendIndex):\n    \"\"\"\n    Simple Ratio SWIR/NIR\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_swir: int,\n        index_nir: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_swir=index_swir,\n            index_nir=index_nir,\n        )\n\n    def _compute_index(self, swir: Tensor, nir: Tensor) -&gt; Tensor:\n        return swir / (nir + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendTNDVI","title":"<code>kelp.core.indices.AppendTNDVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Transformed NDVI</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendTNDVI(AppendIndex):\n    \"\"\"\n    Transformed NDVI\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return torch.sqrt((nir - red) / (nir + red + consts.data.EPS) + 0.5)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendTURB","title":"<code>kelp.core.indices.AppendTURB</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Water Turbidity</p> References <p>Se2WaQ - Sentinel-2 Water Quality</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendTURB(AppendIndex):\n    \"\"\"\n    Water Turbidity\n\n    References:\n       [Se2WaQ - Sentinel-2 Water Quality](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/se2waq/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return 8.93 * (green / (blue + consts.data.EPS)) - 6.39\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendTVI","title":"<code>kelp.core.indices.AppendTVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Transformed Vegetation Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendTVI(AppendIndex):\n    \"\"\"\n    Transformed Vegetation Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor) -&gt; Tensor:\n        return torch.sqrt(((red - green) / (red + green + consts.data.EPS)) + 0.5)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendVARIGreen","title":"<code>kelp.core.indices.AppendVARIGreen</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Visible Atmospherically Resistant Index Green</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendVARIGreen(AppendIndex):\n    \"\"\"\n    Visible Atmospherically Resistant Index Green\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor, blue: Tensor) -&gt; Tensor:\n        return (green - red) / (green + red - blue + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendWAVI","title":"<code>kelp.core.indices.AppendWAVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Water Adjusted Vegetation Index</p> References <p>Villa et al. 2014</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendWAVI(AppendIndex):\n    \"\"\"\n    Water Adjusted Vegetation Index\n\n    References:\n        [Villa et al. 2014](https://doi.org/10.1016/j.jag.2014.01.017)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_blue: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_blue=index_blue,\n        )\n\n    def _compute_index(self, nir: Tensor, blue: Tensor) -&gt; Tensor:\n        return 1.5 * (nir - blue) / (nir + blue + 0.5 + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendWDRVI","title":"<code>kelp.core.indices.AppendWDRVI</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Wide Dynamic Range Vegetation Index</p> References <p>Index DataBase</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendWDRVI(AppendIndex):\n    \"\"\"\n    Wide Dynamic Range Vegetation Index\n\n    References:\n        [Index DataBase](https://www.indexdatabase.de/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_nir: int,\n        index_red: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_nir=index_nir,\n            index_red=index_red,\n        )\n\n    def _compute_index(self, nir: Tensor, red: Tensor) -&gt; Tensor:\n        return (0.1 * nir - red) / (0.1 * nir + red + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/core/indices/#kelp.core.indices.AppendWaterColor","title":"<code>kelp.core.indices.AppendWaterColor</code>","text":"<p>             Bases: <code>AppendIndex</code></p> <p>Water color index</p> References <p>Se2WaQ - Sentinel-2 Water Quality</p> Source code in <code>kelp/core/indices.py</code> <pre><code>class AppendWaterColor(AppendIndex):\n    \"\"\"\n    Water color index\n\n    References:\n       [Se2WaQ - Sentinel-2 Water Quality](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/se2waq/)\n    \"\"\"\n\n    def __init__(\n        self,\n        index_red: int,\n        index_green: int,\n        index_qa: int = 5,\n        index_water_mask: int = 8,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        fill_val: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(\n            index_qa=index_qa,\n            index_water_mask=index_water_mask,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n            fill_val=fill_val,\n            index_red=index_red,\n            index_green=index_green,\n        )\n\n    def _compute_index(self, red: Tensor, green: Tensor) -&gt; Tensor:\n        return 25366 * torch.exp(-4.53 * green / (red + consts.data.EPS))\n</code></pre>"},{"location":"api_ref/core/settings/","title":"settings","text":"<p>The application settings.</p>"},{"location":"api_ref/core/settings/#kelp.core.settings.Settings","title":"<code>kelp.core.settings.Settings</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Represents Application Settings with nested configuration sections</p> Source code in <code>kelp/core/settings.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"Represents Application Settings with nested configuration sections\"\"\"\n\n    environment: str = \"local\"\n\n    model_config = SettingsConfigDict(\n        env_file=consts.directories.ROOT_DIR / \".env\",\n        env_file_encoding=\"utf-8\",\n        env_nested_delimiter=\"__\",\n        extra=\"ignore\",\n    )\n</code></pre>"},{"location":"api_ref/core/settings/#importing-settings","title":"Importing settings","text":"<p>To use current settings without the need to parse them each time you can use:</p> <pre><code>import logging\n\nfrom kelp.core.settings import current_settings\n\n\n# log current environment\nlogging.info(current_settings.env)  # INFO:dev\n</code></pre>"},{"location":"api_ref/core/submission/","title":"submission","text":"<p>The submission creation helpers.</p>"},{"location":"api_ref/core/submission/#kelp.core.submission.create_submission_tar","title":"<code>kelp.core.submission.create_submission_tar</code>","text":"<p>Creates submission TAR archive.</p> <p>Parameters:</p> Name Type Description Default <code>preds_dir</code> <code>Path</code> <p>The directory with predictions.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory where the submission file will be saved.</p> required <code>submission_file_name</code> <code>str</code> <p>The name of the submission file.</p> <code>'submission.tar'</code> Source code in <code>kelp/core/submission.py</code> <pre><code>def create_submission_tar(preds_dir: Path, output_dir: Path, submission_file_name: str = \"submission.tar\") -&gt; None:\n    \"\"\"\n    Creates submission TAR archive.\n\n    Args:\n        preds_dir: The directory with predictions.\n        output_dir: The output directory where the submission file will be saved.\n        submission_file_name: The name of the submission file.\n\n    \"\"\"\n    # Create a TAR file\n    with tarfile.open(output_dir / submission_file_name, \"w\") as tar:\n        for fp in preds_dir.glob(\"*_kelp.tif\"):\n            tar.add(fp, arcname=fp.name)\n\n    _logger.info(f\"Submission TAR file '{(output_dir / submission_file_name).as_posix()}' created successfully.\")\n</code></pre>"},{"location":"api_ref/core/submission/#kelp.core.submission.main","title":"<code>kelp.core.submission.main</code>","text":"<p>Main entrypoint for creating submission from predictions directory.</p> Source code in <code>kelp/core/submission.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for creating submission from predictions directory.\"\"\"\n    cfg = parse_args()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    create_submission_tar(preds_dir=cfg.predictions_dir, output_dir=cfg.output_dir)\n</code></pre>"},{"location":"api_ref/core/submission/#kelp.core.submission.parse_args","title":"<code>kelp.core.submission.parse_args</code>","text":"<p>Parse command line arguments for making a submission file.</p> Source code in <code>kelp/core/submission.py</code> <pre><code>def parse_args() -&gt; SubmissionConfig:\n    \"\"\"Parse command line arguments for making a submission file.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--predictions_dir\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    args = parser.parse_args()\n    cfg = SubmissionConfig(**vars(args))\n    cfg.log_self()\n    return cfg\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/","title":"aoi_grouping","text":"<p>AOI grouping step logic.</p>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.AOIGroupingConfig","title":"<code>kelp.data_prep.aoi_grouping.AOIGroupingConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>AOI grouping configuration</p> Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>class AOIGroupingConfig(ConfigBase):\n    \"\"\"AOI grouping configuration\"\"\"\n\n    dem_dir: Path\n    metadata_fp: Path\n    output_dir: Path\n    batch_size: int = 32\n    num_workers: int = 6\n    similarity_threshold: float = 0.95\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.ImageDataset","title":"<code>kelp.data_prep.aoi_grouping.ImageDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>A simple image dataset that loads images from a list of file paths.</p> <p>Parameters:</p> Name Type Description Default <code>fps</code> <code>List[Path]</code> <p>The file paths.</p> required <code>transform</code> <code>Callable[[Any], Tensor]</code> <p>Transform to apply to the input images.</p> required Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>class ImageDataset(Dataset):\n    \"\"\"\n    A simple image dataset that loads images from a list of file paths.\n\n    Args:\n        fps: The file paths.\n        transform: Transform to apply to the input images.\n    \"\"\"\n\n    def __init__(self, fps: List[Path], transform: Callable[[Any], Tensor]) -&gt; None:\n        self.fps = fps\n        self.transform = transform\n\n    def __getitem__(self, idx: int) -&gt; Tensor:\n        \"\"\"\n        Get image by its index.\n\n        Args:\n            idx: The image index.\n\n        Returns: A tensor with transformed image\n\n        \"\"\"\n        with open(self.fps[idx], \"rb\") as f:\n            img = Image.open(f)\n            sample = img.convert(\"RGB\")\n        sample = self.transform(sample)\n        return sample\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Get the number of images in the dataset.\n\n        Returns: The number of images in the dataset.\n\n        \"\"\"\n        return len(self.fps)\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.calculate_similarity_groups","title":"<code>kelp.data_prep.aoi_grouping.calculate_similarity_groups</code>","text":"<p>Calculate similarity groups.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ImageDataset</code> <p>An instance of ImageDataset class.</p> required <code>features</code> <code>ndarray</code> <p>The embeddings for all images.</p> required <code>threshold</code> <code>float</code> <p>The similarity threshold to use when comparing individual image pairs.</p> <code>0.95</code> Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>@timed\ndef calculate_similarity_groups(\n    dataset: ImageDataset,\n    features: np.ndarray,  # type: ignore[type-arg]\n    threshold: float = 0.95,\n) -&gt; List[List[str]]:\n    \"\"\"\n    Calculate similarity groups.\n\n    Args:\n        dataset: An instance of ImageDataset class.\n        features: The embeddings for all images.\n        threshold: The similarity threshold to use when comparing individual image pairs.\n\n    Returns: A list of similar images.\n\n    \"\"\"\n    similarity_matrix = cosine_similarity(features)\n    groups = []\n    for i in tqdm(range(len(similarity_matrix)), desc=\"Grouping similar images\", total=len(similarity_matrix)):\n        similar_images = []\n        for j in range(len(similarity_matrix[i])):\n            if i != j and similarity_matrix[i][j] &gt;= threshold:\n                similar_images.append(dataset.fps[j].stem.split(\"_\")[0])  # Add image path\n        if similar_images:\n            similar_images.append(dataset.fps[i].stem.split(\"_\")[0])\n            similar_images = sorted(similar_images)\n            if similar_images in groups:\n                continue\n            groups.append(similar_images)\n        else:\n            groups.append([dataset.fps[i].stem.split(\"_\")[0]])\n    return groups\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.explode_groups_if_needed","title":"<code>kelp.data_prep.aoi_grouping.explode_groups_if_needed</code>","text":"<p>Explodes all groups if needed.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>List[List[str]]</code> <p>The list of groups of similar images to explode.</p> required Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>@timed\ndef explode_groups_if_needed(groups: List[List[str]]) -&gt; List[List[str]]:\n    \"\"\"\n    Explodes all groups if needed.\n\n    Args:\n        groups: The list of groups of similar images to explode.\n\n    Returns: A list of groups of similar images.\n\n    \"\"\"\n\n    final_groups = []\n    for group in groups:\n        if len(group) &gt; IMAGES_PER_GROUP_EXPLODE_THRESHOLD:\n            final_groups.extend([[tile_id] for tile_id in group])\n            continue\n        final_groups.append(group)\n    return final_groups\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.find_similar_images","title":"<code>kelp.data_prep.aoi_grouping.find_similar_images</code>","text":"<p>Finds similar images in specified data folder.</p> <p>Parameters:</p> Name Type Description Default <code>data_folder</code> <code>Path</code> <p>The data folder with input images.</p> required <code>tile_ids</code> <code>List[str]</code> <p>A list of Tile IDs for corresponding images.</p> required <code>threshold</code> <code>float</code> <p>The similarity threshold to use when comparing individual image pairs.</p> <code>0.95</code> <code>batch_size</code> <code>int</code> <p>Batch size to use when generating embeddings.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>Number of worker processes to use when generating embeddings.</p> <code>6</code> Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>@timed\ndef find_similar_images(\n    data_folder: Path,\n    tile_ids: List[str],\n    threshold: float = 0.95,\n    batch_size: int = 32,\n    num_workers: int = 6,\n) -&gt; List[List[str]]:\n    \"\"\"\n    Finds similar images in specified data folder.\n\n    Args:\n        data_folder: The data folder with input images.\n        tile_ids: A list of Tile IDs for corresponding images.\n        threshold: The similarity threshold to use when comparing individual image pairs.\n        batch_size: Batch size to use when generating embeddings.\n        num_workers: Number of worker processes to use when generating embeddings.\n\n    Returns: A list of similar images.\n\n    \"\"\"\n    features, dataset = generate_embeddings(\n        data_folder=data_folder,\n        tile_ids=tile_ids,\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    groups = calculate_similarity_groups(\n        dataset=dataset,\n        features=features,\n        threshold=threshold,\n    )\n    return groups\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.generate_embeddings","title":"<code>kelp.data_prep.aoi_grouping.generate_embeddings</code>","text":"<p>Generates embeddings for images in specified data folder.</p> <p>Parameters:</p> Name Type Description Default <code>data_folder</code> <code>Path</code> <p>A path to the data folder.</p> required <code>tile_ids</code> <code>List[str]</code> <p>A list of Tile IDs for corresponding images.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use when generating embeddings.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>Number of worker processes to use when generating embeddings</p> <code>6</code> Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>@timed\ndef generate_embeddings(\n    data_folder: Path,\n    tile_ids: List[str],\n    batch_size: int = 32,\n    num_workers: int = 6,\n) -&gt; Tuple[np.ndarray, ImageDataset]:  # type: ignore[type-arg]\n    \"\"\"\n    Generates embeddings for images in specified data folder.\n\n    Args:\n        data_folder: A path to the data folder.\n        tile_ids: A list of Tile IDs for corresponding images.\n        batch_size: Batch size to use when generating embeddings.\n        num_workers: Number of worker processes to use when generating embeddings\n\n    Returns: A tuple of array with embeddings for each tile and an Image Dataset instance.\n\n    \"\"\"\n    fps = sorted([data_folder / f\"{tile_id}_dem.png\" for tile_id in tile_ids])\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n    model.eval()\n    model.to(device)\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n\n    dataset = ImageDataset(fps=fps, transform=transform)\n    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n\n    features = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Calculating embeddings\"):\n            outputs: Tensor = model(batch.to(device))\n            features.append(outputs.detach().cpu())\n\n    features_arr = torch.cat(features, dim=0).numpy()\n\n    return features_arr, dataset\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.group_aoi","title":"<code>kelp.data_prep.aoi_grouping.group_aoi</code>","text":"<p>Groups images in the specified DEM directory into similar AOIs.</p> <p>Parameters:</p> Name Type Description Default <code>dem_dir</code> <code>Path</code> <p>The path to the directory with DEM images.</p> required <code>metadata_fp</code> <code>Path</code> <p>The path to the metadata CSV file.</p> required <code>output_dir</code> <code>Path</code> <p>The path where to save the results.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use when generating embeddings.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>Number of worker processes to use when generating embeddings.</p> <code>6</code> <code>similarity_threshold</code> <code>float</code> <p>The similarity threshold to use when comparing individual image pairs.</p> <code>0.95</code> Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>@timed\ndef group_aoi(\n    dem_dir: Path,\n    metadata_fp: Path,\n    output_dir: Path,\n    batch_size: int = 32,\n    num_workers: int = 6,\n    similarity_threshold: float = 0.95,\n) -&gt; None:\n    \"\"\"\n    Groups images in the specified DEM directory into similar AOIs.\n\n    Args:\n        dem_dir: The path to the directory with DEM images.\n        metadata_fp: The path to the metadata CSV file.\n        output_dir: The path where to save the results.\n        batch_size: Batch size to use when generating embeddings.\n        num_workers: Number of worker processes to use when generating embeddings.\n        similarity_threshold: The similarity threshold to use when comparing individual image pairs.\n\n    \"\"\"\n    metadata = pd.read_csv(metadata_fp)\n    metadata[\"split\"] = metadata[\"in_train\"].apply(lambda x: \"train\" if x else \"test\")\n    training_tiles = metadata[metadata[\"split\"] == consts.data.TRAIN][\"tile_id\"].tolist()\n    groups = find_similar_images(\n        data_folder=dem_dir,\n        tile_ids=training_tiles,\n        threshold=similarity_threshold,\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    save_json(output_dir / f\"intermediate_image_groups_{similarity_threshold=}.json\", groups)\n    merged_groups = group_duplicate_images(groups=groups)\n    save_json(output_dir / f\"merged_image_groups_{similarity_threshold=}.json\", merged_groups)\n    final_groups = explode_groups_if_needed(groups=merged_groups)\n    save_json(output_dir / f\"final_image_groups_{similarity_threshold=}.json\", final_groups)\n    groups_df = groups_to_dataframe(final_groups)\n    (\n        metadata.merge(\n            groups_df,\n            left_on=\"tile_id\",\n            right_on=\"tile_id\",\n            how=\"left\",\n        ).to_parquet(output_dir / f\"metadata_{similarity_threshold=}.parquet\", index=False)\n    )\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.group_duplicate_images","title":"<code>kelp.data_prep.aoi_grouping.group_duplicate_images</code>","text":"<p>Groups duplicate (similar) images.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>List[List[str]]</code> <p>A list of lists where inner items are similar images.</p> required Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>@timed\ndef group_duplicate_images(groups: List[List[str]]) -&gt; List[List[str]]:\n    \"\"\"\n    Groups duplicate (similar) images.\n\n    Args:\n        groups: A list of lists where inner items are similar images.\n\n    Returns: Deduplicated list of the grouped images.\n\n    \"\"\"\n\n    # Step 1: Flatten the list of lists\n    flattened_list = [tile_id for similar_image_group in groups for tile_id in similar_image_group]\n\n    # Step 2: Create a map of image IDs to groups\n    id_to_group: Dict[str, int] = {}\n    group_to_ids: Dict[int, set[str]] = defaultdict(set)\n\n    # Step 3: Iterate through the image IDs\n    for tile_id in flattened_list:\n        if tile_id in id_to_group:\n            # Already assigned to a group, continue\n            continue\n\n        # Check for duplicates in other lists\n        assigned_group = None\n        for sublist in groups:\n            if tile_id in sublist:\n                # Check if any other ID in this sublist has been assigned a group\n                for other_id in sublist:\n                    if other_id in id_to_group:\n                        assigned_group = id_to_group[other_id]\n                        break\n                if assigned_group is not None:\n                    break\n\n        # Step 4: Assign groups to image IDs\n        if assigned_group is None:\n            # Create a new group\n            assigned_group = len(group_to_ids) + 1\n\n        id_to_group[tile_id] = assigned_group\n        group_to_ids[assigned_group].add(tile_id)\n\n    # Step 5: Group the IDs\n    final_groups = [list(group) for group in list(group_to_ids.values())]\n\n    return final_groups\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.groups_to_dataframe","title":"<code>kelp.data_prep.aoi_grouping.groups_to_dataframe</code>","text":"<p>Creates a DataFrame with groups of similar images.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>List[List[str]]</code> <p>A list of groups of similar images.</p> required Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>@timed\ndef groups_to_dataframe(groups: List[List[str]]) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a DataFrame with groups of similar images.\n\n    Args:\n        groups: A list of groups of similar images.\n\n    Returns: A DataFrame with groups of similar images.\n\n    \"\"\"\n    records = []\n    for idx, group in enumerate(groups):\n        for tile_id in group:\n            records.append((tile_id, idx))\n    return pd.DataFrame(records, columns=[\"tile_id\", \"aoi_id\"])\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.main","title":"<code>kelp.data_prep.aoi_grouping.main</code>","text":"<p>Main entrypoint for grouping similar tiles together into AOIs</p> Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for grouping similar tiles together into AOIs\"\"\"\n    cfg = parse_args()\n    group_aoi(\n        dem_dir=cfg.dem_dir,\n        metadata_fp=cfg.metadata_fp,\n        output_dir=cfg.output_dir,\n        batch_size=cfg.batch_size,\n        num_workers=cfg.num_workers,\n        similarity_threshold=cfg.similarity_threshold,\n    )\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.parse_args","title":"<code>kelp.data_prep.aoi_grouping.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of AOI Grouping Config.</p> Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>def parse_args() -&gt; AOIGroupingConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of AOI Grouping Config.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dem_dir\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--metadata_fp\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        default=32,\n    )\n    parser.add_argument(\n        \"--num_workers\",\n        type=int,\n        default=6,\n    )\n    parser.add_argument(\n        \"--similarity_threshold\",\n        type=float,\n        default=0.8,\n    )\n    args = parser.parse_args()\n    cfg = AOIGroupingConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/data_prep/aoi_grouping/#kelp.data_prep.aoi_grouping.save_json","title":"<code>kelp.data_prep.aoi_grouping.save_json</code>","text":"<p>Saves data to specified JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>fp</code> <code>Path</code> <p>The path to JSON file.</p> required <code>data</code> <code>Any</code> <p>The content to save as JSON.</p> required Source code in <code>kelp/data_prep/aoi_grouping.py</code> <pre><code>@timed\ndef save_json(fp: Path, data: Any) -&gt; None:\n    \"\"\"\n    Saves data to specified JSON file.\n\n    Args:\n        fp: The path to JSON file.\n        data: The content to save as JSON.\n\n    \"\"\"\n    with open(fp, \"w\") as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"api_ref/data_prep/calculate_band_stats/","title":"calculate_band_stats","text":"<p>Band stats calculation logic.</p>"},{"location":"api_ref/data_prep/calculate_band_stats/#kelp.data_prep.calculate_band_stats.StatisticsCalculationConfig","title":"<code>kelp.data_prep.calculate_band_stats.StatisticsCalculationConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>A Config class for running statistics calculations for training dataset.</p> Source code in <code>kelp/data_prep/calculate_band_stats.py</code> <pre><code>class StatisticsCalculationConfig(ConfigBase):\n    \"\"\"A Config class for running statistics calculations for training dataset.\"\"\"\n\n    data_dir: Path\n    output_dir: Path\n    mask_using_qa: bool = False\n    mask_using_water_mask: bool = False\n    fill_missing_pixels_with_torch_nan: bool = False\n\n    @property\n    def file_paths(self) -&gt; List[Path]:\n        \"\"\"List of file paths with satellite images.\"\"\"\n        return sorted(list(self.data_dir.rglob(\"*_satellite.tif\")))\n\n    @property\n    def fill_value(self) -&gt; float:\n        \"\"\"Resolved fill value for masking corrupted pixels.\"\"\"\n        return torch.nan if self.fill_missing_pixels_with_torch_nan else 0.0  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/data_prep/calculate_band_stats/#kelp.data_prep.calculate_band_stats.StatisticsCalculationConfig.file_paths","title":"<code>kelp.data_prep.calculate_band_stats.StatisticsCalculationConfig.file_paths: List[Path]</code>  <code>property</code>","text":"<p>List of file paths with satellite images.</p>"},{"location":"api_ref/data_prep/calculate_band_stats/#kelp.data_prep.calculate_band_stats.StatisticsCalculationConfig.fill_value","title":"<code>kelp.data_prep.calculate_band_stats.StatisticsCalculationConfig.fill_value: float</code>  <code>property</code>","text":"<p>Resolved fill value for masking corrupted pixels.</p>"},{"location":"api_ref/data_prep/calculate_band_stats/#kelp.data_prep.calculate_band_stats.calculate_band_statistics","title":"<code>kelp.data_prep.calculate_band_stats.calculate_band_statistics</code>","text":"<p>Runs statistics calculation for specified images.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[Path]</code> <p>The input image paths.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required <code>mask_using_qa</code> <code>bool</code> <p>A flag indicating whether the corrupted pixels should be masked using QA band.</p> <code>False</code> <code>mask_using_water_mask</code> <code>bool</code> <p>A flag indicating whether the corrupted pixels should be masked using Water Mask.</p> <code>False</code> <code>fill_value</code> <code>float</code> <p>The fill value to use for corrupted pixels.</p> <code>0</code> Source code in <code>kelp/data_prep/calculate_band_stats.py</code> <pre><code>@torch.inference_mode()\ndef calculate_band_statistics(\n    image_paths: List[Path],\n    output_dir: Path,\n    mask_using_qa: bool = False,\n    mask_using_water_mask: bool = False,\n    fill_value: float = 0,\n) -&gt; Dict[str, Dict[str, float]]:\n    \"\"\"\n    Runs statistics calculation for specified images.\n\n    Args:\n        image_paths: The input image paths.\n        output_dir: The output directory.\n        mask_using_qa: A flag indicating whether the corrupted pixels should be masked using QA band.\n        mask_using_water_mask: A flag indicating whether the corrupted pixels should be masked using Water Mask.\n        fill_value: The fill value to use for corrupted pixels.\n\n    Returns: A dictionary with per band statistics.\n\n    \"\"\"\n    # Move computations to GPU if available\n    transform = K.AugmentationSequential(\n        AppendDEMWM(  # type: ignore\n            index_dem=BAND_INDEX_LOOKUP[\"DEM\"],\n            index_qa=BAND_INDEX_LOOKUP[\"QA\"],\n        ),\n        *[\n            append_index_transform(\n                index_swir=BAND_INDEX_LOOKUP[\"SWIR\"],\n                index_nir=BAND_INDEX_LOOKUP[\"NIR\"],\n                index_red=BAND_INDEX_LOOKUP[\"R\"],\n                index_green=BAND_INDEX_LOOKUP[\"G\"],\n                index_blue=BAND_INDEX_LOOKUP[\"B\"],\n                index_dem=BAND_INDEX_LOOKUP[\"DEM\"],\n                index_qa=BAND_INDEX_LOOKUP[\"QA\"],\n                index_water_mask=BAND_INDEX_LOOKUP[\"DEMWM\"],\n                mask_using_qa=False if index_name.endswith(\"WM\") else mask_using_qa,\n                mask_using_water_mask=False if index_name.endswith(\"WM\") else mask_using_water_mask,\n                fill_val=torch.nan,\n            )\n            for index_name, append_index_transform in SPECTRAL_INDEX_LOOKUP.items()\n            if index_name != \"DEMWM\"\n        ],\n        data_keys=[\"input\"],\n    ).to(DEVICE)\n\n    # Initialize statistics arrays\n    band_names = BASE_BANDS + [index_name for index_name in SPECTRAL_INDEX_LOOKUP.keys() if index_name != \"DEMWM\"]\n    num_bands = len(band_names)\n    min_per_band = torch.full((num_bands,), float(\"inf\")).to(DEVICE)\n    max_per_band = torch.full((num_bands,), float(\"-inf\")).to(DEVICE)\n    sum_per_band = torch.zeros(num_bands).to(DEVICE)\n    sum_sq_per_band = torch.zeros(num_bands).to(DEVICE)\n    q01_items = []\n    q99_items = []\n    total_pixels = 0\n\n    for image_path in tqdm(image_paths, desc=\"Calculating band statistics\"):\n        # Open the image and convert to numpy array\n        src: rasterio.DatasetReader\n        with rasterio.open(image_path) as src:\n            image_arr = src.read()\n            # Convert image to PyTorch tensor\n            image = torch.from_numpy(image_arr).float().to(DEVICE).unsqueeze(0)\n            # Mask missing pixels\n            image = torch.where(image == -32768.0, fill_value, image)\n\n        image = transform(image).squeeze()\n\n        # Assuming the image has shape (num_bands, height, width)\n        if image.shape[0] != num_bands:\n            raise ValueError(f\"Image at {image_path} does not have {num_bands} bands\")\n\n        # Update min and max\n        current_image_min = torch.amin(image, dim=(1, 2))\n        current_image_min = torch.where(torch.isnan(current_image_min), min_per_band, current_image_min)\n        current_image_max = torch.amax(image, dim=(1, 2))\n        current_image_max = torch.where(torch.isnan(current_image_max), max_per_band, current_image_max)\n        min_per_band = torch.minimum(min_per_band, current_image_min)\n        max_per_band = torch.maximum(max_per_band, current_image_max)\n\n        # Update sum and sum of squares for mean and std calculation\n        sum_per_band += torch.nansum(image, dim=(1, 2))\n        sum_sq_per_band += torch.nansum(image**2, dim=(1, 2))\n\n        # Update total pixel count\n        total_pixels += image.shape[1] * image.shape[2]\n\n        # Append quantile values\n        q01_per_band = torch.nanquantile(image.view(image.shape[0], -1), 0.01, dim=1)\n        q99_per_band = torch.nanquantile(image.view(image.shape[0], -1), 0.99, dim=1)\n        q01_items.append(q01_per_band)\n        q99_items.append(q99_per_band)\n\n    # Calculate mean and standard deviation\n    mean_per_band = sum_per_band / total_pixels\n    std_per_band = torch.sqrt(sum_sq_per_band / total_pixels - mean_per_band**2)\n    mean_q01_per_band = torch.nanmean(torch.stack(q01_items), dim=0)\n    mean_q99_per_band = torch.nanmean(torch.stack(q99_items), dim=0)\n\n    stats = {\n        band_name: {\n            \"mean\": mean_per_band[idx].item(),\n            \"std\": std_per_band[idx].item(),\n            \"min\": min_per_band[idx].item(),\n            \"max\": max_per_band[idx].item(),\n            \"q01\": mean_q01_per_band[idx].item(),\n            \"q99\": mean_q99_per_band[idx].item(),\n        }\n        for idx, band_name in enumerate(band_names)\n    }\n\n    # Adjust stats for binary band\n    for band, band_stats in stats.items():\n        if band.endswith(\"WM\") or band == \"QA\":\n            band_stats[\"min\"] = 0.0\n            band_stats[\"max\"] = 1.0\n            band_stats[\"mean\"] = 0.0\n            band_stats[\"std\"] = 1.0\n            band_stats[\"q01\"] = 0.0\n            band_stats[\"q99\"] = 1.0\n\n    stats_str = json.dumps(stats, indent=4)\n    _logger.info(\"Per band statistics calculated. Review and adjust!\")\n    _logger.info(stats_str)\n    now = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S\")\n    (output_dir / f\"{now}-stats-{fill_value=}-{mask_using_qa=}-{mask_using_water_mask=}.json\").write_text(stats_str)\n\n    return stats\n</code></pre>"},{"location":"api_ref/data_prep/calculate_band_stats/#kelp.data_prep.calculate_band_stats.main","title":"<code>kelp.data_prep.calculate_band_stats.main</code>","text":"<p>Main entry point for band statistics calculation.</p> Source code in <code>kelp/data_prep/calculate_band_stats.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"\n    Main entry point for band statistics calculation.\n    \"\"\"\n    cfg = parse_args()\n    calculate_band_statistics(\n        image_paths=cfg.file_paths,\n        output_dir=cfg.output_dir,\n        mask_using_qa=cfg.mask_using_qa,\n        mask_using_water_mask=cfg.mask_using_water_mask,\n        fill_value=cfg.fill_value,\n    )\n</code></pre>"},{"location":"api_ref/data_prep/calculate_band_stats/#kelp.data_prep.calculate_band_stats.parse_args","title":"<code>kelp.data_prep.calculate_band_stats.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of StatisticsCalculationConfig.</p> Source code in <code>kelp/data_prep/calculate_band_stats.py</code> <pre><code>def parse_args() -&gt; StatisticsCalculationConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of StatisticsCalculationConfig.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--data_dir\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--mask_using_qa\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--mask_using_water_mask\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--fill_missing_pixels_with_torch_nan\",\n        action=\"store_true\",\n    )\n    args = parser.parse_args()\n    cfg = StatisticsCalculationConfig(**vars(args))\n    cfg.log_self()\n    return cfg\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/","title":"dataset_prep","text":"<p>Pixel-level dataset prep logic for training XGBoost or other tree-based models.</p>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.DataPrepConfig","title":"<code>kelp.data_prep.dataset_prep.DataPrepConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>A Config class for running pixel-level dataset prep step.</p> Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>class DataPrepConfig(ConfigBase):\n    \"\"\"A Config class for running pixel-level dataset prep step.\"\"\"\n\n    data_dir: Path\n    output_dir: Path\n    metadata_fp: Path\n    train_size: float = 0.98\n    test_size: float = 0.5\n    buffer_pixels: int = 10\n    random_sample_pixel_frac: float = 0.02\n    seed: int = 42\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.append_indices","title":"<code>kelp.data_prep.dataset_prep.append_indices</code>","text":"<p>Appends spectral indices to the dataframe as extra columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame with pixel level values.</p> required Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>@torch.inference_mode()\ndef append_indices(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Appends spectral indices to the dataframe as extra columns.\n\n    Args:\n        df: A DataFrame with pixel level values.\n\n    Returns: A DataFrame with appended spectral indices.\n\n    \"\"\"\n    arr = df.values\n    x = torch.tensor(arr, dtype=torch.float32, device=DEVICE)\n    x = x.reshape(x.size(0), x.size(1), 1, 1)\n    x = _transforms(x).squeeze()\n    df = pd.DataFrame(x.detach().cpu().numpy(), columns=df.columns.tolist() + list(SPECTRAL_INDEX_LOOKUP.keys()))\n    df = df.replace({v: -32768.0 for v in [np.nan, np.inf, -np.inf]})\n    return df\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.extract_labels","title":"<code>kelp.data_prep.dataset_prep.extract_labels</code>","text":"<p>Extracts pixel level values from all original bands and extra spectral indices for all files in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The data directory.</p> required <code>metadata</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required <code>buffer_pixels</code> <code>int</code> <p>The buffer size in pixels around the Kelp Forest masks.</p> <code>10</code> <code>random_sample_pixel_frac</code> <code>float</code> <p>The fraction of randomly sampled pixels from the images.</p> <code>0.02</code> Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>@timed\ndef extract_labels(\n    data_dir: Path,\n    metadata: pd.DataFrame,\n    buffer_pixels: int = 10,\n    random_sample_pixel_frac: float = 0.02,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Extracts pixel level values from all original bands\n    and extra spectral indices for all files in the specified directory.\n\n    Args:\n        data_dir: The data directory.\n        metadata: The metadata dataframe.\n        buffer_pixels: The buffer size in pixels around the Kelp Forest masks.\n        random_sample_pixel_frac: The fraction of randomly sampled pixels from the images.\n\n    Returns: A pixel level values dataframe.\n\n    \"\"\"\n    frames = []\n    metadata = metadata[metadata[\"original_split\"] == \"train\"]\n    for _, row in tqdm(metadata.iterrows(), desc=\"Processing files\", total=len(metadata)):\n        frames.append(\n            process_single_file(\n                data_dir=data_dir,\n                tile_id=row[\"tile_id\"],\n                buffer_pixels=buffer_pixels,\n                random_sample_pixel_fraction=random_sample_pixel_frac,\n            )\n        )\n    df = pd.concat(frames, ignore_index=True)\n    return df\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.load_data","title":"<code>kelp.data_prep.dataset_prep.load_data</code>","text":"<p>Loads data from specified metadata parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_fp</code> <code>Path</code> <p>The path to the metadata parquet file.</p> required Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>def load_data(metadata_fp: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads data from specified metadata parquet file.\n\n    Args:\n        metadata_fp: The path to the metadata parquet file.\n\n    Returns: A pandas DataFrame.\n\n    \"\"\"\n    metadata = pd.read_parquet(metadata_fp).rename(columns={\"split\": \"original_split\"})\n    metadata = metadata[metadata[\"high_kelp_pixels_pct\"].isin([False, None])]\n    return metadata\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.main","title":"<code>kelp.data_prep.dataset_prep.main</code>","text":"<p>Main entrypoint for the pixel level dataset preparation.</p> Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for the pixel level dataset preparation.\"\"\"\n    cfg = parse_args()\n    np.random.seed(cfg.seed)\n    random.seed(cfg.seed)\n\n    metadata = load_data(cfg.metadata_fp)\n\n    ds = prepare_dataset(\n        data_dir=cfg.data_dir,\n        metadata=metadata,\n        train_size=cfg.train_size,\n        test_size=cfg.test_size,\n        buffer_pixels=cfg.buffer_pixels,\n        random_sample_pixel_frac=cfg.random_sample_pixel_frac,\n        seed=cfg.seed,\n    )\n    save_data(output_dir=cfg.output_dir, df=ds)\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.parse_args","title":"<code>kelp.data_prep.dataset_prep.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of DataPrepConfig.</p> Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>def parse_args() -&gt; DataPrepConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of DataPrepConfig.\n\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument(\"--data_dir\", type=Path, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    parser.add_argument(\"--metadata_fp\", type=str, required=True)\n    parser.add_argument(\"--buffer_pixels\", type=int, default=10)\n    parser.add_argument(\"--random_sample_pixel_frac\", type=float, default=0.05)\n    parser.add_argument(\"--train_size\", type=float, default=0.98)\n    parser.add_argument(\"--test_size\", type=float, default=0.5)\n    parser.add_argument(\"--seed\", type=int, default=42)\n    args = parser.parse_args()\n    cfg = DataPrepConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(parents=True, exist_ok=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.prepare_dataset","title":"<code>kelp.data_prep.dataset_prep.prepare_dataset</code>","text":"<p>Runs pixel level dataset generation and splits the data into training and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The data directory with input images.</p> required <code>metadata</code> <code>DataFrame</code> <p>The metadata dataframe containing tile IDs.</p> required <code>train_size</code> <code>float</code> <p>The size of the training dataset.</p> <code>0.98</code> <code>test_size</code> <code>float</code> <p>The size of the test dataset.</p> <code>0.5</code> <code>buffer_pixels</code> <code>int</code> <p>The buffer size in pixels around the Kelp Forest masks.</p> <code>10</code> <code>random_sample_pixel_frac</code> <code>float</code> <p>The fraction of randomly sampled pixels from the images.</p> <code>0.02</code> <code>seed</code> <code>int</code> <p>The seed for reproducibility.</p> <code>42</code> Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>@timed\ndef prepare_dataset(\n    data_dir: Path,\n    metadata: pd.DataFrame,\n    train_size: float = 0.98,\n    test_size: float = 0.5,\n    buffer_pixels: int = 10,\n    random_sample_pixel_frac: float = 0.02,\n    seed: int = 42,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Runs pixel level dataset generation and splits the data into training and test sets.\n\n    Args:\n        data_dir: The data directory with input images.\n        metadata: The metadata dataframe containing tile IDs.\n        train_size: The size of the training dataset.\n        test_size: The size of the test dataset.\n        buffer_pixels: The buffer size in pixels around the Kelp Forest masks.\n        random_sample_pixel_frac: The fraction of randomly sampled pixels from the images.\n        seed: The seed for reproducibility.\n\n    Returns: A pixel level values dataframe.\n\n    \"\"\"\n    df = extract_labels(\n        data_dir=data_dir,\n        metadata=metadata,\n        buffer_pixels=buffer_pixels,\n        random_sample_pixel_frac=random_sample_pixel_frac,\n    )\n    df = split_data(\n        df=df,\n        train_size=train_size,\n        test_size=test_size,\n        seed=seed,\n    )\n    return df\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.process_single_file","title":"<code>kelp.data_prep.dataset_prep.process_single_file</code>","text":"<p>Extracts pixel level values from all bands and extra spectral indices for specified Tile ID.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required <code>tile_id</code> <code>str</code> <p>The Tile ID.</p> required <code>buffer_pixels</code> <code>int</code> <p>The number of pixels to use for mask buffering.</p> <code>10</code> <code>random_sample_pixel_fraction</code> <code>float</code> <p>The fraction of randomly sampled pixels.</p> <code>0.05</code> Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>def process_single_file(\n    data_dir: Path,\n    tile_id: str,\n    buffer_pixels: int = 10,\n    random_sample_pixel_fraction: float = 0.05,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Extracts pixel level values from all bands and extra spectral indices for specified Tile ID.\n\n    Args:\n        data_dir: The path to the data directory.\n        tile_id: The Tile ID.\n        buffer_pixels: The number of pixels to use for mask buffering.\n        random_sample_pixel_fraction: The fraction of randomly sampled pixels.\n\n    Returns: A DataFrame with pixel level band and extra spectral indices values.\n\n    \"\"\"\n    input_fp = data_dir / \"images\" / f\"{tile_id}_satellite.tif\"\n    mask_fp = data_dir / \"masks\" / f\"{tile_id}_kelp.tif\"\n\n    # Load input image file with rasterio\n    with rasterio.open(input_fp) as src:\n        input_image = src.read()\n\n    # Load mask file with rasterio\n    with rasterio.open(mask_fp) as src:\n        mask_image = src.read(1)\n\n    # Extract positive class pixels from mask\n    positive_class_pixels = np.where(mask_image == 1)\n\n    # Buffer mask array\n    buffered_mask = dilation(mask_image, square(1 + 2 * buffer_pixels)) - mask_image\n\n    # Append buffer only pixels to positive class pixels\n    buffer_only_pixels = np.where(buffered_mask == 1)\n    all_pixels = (\n        np.hstack([positive_class_pixels[0], buffer_only_pixels[0]]),\n        np.hstack([positive_class_pixels[1], buffer_only_pixels[1]]),\n    )\n\n    # Create a set of all positive class and buffer pixels to avoid duplication\n    existing_pixels = set(zip(all_pixels[0], all_pixels[1]))\n\n    # Sample random percentage of pixels from the image ensuring no duplication\n    total_pixels = input_image.shape[1] * input_image.shape[2]\n    sample_size = int(total_pixels * random_sample_pixel_fraction)\n    random_pixels: Set[Tuple[int, int]] = set()\n    while len(random_pixels) &lt; sample_size:\n        rand_row = np.random.randint(0, input_image.shape[1])\n        rand_col = np.random.randint(0, input_image.shape[2])\n        if (rand_row, rand_col) not in existing_pixels:\n            random_pixels.add((rand_row, rand_col))\n\n    # Convert set of tuples to numpy arrays for row and col indices\n    random_pixel_indices = np.array(list(random_pixels))\n    random_rows, random_cols = random_pixel_indices[:, 0], random_pixel_indices[:, 1]\n\n    # Append random sample of pixels to the pixel array\n    all_pixels = (np.hstack([all_pixels[0], random_rows]), np.hstack([all_pixels[1], random_cols]))\n\n    # Create a dataframe of shape NxC where N - number of samples, C - number of channels\n    pixel_values = input_image[:, all_pixels[0], all_pixels[1]].T\n    mask_values = mask_image[all_pixels[0], all_pixels[1]]\n    df = pd.DataFrame(pixel_values, columns=consts.data.ORIGINAL_BANDS)\n    df = append_indices(df)\n    df[\"label\"] = mask_values\n    df[\"tile_id\"] = tile_id\n    return df\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.save_data","title":"<code>kelp.data_prep.dataset_prep.save_data</code>","text":"<p>Saves pixel level dataset in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to be saved.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory, where the data is saved.</p> required Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>@timed\ndef save_data(df: pd.DataFrame, output_dir: Path) -&gt; None:\n    \"\"\"\n    Saves pixel level dataset in the specified directory.\n\n    Args:\n        df: The dataframe to be saved.\n        output_dir: The output directory, where the data is saved.\n\n    \"\"\"\n    _logger.info(f\"Saving data to {output_dir}. Generated {len(df):,} rows.\")\n    df.to_parquet(output_dir / \"train_val_test_pixel_level_dataset.parquet\", index=False)\n</code></pre>"},{"location":"api_ref/data_prep/dataset_prep/#kelp.data_prep.dataset_prep.split_data","title":"<code>kelp.data_prep.dataset_prep.split_data</code>","text":"<p>Runs random split on the pixel level dataset.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pixel level dataset to be split.</p> required <code>train_size</code> <code>float</code> <p>The size of the training dataset.</p> <code>0.98</code> <code>test_size</code> <code>float</code> <p>The size of the test dataset.</p> <code>0.5</code> <code>seed</code> <code>int</code> <p>The random seed for reproducibility.</p> <code>42</code> Source code in <code>kelp/data_prep/dataset_prep.py</code> <pre><code>@timed\ndef split_data(\n    df: pd.DataFrame,\n    train_size: float = 0.98,\n    test_size: float = 0.5,\n    seed: int = 42,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Runs random split on the pixel level dataset.\n\n    Args:\n        df: The pixel level dataset to be split.\n        train_size: The size of the training dataset.\n        test_size: The size of the test dataset.\n        seed: The random seed for reproducibility.\n\n    Returns: A dataframe with splits.\n\n    \"\"\"\n    X_train, X_test = train_test_split(\n        df,\n        train_size=train_size,\n        shuffle=True,\n        stratify=df[\"label\"],\n        random_state=seed,\n    )\n    X_val, X_test = train_test_split(\n        X_test,\n        test_size=test_size,\n        shuffle=True,\n        stratify=X_test[\"label\"],\n        random_state=seed,\n    )\n    X_train[\"split\"] = \"train\"\n    X_val[\"split\"] = \"val\"\n    X_test[\"split\"] = \"test\"\n    return pd.concat([X_train, X_val, X_test], ignore_index=True)\n</code></pre>"},{"location":"api_ref/data_prep/eda/","title":"eda","text":"<p>EDA logic.</p>"},{"location":"api_ref/data_prep/eda/#kelp.data_prep.eda.SatelliteImageStats","title":"<code>kelp.data_prep.eda.SatelliteImageStats</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A data class for holding stats for single satellite image.</p> Source code in <code>kelp/data_prep/eda.py</code> <pre><code>class SatelliteImageStats(BaseModel):\n    \"\"\"\n    A data class for holding stats for single satellite image.\n    \"\"\"\n\n    tile_id: str\n    aoi_id: Optional[int] = None\n    split: str\n\n    has_kelp: Optional[bool] = None\n    non_kelp_pixels: Optional[int] = None\n    kelp_pixels: Optional[int] = None\n    kelp_pixels_pct: Optional[float] = None\n    high_kelp_pixels_pct: Optional[bool] = None\n\n    dem_nan_pixels: int\n    dem_has_nans: bool\n    dem_nan_pixels_pct: Optional[float] = None\n\n    dem_zero_pixels: int\n    dem_zero_pixels_pct: Optional[float] = None\n\n    water_pixels: Optional[int] = None\n    water_pixels_pct: Optional[float] = None\n    almost_all_water: bool\n\n    qa_corrupted_pixels: Optional[int] = None\n    qa_ok: bool\n    qa_corrupted_pixels_pct: Optional[float] = None\n    high_corrupted_pixels_pct: Optional[bool] = None\n</code></pre>"},{"location":"api_ref/data_prep/eda/#kelp.data_prep.eda.build_tile_id_aoi_id_and_split_tuples","title":"<code>kelp.data_prep.eda.build_tile_id_aoi_id_and_split_tuples</code>","text":"<p>Builds a list of tile ID, AOI ID and split tuples from specified metadata dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required Source code in <code>kelp/data_prep/eda.py</code> <pre><code>def build_tile_id_aoi_id_and_split_tuples(metadata: pd.DataFrame) -&gt; List[Tuple[str, int, str]]:\n    \"\"\"\n    Builds a list of tile ID, AOI ID and split tuples from specified metadata dataframe.\n\n    Args:\n        metadata: The metadata dataframe.\n\n    Returns: A list of tile ID, AOI ID and split tuples.\n\n    \"\"\"\n    records = []\n    metadata[\"split\"] = metadata[\"in_train\"].apply(lambda x: \"train\" if x else \"test\")\n    for _, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Extracting tile_id and split tuples\"):\n        if row[\"type\"] == \"kelp\":\n            continue\n        records.append((row[\"tile_id\"], row[\"aoi_id\"], row[\"split\"]))\n    return records\n</code></pre>"},{"location":"api_ref/data_prep/eda/#kelp.data_prep.eda.calculate_stats","title":"<code>kelp.data_prep.eda.calculate_stats</code>","text":"<p>Calculates statistics for single tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile_id_aoi_id_split_tuple</code> <code>Tuple[str, int, str]</code> <p>A tuple with tile ID, AOI ID and split name.</p> required <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required Source code in <code>kelp/data_prep/eda.py</code> <pre><code>def calculate_stats(tile_id_aoi_id_split_tuple: Tuple[str, int, str], data_dir: Path) -&gt; SatelliteImageStats:\n    \"\"\"\n    Calculates statistics for single tile.\n\n    Args:\n        tile_id_aoi_id_split_tuple: A tuple with tile ID, AOI ID and split name.\n        data_dir: The path to the data directory.\n\n    Returns: An instance of SatelliteImageStats class.\n\n    \"\"\"\n    tile_id, aoi_id, split = tile_id_aoi_id_split_tuple\n    src: rasterio.DatasetReader\n    with rasterio.open(data_dir / split / \"images\" / f\"{tile_id}_satellite.tif\") as src:\n        input_arr = src.read()\n        dem_band = input_arr[6]\n        qa_band = input_arr[5]\n        all_pixels = np.prod(qa_band.shape)\n        dem_nan_pixels = np.where(dem_band &lt; 0, 1, 0).sum()\n        dem_zero_pixels = np.where(dem_band == 0, 1, 0).sum()\n        dem_zero_pixels_pct = dem_zero_pixels / all_pixels.item()\n        dem_nan_pixels_pct = dem_nan_pixels / all_pixels.item()\n        water_pixels = np.where(dem_band &lt;= 0, 1, 0).sum()\n        water_pixels_pct = water_pixels / all_pixels.item()\n        almost_all_water = water_pixels_pct &gt; HIGH_DEM_ZERO_OR_NAN_PCT_THRESHOLD\n        dem_has_nans = dem_nan_pixels &gt; 0\n        nan_vals = qa_band.sum()\n        qa_ok = nan_vals == 0\n        qa_corrupted_pixels = nan_vals.item()\n        qa_corrupted_pixels_pct = nan_vals.item() / all_pixels.item()\n        high_corrupted_pixels_pct = qa_corrupted_pixels_pct &gt; HIGH_CORRUPTION_PCT_THRESHOLD\n\n    if split != \"test\":\n        with rasterio.open(data_dir / split / \"masks\" / f\"{tile_id}_kelp.tif\") as src:\n            target_arr: np.ndarray = src.read()  # type: ignore[type-arg]\n            kelp_pixels = target_arr.sum()\n            non_kelp_pixels = np.prod(target_arr.shape) - kelp_pixels\n            has_kelp = kelp_pixels &gt; 0\n            kelp_pixels_pct = kelp_pixels.item() / all_pixels.item()\n            high_kelp_pixels_pct = kelp_pixels_pct &gt; HIGH_KELP_PCT_THRESHOLD\n    else:\n        kelp_pixels = None\n        has_kelp = None\n        non_kelp_pixels = None\n        kelp_pixels_pct = None\n        high_kelp_pixels_pct = None\n\n    return SatelliteImageStats(\n        tile_id=tile_id,\n        aoi_id=None if np.isnan(aoi_id) else aoi_id,\n        split=split,\n        has_kelp=has_kelp,\n        kelp_pixels=kelp_pixels,\n        kelp_pixels_pct=kelp_pixels_pct,\n        high_kelp_pixels_pct=high_kelp_pixels_pct,\n        non_kelp_pixels=non_kelp_pixels,\n        dem_has_nans=dem_has_nans,\n        dem_nan_pixels=dem_nan_pixels,\n        dem_nan_pixels_pct=dem_nan_pixels_pct,\n        dem_zero_pixels=dem_zero_pixels,\n        dem_zero_pixels_pct=dem_zero_pixels_pct,\n        water_pixels=water_pixels,\n        water_pixels_pct=water_pixels_pct,\n        almost_all_water=almost_all_water,\n        qa_ok=qa_ok,\n        qa_corrupted_pixels=qa_corrupted_pixels,\n        qa_corrupted_pixels_pct=qa_corrupted_pixels_pct,\n        high_corrupted_pixels_pct=high_corrupted_pixels_pct,\n    )\n</code></pre>"},{"location":"api_ref/data_prep/eda/#kelp.data_prep.eda.extract_stats","title":"<code>kelp.data_prep.eda.extract_stats</code>","text":"<p>Runs stats extraction from images in specified directory in parallel using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the directory.</p> required <code>records</code> <code>List[Tuple[str, int, str]]</code> <p>The list of tuples with tile ID, AOI ID and split per image.</p> required Source code in <code>kelp/data_prep/eda.py</code> <pre><code>@timed\ndef extract_stats(data_dir: Path, records: List[Tuple[str, int, str]]) -&gt; List[SatelliteImageStats]:\n    \"\"\"\n    Runs stats extraction from images in specified directory in parallel using Dask.\n\n    Args:\n        data_dir: The path to the directory.\n        records: The list of tuples with tile ID, AOI ID and split per image.\n\n    Returns: A list of SatelliteImageStats instances.\n\n    \"\"\"\n    return (  # type: ignore[no-any-return]\n        dask.bag.from_sequence(records).map(calculate_stats, data_dir=data_dir).compute()\n    )\n</code></pre>"},{"location":"api_ref/data_prep/eda/#kelp.data_prep.eda.main","title":"<code>kelp.data_prep.eda.main</code>","text":"<p>Main entry point for performing EDA.</p> Source code in <code>kelp/data_prep/eda.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for performing EDA.\"\"\"\n    cfg = parse_args()\n    metadata = pd.read_parquet(cfg.metadata_fp)\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    records = build_tile_id_aoi_id_and_split_tuples(metadata)\n\n    with distributed.LocalCluster(n_workers=8, threads_per_worker=1) as cluster, distributed.Client(cluster) as client:\n        _logger.info(f\"Running dask cluster dashboard on {client.dashboard_link}\")\n        stats_records = extract_stats(cfg.data_dir, records)\n        stats_df = pd.DataFrame([record.model_dump() for record in stats_records])\n        plot_stats(stats_df, output_dir=cfg.output_dir)\n</code></pre>"},{"location":"api_ref/data_prep/eda/#kelp.data_prep.eda.plot_stats","title":"<code>kelp.data_prep.eda.plot_stats</code>","text":"<p>Plots statistics about the training dataset.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe with image statistics.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory, where plots will be saved.</p> required Source code in <code>kelp/data_prep/eda.py</code> <pre><code>@timed\ndef plot_stats(df: pd.DataFrame, output_dir: Path) -&gt; None:\n    \"\"\"\n    Plots statistics about the training dataset.\n\n    Args:\n        df: The dataframe with image statistics.\n        output_dir: The output directory, where plots will be saved.\n\n    \"\"\"\n    out_dir = output_dir / \"stats\"\n    out_dir.mkdir(exist_ok=True, parents=True)\n\n    df = df.replace({None: np.nan})\n    df.to_parquet(out_dir / \"dataset_stats.parquet\", index=False)\n\n    # Descriptive statistics for numerical columns\n    desc_stats = df.describe()\n    desc_stats.reset_index(names=\"stats\").to_parquet(out_dir / \"desc_stats.parquet\")\n\n    # Distribution of data in the train and test splits\n    split_distribution = df[\"split\"].value_counts()\n    split_distribution.to_frame(name=\"value_count\").to_parquet(out_dir / \"split_distribution.parquet\")\n\n    # Summary\n    _logger.info(\"desc_stats:\")\n    _logger.info(desc_stats)\n    _logger.info(\"split_distribution\")\n    _logger.info(split_distribution)\n\n    # Quality Analysis - Proportion of tiles with QA issues\n    qa_issues_proportion = df[\"qa_ok\"].value_counts(normalize=True)\n    qa_issues_proportion.to_frame(name=\"value_count\").to_parquet(out_dir / \"qa_issues_proportion.parquet\")\n\n    # Kelp Presence Analysis - Balance between kelp and non-kelp tiles\n    kelp_presence_proportion = df[\"has_kelp\"].value_counts(normalize=True)\n    kelp_presence_proportion.to_frame(name=\"value_count\").to_parquet(out_dir / \"kelp_presence_proportion.parquet\")\n\n    # Results\n    _logger.info(\"qa_issues_proportion:\")\n    _logger.info(qa_issues_proportion)\n    _logger.info(\"kelp_presence_proportion:\")\n    _logger.info(kelp_presence_proportion)\n\n    # Correlation analysis\n    correlation_matrix = df[[\"kelp_pixels\", \"non_kelp_pixels\", \"qa_corrupted_pixels\", \"dem_nan_pixels\"]].corr()\n\n    # Visualization of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n    plt.title(\"Correlation Matrix\")\n    plt.savefig(out_dir / \"corr_matrix.png\")\n    plt.close()\n\n    # Additional Visualizations\n    # Distribution of kelp pixels\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df[\"kelp_pixels\"], bins=50, kde=True)\n    plt.title(\"Distribution of Kelp pixels\")\n    plt.xlabel(\"Number of Kelp pixels\")\n    plt.ylabel(\"Frequency\")\n    plt.savefig(out_dir / \"kelp_pixels_distribution.png\")\n    plt.close()\n\n    # Distribution of dem_nan_pixels\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df[\"dem_nan_pixels\"], bins=50, kde=True)\n    plt.title(\"Distribution of DEM NaN pixels\")\n    plt.xlabel(\"Number of DEM NaN pixels\")\n    plt.ylabel(\"Frequency\")\n    plt.savefig(out_dir / \"dem_nan_pixels_distribution.png\")\n    plt.close()\n\n    # Image count per split (train/test)\n    plt.figure(figsize=(8, 6))\n    sns.countplot(x=\"split\", data=df)\n    plt.title(\"Image count per split\")\n    plt.xlabel(\"Split\")\n    plt.ylabel(\"Count\")\n    plt.savefig(out_dir / \"splits.png\")\n    plt.close()\n\n    # Image count with and without kelp forest class\n    plt.figure(figsize=(8, 6))\n    sns.countplot(x=\"has_kelp\", data=df)\n    plt.title(\"Image count with and without Kelp Forest\")\n    plt.xlabel(\"Has Kelp\")\n    plt.ylabel(\"Count\")\n    plt.savefig(out_dir / \"has_kelp.png\")\n    plt.close()\n\n    # Image count with and without QA issues\n    plt.figure(figsize=(8, 6))\n    sns.countplot(x=\"qa_ok\", data=df)\n    plt.title(\"Image count with and without QA issues\")\n    plt.xlabel(\"QA OK\")\n    plt.ylabel(\"Count\")\n    plt.savefig(out_dir / \"qa_ok.png\")\n    plt.close()\n\n    # Image count with and without NaN values in DEM band\n    plt.figure(figsize=(8, 6))\n    sns.countplot(x=\"high_corrupted_pixels_pct\", data=df)\n    plt.title(\"Image count with and without high corrupted pixel percentage\")\n    plt.xlabel(\"High corrupted pixel percentage\")\n    plt.ylabel(\"Count\")\n    plt.savefig(out_dir / \"qa_corrupted_pixels_pct.png\")\n    plt.close()\n\n    # Image count with and without NaN values in DEM band\n    plt.figure(figsize=(8, 6))\n    sns.countplot(x=\"dem_has_nans\", data=df)\n    plt.title(\"Image Count with and Without NaN Values in DEM Band\")\n    plt.xlabel(\"DEM Has NaNs\")\n    plt.ylabel(\"Count\")\n    plt.savefig(out_dir / \"dem_has_nans.png\")\n    plt.close()\n\n    # Image count with and without NaN values in DEM band\n    plt.figure(figsize=(8, 6))\n    sns.countplot(x=\"high_kelp_pixels_pct\", data=df)\n    plt.title(\"Image count with and without high percent of Kelp pixels\")\n    plt.xlabel(\"Mask high kelp pixel percentage\")\n    plt.ylabel(\"Count\")\n    plt.savefig(out_dir / \"high_kelp_pixels_pct.png\")\n    plt.close()\n\n    # Image count with and without NaN values in DEM band\n    df.groupby(\"aoi_id\").size()\n    plt.figure(figsize=(8, 6))\n    sns.countplot(x=\"aoi_id\", data=df)\n    plt.title(\"Image count with and without high percent of Kelp pixels\")\n    plt.xlabel(\"Mask high kelp pixel percentage\")\n    plt.ylabel(\"Count\")\n    plt.savefig(out_dir / \"kelp_high_pct.png\")\n    plt.close()\n\n    # Image count per AOI\n    counts = df.groupby(\"aoi_id\").size().reset_index().rename(columns={0: \"count\"})\n    plt.figure(figsize=(10, 6))\n    sns.histplot(counts[\"count\"], bins=35, kde=True)\n    plt.title(\"Distribution of images per AOI\")\n    plt.xlabel(\"Number of images per AOI\")\n    plt.ylabel(\"Frequency\")\n    plt.savefig(out_dir / \"aoi_images_distribution.png\")\n    plt.close()\n\n    # Images per AOI without AOIs that have single image\n    counts = counts[counts[\"count\"] &gt; 1]\n    plt.figure(figsize=(10, 6))\n    sns.histplot(counts[\"count\"], bins=35, kde=True)\n    plt.title(\"Distribution of images per AOI (without singles)\")\n    plt.xlabel(\"Number of images per AOI\")\n    plt.ylabel(\"Frequency\")\n    plt.savefig(out_dir / \"aoi_images_distribution_filtered.png\")\n    plt.close()\n</code></pre>"},{"location":"api_ref/data_prep/move_split_files/","title":"move_split_files","text":"<p>Logic for moving split validation files to separate directories.</p>"},{"location":"api_ref/data_prep/move_split_files/#kelp.data_prep.move_split_files.MoveSplitFilesConfig","title":"<code>kelp.data_prep.move_split_files.MoveSplitFilesConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>A config for moving split files to a new directory.</p> Source code in <code>kelp/data_prep/move_split_files.py</code> <pre><code>class MoveSplitFilesConfig(ConfigBase):\n    \"\"\"A config for moving split files to a new directory.\"\"\"\n\n    data_dir: Path\n    metadata_fp: Path\n    output_dir: Path\n</code></pre>"},{"location":"api_ref/data_prep/move_split_files/#kelp.data_prep.move_split_files.main","title":"<code>kelp.data_prep.move_split_files.main</code>","text":"<p>Main entry point for moving split files.</p> Source code in <code>kelp/data_prep/move_split_files.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for moving split files.\"\"\"\n    cfg = parse_args()\n    move_split_files(data_dir=cfg.data_dir, output_dir=cfg.output_dir, metadata_fp=cfg.metadata_fp)\n</code></pre>"},{"location":"api_ref/data_prep/move_split_files/#kelp.data_prep.move_split_files.move_split_files","title":"<code>kelp.data_prep.move_split_files.move_split_files</code>","text":"<p>Move split files to a new directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>Path to the data directory.</p> required <code>output_dir</code> <code>Path</code> <p>Path to the output directory.</p> required <code>metadata_fp</code> <code>Path</code> <p>Path to the metadata parquet file.</p> required Source code in <code>kelp/data_prep/move_split_files.py</code> <pre><code>def move_split_files(data_dir: Path, output_dir: Path, metadata_fp: Path) -&gt; None:\n    \"\"\"\n    Move split files to a new directory.\n\n    Args:\n        data_dir: Path to the data directory.\n        output_dir: Path to the output directory.\n        metadata_fp: Path to the metadata parquet file.\n\n    \"\"\"\n    df = pd.read_parquet(metadata_fp)\n    split_cols = [col for col in df.columns if col.startswith(\"split_\")]\n    for split_col in tqdm(split_cols, desc=\"Moving CV split files\"):\n        val_tiles = df[df[split_col] == consts.data.VAL][\"tile_id\"].tolist()\n        out_dir_images = output_dir / split_col / \"images\"\n        out_dir_images.mkdir(exist_ok=True, parents=True)\n        out_dir_masks = output_dir / split_col / \"masks\"\n        out_dir_masks.mkdir(exist_ok=True, parents=True)\n        for tile_id in tqdm(val_tiles, desc=f\"Moving val files for {split_col}\"):\n            fname = f\"{tile_id}_satellite.tif\"\n            shutil.copy(data_dir / \"images\" / fname, out_dir_images / fname)\n            fname = f\"{tile_id}_kelp.tif\"\n            shutil.copy(data_dir / \"masks\" / fname, out_dir_masks / fname)\n</code></pre>"},{"location":"api_ref/data_prep/move_split_files/#kelp.data_prep.move_split_files.parse_args","title":"<code>kelp.data_prep.move_split_files.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of MoveSplitFilesConfig.</p> Source code in <code>kelp/data_prep/move_split_files.py</code> <pre><code>def parse_args() -&gt; MoveSplitFilesConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of MoveSplitFilesConfig.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    parser.add_argument(\"--metadata_fp\", type=str, required=True)\n    args = parser.parse_args()\n    cfg = MoveSplitFilesConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/data_prep/plot_samples/","title":"plot_samples","text":"<p>Plot samples logic.</p>"},{"location":"api_ref/data_prep/plot_samples/#kelp.data_prep.plot_samples.AnalysisConfig","title":"<code>kelp.data_prep.plot_samples.AnalysisConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>A config for plotting samples.</p> Source code in <code>kelp/data_prep/plot_samples.py</code> <pre><code>class AnalysisConfig(ConfigBase):\n    \"\"\"A config for plotting samples.\"\"\"\n\n    data_dir: Path\n    metadata_fp: Path\n    output_dir: Path\n</code></pre>"},{"location":"api_ref/data_prep/plot_samples/#kelp.data_prep.plot_samples.build_tile_id_and_split_tuples","title":"<code>kelp.data_prep.plot_samples.build_tile_id_and_split_tuples</code>","text":"<p>Builds a list of tile ID and split tuples from specified metadata dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required Source code in <code>kelp/data_prep/plot_samples.py</code> <pre><code>def build_tile_id_and_split_tuples(metadata: pd.DataFrame) -&gt; List[Tuple[str, str]]:\n    \"\"\"\n    Builds a list of tile ID and split tuples from specified metadata dataframe.\n\n    Args:\n        metadata: The metadata dataframe.\n\n    Returns: A list of tile ID and split tuples.\n\n    \"\"\"\n    records = []\n    metadata[\"split\"] = metadata[\"in_train\"].apply(lambda x: \"train\" if x else \"test\")\n    for _, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Extracting tile_id and split tuples\"):\n        if row[\"type\"] == \"kelp\":\n            continue\n        records.append((row[\"tile_id\"], row[\"split\"]))\n    return records\n</code></pre>"},{"location":"api_ref/data_prep/plot_samples/#kelp.data_prep.plot_samples.extract_composite","title":"<code>kelp.data_prep.plot_samples.extract_composite</code>","text":"<p>Extracts a band composite from given tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile_id_split_tuple</code> <code>Tuple[str, str]</code> <p>A tuple with Tile ID and split name.</p> required <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required <code>bands</code> <code>Union[int, List[int]]</code> <p>The band index or indices to create the composite.</p> required <code>name</code> <code>str</code> <p>The name of the composite.</p> required <code>output_dir</code> <code>Path</code> <p>The path to the output directory.</p> required Source code in <code>kelp/data_prep/plot_samples.py</code> <pre><code>def extract_composite(\n    tile_id_split_tuple: Tuple[str, str],\n    data_dir: Path,\n    bands: Union[int, List[int]],\n    name: str,\n    output_dir: Path,\n) -&gt; None:\n    \"\"\"\n    Extracts a band composite from given tile.\n\n    Args:\n        tile_id_split_tuple: A tuple with Tile ID and split name.\n        data_dir: The path to the data directory.\n        bands: The band index or indices to create the composite.\n        name: The name of the composite.\n        output_dir: The path to the output directory.\n\n    \"\"\"\n    tile_id, split = tile_id_split_tuple\n    src: rasterio.DatasetReader\n    with rasterio.open(data_dir / split / \"images\" / f\"{tile_id}_satellite.tif\") as src:\n        input_arr = src.read()[bands]\n    if isinstance(bands, list):\n        input_arr = np.rollaxis(input_arr, 0, 3)\n    input_arr = min_max_normalize(input_arr)\n    input_arr = (input_arr * 255).astype(np.uint8)\n    img = Image.fromarray(input_arr)\n    out_dir = output_dir / name\n    out_dir.mkdir(exist_ok=True, parents=True)\n    img.save(out_dir / f\"{tile_id}_{name}.png\")\n</code></pre>"},{"location":"api_ref/data_prep/plot_samples/#kelp.data_prep.plot_samples.extract_composites","title":"<code>kelp.data_prep.plot_samples.extract_composites</code>","text":"<p>Extracts composite images from input tiles in the specified directory in parallel using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required <code>output_dir</code> <code>Path</code> <p>The path to the output directory.</p> required <code>records</code> <code>List[Tuple[str, str]]</code> <p>The list of tile ID and split name tuples.</p> required Source code in <code>kelp/data_prep/plot_samples.py</code> <pre><code>@timed\ndef extract_composites(data_dir: Path, output_dir: Path, records: List[Tuple[str, str]]) -&gt; None:\n    \"\"\"\n    Extracts composite images from input tiles in the specified directory in parallel using Dask.\n\n    Args:\n        data_dir: The path to the data directory.\n        output_dir: The path to the output directory.\n        records: The list of tile ID and split name tuples.\n\n    \"\"\"\n    for name, bands in zip([\"tci\", \"false_color\", \"agriculture\", \"dem\"], [[2, 3, 4], [1, 2, 3], [0, 1, 2], 6]):\n        if name != \"dem\":\n            continue\n        _logger.info(f\"Extracting {name} composites\")\n        (\n            dask.bag.from_sequence(records)\n            .map(extract_composite, data_dir=data_dir, output_dir=output_dir, name=name, bands=bands)\n            .compute()\n        )\n</code></pre>"},{"location":"api_ref/data_prep/plot_samples/#kelp.data_prep.plot_samples.main","title":"<code>kelp.data_prep.plot_samples.main</code>","text":"<p>The main entrypoint for plotting the input samples.</p> Source code in <code>kelp/data_prep/plot_samples.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"The main entrypoint for plotting the input samples.\"\"\"\n    cfg = parse_args()\n    metadata = pd.read_csv(cfg.metadata_fp)\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    records = build_tile_id_and_split_tuples(metadata)\n\n    with distributed.LocalCluster(n_workers=8, threads_per_worker=1) as cluster, distributed.Client(cluster) as client:\n        _logger.info(f\"Running dask cluster dashboard on {client.dashboard_link}\")\n        extract_composites(cfg.data_dir, cfg.output_dir, records)\n        plot_samples(cfg.data_dir, cfg.output_dir, records)\n</code></pre>"},{"location":"api_ref/data_prep/plot_samples/#kelp.data_prep.plot_samples.parse_args","title":"<code>kelp.data_prep.plot_samples.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of AnalysisConfig.</p> Source code in <code>kelp/data_prep/plot_samples.py</code> <pre><code>def parse_args() -&gt; AnalysisConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of AnalysisConfig.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--data_dir\",\n        help=\"Path to the data\",\n        required=True,\n        type=str,\n    )\n    parser.add_argument(\n        \"--metadata_fp\",\n        help=\"Path to the metadata CSV file\",\n        required=True,\n        type=str,\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        help=\"Path to the output directory\",\n        required=True,\n        type=str,\n    )\n    args = parser.parse_args()\n    cfg = AnalysisConfig(**vars(args))\n    cfg.log_self()\n    return cfg\n</code></pre>"},{"location":"api_ref/data_prep/plot_samples/#kelp.data_prep.plot_samples.plot_samples","title":"<code>kelp.data_prep.plot_samples.plot_samples</code>","text":"<p>Runs sample plotting for files in specified directory in parallel using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required <code>output_dir</code> <code>Path</code> <p>The path to the output directory.</p> required <code>records</code> <code>List[Tuple[str, str]]</code> <p>The list of tile ID and split name tuples.</p> required Source code in <code>kelp/data_prep/plot_samples.py</code> <pre><code>@timed\ndef plot_samples(data_dir: Path, output_dir: Path, records: List[Tuple[str, str]]) -&gt; None:\n    \"\"\"\n    Runs sample plotting for files in specified directory in parallel using Dask.\n\n    Args:\n        data_dir: The path to the data directory.\n        output_dir: The path to the output directory.\n        records: The list of tile ID and split name tuples.\n\n    \"\"\"\n    _logger.info(\"Running sample plotting\")\n    (dask.bag.from_sequence(records).map(plot_single_image, data_dir=data_dir, output_dir=output_dir).compute())\n</code></pre>"},{"location":"api_ref/data_prep/plot_samples/#kelp.data_prep.plot_samples.plot_single_image","title":"<code>kelp.data_prep.plot_samples.plot_single_image</code>","text":"<p>Plots a single image for visual inspection.</p> <p>Parameters:</p> Name Type Description Default <code>tile_id_split_tuple</code> <code>Tuple[str, str]</code> <p>A tuple containing tile ID and split</p> required <code>data_dir</code> <code>Path</code> required <code>output_dir</code> <code>Path</code> required Source code in <code>kelp/data_prep/plot_samples.py</code> <pre><code>def plot_single_image(tile_id_split_tuple: Tuple[str, str], data_dir: Path, output_dir: Path) -&gt; None:\n    \"\"\"\n    Plots a single image for visual inspection.\n\n    Args:\n        tile_id_split_tuple: A tuple containing tile ID and split\n        data_dir:\n        output_dir:\n\n    Returns:\n\n    \"\"\"\n    tile_id, split = tile_id_split_tuple\n    out_dir = output_dir / \"plots\"\n    out_dir.mkdir(exist_ok=True, parents=True)\n\n    src: rasterio.DatasetReader\n    with rasterio.open(data_dir / split / \"images\" / f\"{tile_id}_satellite.tif\") as src:\n        input_arr = src.read()\n\n    target_arr: Optional[np.ndarray] = None  # type: ignore[type-arg]\n    if split != \"test\":\n        with rasterio.open(data_dir / split / \"masks\" / f\"{tile_id}_kelp.tif\") as src:\n            target_arr = src.read(1)\n\n    fig = plot_sample(input_arr=input_arr, target_arr=target_arr, suptitle=f\"Tile ID = {tile_id}\")\n    plt.savefig(out_dir / f\"{tile_id}_plot.png\", dpi=500)\n    plt.close(fig)\n</code></pre>"},{"location":"api_ref/data_prep/sahi_dataset_prep/","title":"sahi_dataset_prep","text":"<p>SAHI (Slicing Aided Hyper Inference) dataset preparation logic.</p>"},{"location":"api_ref/data_prep/sahi_dataset_prep/#kelp.data_prep.sahi_dataset_prep.SahiDatasetPrepConfig","title":"<code>kelp.data_prep.sahi_dataset_prep.SahiDatasetPrepConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>Config class for creating SAHI dataset</p> Source code in <code>kelp/data_prep/sahi_dataset_prep.py</code> <pre><code>class SahiDatasetPrepConfig(ConfigBase):\n    \"\"\"Config class for creating SAHI dataset\"\"\"\n\n    data_dir: Path\n    metadata_fp: Path\n    output_dir: Path\n    image_size: int = 128\n    stride: int = 64\n</code></pre>"},{"location":"api_ref/data_prep/sahi_dataset_prep/#kelp.data_prep.sahi_dataset_prep.generate_tiles_from_image","title":"<code>kelp.data_prep.sahi_dataset_prep.generate_tiles_from_image</code>","text":"<p>Generates small tiles from the input image using specified tile size and stride.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required <code>tile_id</code> <code>str</code> <p>The tile ID.</p> required <code>tile_size</code> <code>Tuple[int, int]</code> <p>The tile size in pixels.</p> required <code>stride</code> <code>Tuple[int, int]</code> <p>The tile stride in pixels.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required Source code in <code>kelp/data_prep/sahi_dataset_prep.py</code> <pre><code>def generate_tiles_from_image(\n    data_dir: Path,\n    tile_id: str,\n    tile_size: Tuple[int, int],\n    stride: Tuple[int, int],\n    output_dir: Path,\n) -&gt; List[Tuple[int, int, float, float]]:\n    \"\"\"\n    Generates small tiles from the input image using specified tile size and stride.\n\n    Args:\n        data_dir: The path to the data directory.\n        tile_id: The tile ID.\n        tile_size: The tile size in pixels.\n        stride: The tile stride in pixels.\n        output_dir: The output directory.\n\n    Returns: A list of tuples with the tile coordinates and stats about kelp pixel number and kelp pixel percentage.\n\n    \"\"\"\n    records: List[Tuple[int, int, float, float]] = []\n\n    with rasterio.open(data_dir / \"images\" / f\"{tile_id}_satellite.tif\") as src:\n        for j in range(0, src.height, stride[1]):\n            for i in range(0, src.width, stride[0]):\n                window = Window(i, j, *tile_size)\n                data = src.read(window=window)\n\n                # Check if the tile is smaller than expected\n                if data.shape[1] &lt; tile_size[0] or data.shape[2] &lt; tile_size[1]:\n                    # Pad the data to match the expected tile size\n                    padded_data = np.full((src.count, *tile_size), -32768, dtype=data.dtype)\n                    padded_data[:, : data.shape[1], : data.shape[2]] = data\n                    data = padded_data\n\n                # Save the tile\n                output_tile_path = output_dir / \"images\" / f\"{tile_id}_satellite_{i}_{j}.tif\"\n                with rasterio.open(\n                    output_tile_path,\n                    \"w\",\n                    driver=\"GTiff\",\n                    height=tile_size[1],\n                    width=tile_size[0],\n                    count=src.count,\n                    dtype=data.dtype,\n                    crs=src.crs,\n                    transform=src.window_transform(window),\n                ) as dst:\n                    dst.write(data)\n\n    with rasterio.open(data_dir / \"masks\" / f\"{tile_id}_kelp.tif\") as src:\n        for j in range(0, src.height, stride[1]):\n            for i in range(0, src.width, stride[0]):\n                window = Window(i, j, *tile_size)\n                data = src.read(window=window)\n\n                # Check if the tile is smaller than expected\n                if data.shape[1] &lt; tile_size[0] or data.shape[2] &lt; tile_size[1]:\n                    # Pad the data to match the expected tile size\n                    padded_data = np.full((src.count, *tile_size), -32768, dtype=data.dtype)\n                    padded_data[:, : data.shape[1], : data.shape[2]] = data\n                    data = padded_data\n\n                # Save the tile\n                output_tile_path = output_dir / \"masks\" / f\"{tile_id}_kelp_{i}_{j}.tif\"\n                with rasterio.open(\n                    output_tile_path,\n                    \"w\",\n                    driver=\"GTiff\",\n                    height=tile_size[1],\n                    width=tile_size[0],\n                    count=src.count,\n                    dtype=data.dtype,\n                    crs=src.crs,\n                    transform=src.window_transform(window),\n                ) as dst:\n                    dst.write(data)\n\n                kelp_pct: float = data.sum() / np.prod([tile_size[1], tile_size[0]])\n                kelp_pxls: float = data.sum()\n                records.append((i, j, kelp_pxls, kelp_pct))\n\n    return records\n</code></pre>"},{"location":"api_ref/data_prep/sahi_dataset_prep/#kelp.data_prep.sahi_dataset_prep.main","title":"<code>kelp.data_prep.sahi_dataset_prep.main</code>","text":"<p>Main entrypoint for generating SAHI dataset.</p> Source code in <code>kelp/data_prep/sahi_dataset_prep.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for generating SAHI dataset.\"\"\"\n    cfg = parse_args()\n    prep_sahi_dataset(**cfg.model_dump())\n</code></pre>"},{"location":"api_ref/data_prep/sahi_dataset_prep/#kelp.data_prep.sahi_dataset_prep.parse_args","title":"<code>kelp.data_prep.sahi_dataset_prep.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of SahiDatasetPrepConfig.</p> Source code in <code>kelp/data_prep/sahi_dataset_prep.py</code> <pre><code>def parse_args() -&gt; SahiDatasetPrepConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of SahiDatasetPrepConfig.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", type=str, required=True)\n    parser.add_argument(\"--metadata_fp\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    parser.add_argument(\"--image_size\", type=int, default=128)\n    parser.add_argument(\"--stride\", type=int, default=64)\n    args = parser.parse_args()\n    cfg = SahiDatasetPrepConfig(**vars(args))\n    cfg.log_self()\n    return cfg\n</code></pre>"},{"location":"api_ref/data_prep/sahi_dataset_prep/#kelp.data_prep.sahi_dataset_prep.prep_sahi_dataset","title":"<code>kelp.data_prep.sahi_dataset_prep.prep_sahi_dataset</code>","text":"<p>Runs data preparation for SAHI model training.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required <code>metadata_fp</code> <code>Path</code> <p>The path to the metadata parquet file.</p> required <code>output_dir</code> <code>Path</code> <p>The path to the output directory.</p> required <code>image_size</code> <code>int</code> <p>The image size to use for tiles.</p> required <code>stride</code> <code>int</code> <p>The stride to use for overlap between tiles.</p> required Source code in <code>kelp/data_prep/sahi_dataset_prep.py</code> <pre><code>def prep_sahi_dataset(data_dir: Path, metadata_fp: Path, output_dir: Path, image_size: int, stride: int) -&gt; None:\n    \"\"\"\n    Runs data preparation for SAHI model training.\n\n    Args:\n        data_dir: The path to the data directory.\n        metadata_fp: The path to the metadata parquet file.\n        output_dir: The path to the output directory.\n        image_size: The image size to use for tiles.\n        stride: The stride to use for overlap between tiles.\n\n    \"\"\"\n    df = pd.read_parquet(metadata_fp)\n    df = df[df[\"original_split\"] == \"train\"]\n    records: List[Tuple[Any, ...]] = []\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing files\"):\n        out_dir_images = output_dir / \"images\"\n        out_dir_masks = output_dir / \"masks\"\n        out_dir_images.mkdir(exist_ok=True, parents=True)\n        out_dir_masks.mkdir(exist_ok=True, parents=True)\n        sub_records = generate_tiles_from_image(\n            data_dir=data_dir,\n            tile_id=row[\"tile_id\"],\n            output_dir=output_dir,\n            tile_size=(image_size, image_size),\n            stride=(stride, stride),\n        )\n        for j, i, kelp_pxls, kelp_pct in sub_records:\n            records.append((row[\"tile_id\"], j, i, kelp_pxls, kelp_pct))\n    results_df = pd.DataFrame(records, columns=[\"tile_id\", \"j\", \"i\", \"kelp_pxls\", \"kelp_pct\"])\n    results_df = df.merge(results_df, how=\"inner\", left_on=\"tile_id\", right_on=\"tile_id\")\n    results_df.to_parquet(output_dir / \"sahi_train_val_test_dataset.parquet\")\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/","title":"train_val_test_split","text":"<p>Train, validation and test dataset split logic.</p>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.TrainTestSplitConfig","title":"<code>kelp.data_prep.train_val_test_split.TrainTestSplitConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>A config for generating train and test splits.</p> Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>class TrainTestSplitConfig(ConfigBase):\n    \"\"\"A config for generating train and test splits.\"\"\"\n\n    dataset_metadata_fp: Path\n    stratification_columns: List[str]\n    split_strategy: Literal[\"cross_val\", \"random\"] = \"cross_val\"\n    random_split_train_size: float = 0.95\n    seed: int = consts.reproducibility.SEED\n    splits: int = 5\n    output_dir: Path\n\n    @field_validator(\"stratification_columns\", mode=\"before\")\n    def validate_stratification_columns(cls, val: str) -&gt; List[str]:\n        return [s.strip() for s in val.split(\",\")]\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.filter_data","title":"<code>kelp.data_prep.train_val_test_split.filter_data</code>","text":"<p>Filters dataset by removing images with high kelp pixel percentage.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataset metadata dataframe.</p> required Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>@timed\ndef filter_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters dataset by removing images with high kelp pixel percentage.\n\n    Args:\n        df: The dataset metadata dataframe.\n\n    Returns: A pandas dataframe with filtered data.\n\n    \"\"\"\n    df = df[df[\"high_kelp_pixels_pct\"].isin([False, None])]\n    return df\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.k_fold_split","title":"<code>kelp.data_prep.train_val_test_split.k_fold_split</code>","text":"<p>Runs Stratified K-Fold Cross Validation split on dataset.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataset metadata dataframe.</p> required <code>splits</code> <code>int</code> <p>The number of splits to perform.</p> <code>5</code> <code>seed</code> <code>int</code> <p>The seed for reproducibility.</p> <code>SEED</code> Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>@timed\ndef k_fold_split(df: pd.DataFrame, splits: int = 5, seed: int = consts.reproducibility.SEED) -&gt; pd.DataFrame:\n    \"\"\"\n    Runs Stratified K-Fold Cross Validation split on dataset.\n\n    Args:\n        df: The dataset metadata dataframe.\n        splits: The number of splits to perform.\n        seed: The seed for reproducibility.\n\n    Returns: A dataframe with extra columns indicating to which splits the record belongs.\n\n    \"\"\"\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=seed)\n\n    for i in range(splits):\n        df[f\"split_{i}\"] = \"train\"\n\n    for i, (_, val_idx) in enumerate(skf.split(df, df[\"stratification\"])):\n        df.loc[val_idx, f\"split_{i}\"] = \"val\"\n\n    return df\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.load_data","title":"<code>kelp.data_prep.train_val_test_split.load_data</code>","text":"<p>Loads dataset metadata parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>fp</code> <code>Path</code> <p>The path to the metadata parquet file.</p> required Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>@timed\ndef load_data(fp: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads dataset metadata parquet file.\n\n    Args:\n        fp: The path to the metadata parquet file.\n\n    Returns: A pandas dataframe with dataset metadata.\n\n    \"\"\"\n    return pd.read_parquet(fp).rename(columns={\"split\": \"original_split\"})\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.main","title":"<code>kelp.data_prep.train_val_test_split.main</code>","text":"<p>Main entry point for running train/val/test dataset split.</p> Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>@timed\ndef main() -&gt; None:\n    \"\"\"Main entry point for running train/val/test dataset split.\"\"\"\n    cfg = parse_args()\n    (\n        load_data(cfg.dataset_metadata_fp)\n        .pipe(filter_data)\n        .pipe(make_stratification_column, stratification_columns=cfg.stratification_columns)\n        .pipe(\n            split_dataset,\n            split_strategy=cfg.split_strategy,\n            splits=cfg.splits,\n            random_split_train_size=cfg.random_split_train_size,\n            seed=cfg.seed,\n        )\n        .pipe(save_data, output_path=cfg.output_dir / f\"train_val_test_dataset_strategy={cfg.split_strategy}.parquet\")\n    )\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.make_stratification_column","title":"<code>kelp.data_prep.train_val_test_split.make_stratification_column</code>","text":"<p>Creates a stratification column from dataset metadata and specified metadata columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataset metadata dataframe.</p> required <code>stratification_columns</code> <code>List[str]</code> <p>The metadata columns to use for the stratification.</p> required Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>@timed\ndef make_stratification_column(df: pd.DataFrame, stratification_columns: List[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a stratification column from dataset metadata and specified metadata columns.\n\n    Args:\n        df: The dataset metadata dataframe.\n        stratification_columns: The metadata columns to use for the stratification.\n\n    Returns: The same pandas dataframe with appended stratification column.\n\n    \"\"\"\n\n    def make_stratification_key(series: pd.Series) -&gt; str:\n        vals = [f\"{col}={str(series[col])}\" for col in stratification_columns]\n        return \"-\".join(vals)\n\n    df[\"stratification\"] = df.apply(lambda row: make_stratification_key(row), axis=1).astype(\"category\")\n\n    return df\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.parse_args","title":"<code>kelp.data_prep.train_val_test_split.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of TrainTestSplitConfig.</p> Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>def parse_args() -&gt; TrainTestSplitConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of TrainTestSplitConfig.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset_metadata_fp\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--stratification_columns\",\n        type=str,\n        default=\"has_kelp,almost_all_water,qa_ok,high_corrupted_pixels_pct\",\n    )\n    parser.add_argument(\n        \"--split_strategy\",\n        type=str,\n        choices=[\"cross_val\", \"random\"],\n        default=\"cross_val\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=consts.reproducibility.SEED,\n    )\n    parser.add_argument(\n        \"--splits\",\n        type=int,\n        default=10,\n    )\n    parser.add_argument(\n        \"--random_split_train_size\",\n        type=float,\n        default=0.95,\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=True,\n    )\n    args = parser.parse_args()\n    cfg = TrainTestSplitConfig(**vars(args))\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    cfg.log_self()\n    return cfg\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.run_cross_val_split","title":"<code>kelp.data_prep.train_val_test_split.run_cross_val_split</code>","text":"<p>Runs Stratified K-Fold Cross Validation split on training samples. The test samples will be marked as test split.</p> <p>Parameters:</p> Name Type Description Default <code>train_samples</code> <code>DataFrame</code> <p>The dataframe with training samples.</p> required <code>test_samples</code> <code>DataFrame</code> <p>The dataframe with test samples.</p> required <code>splits</code> <code>int</code> <p>The number of splits to perform.</p> <code>5</code> <code>seed</code> <code>int</code> <p>The seed for reproducibility.</p> <code>SEED</code> Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>@timed\ndef run_cross_val_split(\n    train_samples: pd.DataFrame,\n    test_samples: pd.DataFrame,\n    splits: int = 5,\n    seed: int = consts.reproducibility.SEED,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Runs Stratified K-Fold Cross Validation split on training samples. The test samples will be marked as test split.\n\n    Args:\n        train_samples: The dataframe with training samples.\n        test_samples: The dataframe with test samples.\n        splits: The number of splits to perform.\n        seed: The seed for reproducibility.\n\n    Returns: A dataframe with merged training and test samples.\n\n    \"\"\"\n    results = []\n    for aoi_id, frame in train_samples[[\"aoi_id\", \"stratification\"]].groupby(\"aoi_id\"):\n        results.append((aoi_id, frame[\"stratification\"].value_counts().reset_index().iloc[0][\"stratification\"]))\n    results_df = pd.DataFrame(results, columns=[\"aoi_id\", \"stratification\"])\n    train_val_samples = k_fold_split(results_df, splits=splits, seed=seed)\n    train_samples = train_samples.drop(\"stratification\", axis=1)\n    train_samples = train_samples.merge(train_val_samples, how=\"inner\", left_on=\"aoi_id\", right_on=\"aoi_id\")\n    for split in range(splits):\n        test_samples[f\"split_{split}\"] = \"test\"\n    return train_samples\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.run_random_split","title":"<code>kelp.data_prep.train_val_test_split.run_random_split</code>","text":"<p>Runs random split on train_samples. The test samples will be marked as test split.</p> <p>Parameters:</p> Name Type Description Default <code>train_samples</code> <code>DataFrame</code> <p>The dataframe with training samples.</p> required <code>test_samples</code> <code>DataFrame</code> <p>The dataframe with test samples.</p> required <code>random_split_train_size</code> <code>float</code> <p>The size of training split as a fraction of the whole dataset.</p> <code>0.95</code> <code>seed</code> <code>int</code> <p>The seed for reproducibility.</p> <code>SEED</code> Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>@timed\ndef run_random_split(\n    train_samples: pd.DataFrame,\n    test_samples: pd.DataFrame,\n    random_split_train_size: float = 0.95,\n    seed: int = consts.reproducibility.SEED,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Runs random split on train_samples. The test samples will be marked as test split.\n\n    Args:\n        train_samples: The dataframe with training samples.\n        test_samples: The dataframe with test samples.\n        random_split_train_size: The size of training split as a fraction of the whole dataset.\n        seed: The seed for reproducibility.\n\n    Returns: A dataframe with training and test samples.\n\n    \"\"\"\n    X_train, X_val = train_test_split(\n        train_samples,\n        train_size=random_split_train_size,\n        stratify=train_samples[\"stratification\"],\n        shuffle=True,\n        random_state=seed,\n    )\n    X_train[\"split_0\"] = \"train\"\n    X_val[\"split_0\"] = \"val\"\n    test_samples[\"split_0\"] = \"test\"\n    return pd.concat([X_train, X_val])\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.save_data","title":"<code>kelp.data_prep.train_val_test_split.save_data</code>","text":"<p>Saves the specified dataframe under specified output path.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to save.</p> required <code>output_path</code> <code>Path</code> <p>The path to save the dataframe under.</p> required Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>@timed\ndef save_data(df: pd.DataFrame, output_path: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Saves the specified dataframe under specified output path.\n\n    Args:\n        df: The dataframe to save.\n        output_path: The path to save the dataframe under.\n\n    Returns: The same dataframe as input.\n\n    \"\"\"\n    df.to_parquet(output_path, index=False)\n    return df\n</code></pre>"},{"location":"api_ref/data_prep/train_val_test_split/#kelp.data_prep.train_val_test_split.split_dataset","title":"<code>kelp.data_prep.train_val_test_split.split_dataset</code>","text":"<p>Performs dataset split into training, validation and test sets using specified split strategy.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The metadata dataframe containing the training and test records.</p> required <code>split_strategy</code> <code>Literal['cross_val', 'random']</code> <p>The strategy to use.</p> <code>'cross_val'</code> <code>random_split_train_size</code> <code>float</code> <p>The size of training split as a fraction of the whole dateset.</p> <code>0.95</code> <code>splits</code> <code>int</code> <p>The number of CV splits.</p> <code>5</code> <code>seed</code> <code>int</code> <p>The seed for reproducibility.</p> <code>SEED</code> Source code in <code>kelp/data_prep/train_val_test_split.py</code> <pre><code>@timed\ndef split_dataset(\n    df: pd.DataFrame,\n    split_strategy: Literal[\"cross_val\", \"random\"] = \"cross_val\",\n    random_split_train_size: float = 0.95,\n    splits: int = 5,\n    seed: int = consts.reproducibility.SEED,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs dataset split into training, validation and test sets using specified split strategy.\n\n    Args:\n        df: The metadata dataframe containing the training and test records.\n        split_strategy: The strategy to use.\n        random_split_train_size: The size of training split as a fraction of the whole dateset.\n        splits: The number of CV splits.\n        seed: The seed for reproducibility.\n\n    Returns: A dataframe with training, validation and test splits.\n\n    \"\"\"\n    train_samples = df[df[\"original_split\"] == \"train\"].copy()\n    test_samples = df[df[\"original_split\"] == \"test\"].copy()\n    if split_strategy == \"cross_val\":\n        train_samples = run_cross_val_split(\n            train_samples=train_samples,\n            test_samples=test_samples,\n            splits=splits,\n            seed=seed,\n        )\n    elif split_strategy == \"random\":\n        train_samples = run_random_split(\n            train_samples=train_samples,\n            test_samples=test_samples,\n            random_split_train_size=random_split_train_size,\n            seed=seed,\n        )\n    else:\n        raise ValueError(f\"{split_strategy=} is not supported\")\n    return pd.concat([train_samples, test_samples])\n</code></pre>"},{"location":"api_ref/nn/data/band_stats/","title":"band_stats","text":"<p>The band stats classes.</p>"},{"location":"api_ref/nn/data/band_stats/#kelp.nn.data.band_stats.BandStats","title":"<code>kelp.nn.data.band_stats.BandStats</code>  <code>dataclass</code>","text":"<p>A dataclass for holding band statistics.</p> Source code in <code>kelp/nn/data/band_stats.py</code> <pre><code>@dataclass\nclass BandStats:\n    \"\"\"A dataclass for holding band statistics.\"\"\"\n\n    mean: Tensor\n    std: Tensor\n    min: Tensor\n    max: Tensor\n    q01: Tensor\n    q99: Tensor\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/","title":"datamodule","text":"<p>The Kelp Forest DataModule.</p>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule that handles all data-related setup for the Kelp Forest Segmentation Task.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_stats</code> <code>Dict[str, Dict[str, float]]</code> <p>The per-band statistics dictionary.</p> required <code>train_images</code> <code>Optional[List[Path]]</code> <p>The list of training images.</p> <code>None</code> <code>train_masks</code> <code>Optional[List[Path]]</code> <p>The list of training masks.</p> <code>None</code> <code>val_images</code> <code>Optional[List[Path]]</code> <p>The list of validation images.</p> <code>None</code> <code>val_masks</code> <code>Optional[List[Path]]</code> <p>The list of validation mask.</p> <code>None</code> <code>test_images</code> <code>Optional[List[Path]]</code> <p>The list of test images.</p> <code>None</code> <code>test_masks</code> <code>Optional[List[Path]]</code> <p>The list of test masks.</p> <code>None</code> <code>predict_images</code> <code>Optional[List[Path]]</code> <p>The list of prediction images.</p> <code>None</code> <code>spectral_indices</code> <code>Optional[List[str]]</code> <p>The list of spectral indices to append to the input tensor.</p> <code>None</code> <code>bands</code> <code>Optional[List[str]]</code> <p>The list of band names to use.</p> <code>None</code> <code>missing_pixels_fill_value</code> <code>float</code> <p>The value to fill missing pixels with.</p> <code>0.0</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>sahi</code> <code>bool</code> <p>Flag indicating whether we are using SAHI dataset.</p> <code>False</code> <code>image_size</code> <code>int</code> <p>The size of the input image.</p> <code>352</code> <code>interpolation</code> <code>Literal['nearest', 'nearest-exact', 'bilinear', 'bicubic']</code> <p>The interpolation to use when performing resize operation.</p> <code>'nearest'</code> <code>resize_strategy</code> <code>Literal['pad', 'resize']</code> <p>The resize strategy to use. One of ['pad', 'resize'].</p> <code>'pad'</code> <code>normalization_strategy</code> <code>Literal['min-max', 'quantile', 'per-sample-min-max', 'per-sample-quantile', 'z-score']</code> <p>The normalization strategy to use.</p> <code>'quantile'</code> <code>mask_using_qa</code> <code>bool</code> <p>A flag indicating whether spectral index bands should be masked with QA band.</p> <code>False</code> <code>mask_using_water_mask</code> <code>bool</code> <p>A flag indicating whether spectral index bands should be masked with DEM Water Mask.</p> <code>False</code> <code>use_weighted_sampler</code> <code>bool</code> <p>A flag indicating whether to use weighted sampler.</p> <code>False</code> <code>samples_per_epoch</code> <code>int</code> <p>The number of samples per epoch if using weighted sampler.</p> <code>10240</code> <code>image_weights</code> <code>Optional[List[float]]</code> <p>The weights per input image for weighted sampler if using weighted sampler.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Extra keywords. Unused.</p> <code>{}</code> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>class KelpForestDataModule(pl.LightningDataModule):\n    \"\"\"\n    A LightningDataModule that handles all data-related setup for the Kelp Forest Segmentation Task.\n\n    Args:\n        dataset_stats: The per-band statistics dictionary.\n        train_images: The list of training images.\n        train_masks: The list of training masks.\n        val_images: The list of validation images.\n        val_masks: The list of validation mask.\n        test_images: The list of test images.\n        test_masks: The list of test masks.\n        predict_images: The list of prediction images.\n        spectral_indices: The list of spectral indices to append to the input tensor.\n        bands: The list of band names to use.\n        missing_pixels_fill_value: The value to fill missing pixels with.\n        batch_size: The batch size.\n        num_workers: The number of workers to use for data loading.\n        sahi: Flag indicating whether we are using SAHI dataset.\n        image_size: The size of the input image.\n        interpolation: The interpolation to use when performing resize operation.\n        resize_strategy: The resize strategy to use. One of ['pad', 'resize'].\n        normalization_strategy: The normalization strategy to use.\n        mask_using_qa: A flag indicating whether spectral index bands should be masked with QA band.\n        mask_using_water_mask: A flag indicating whether spectral index bands should be masked with DEM Water Mask.\n        use_weighted_sampler: A flag indicating whether to use weighted sampler.\n        samples_per_epoch: The number of samples per epoch if using weighted sampler.\n        image_weights: The weights per input image for weighted sampler if using weighted sampler.\n        **kwargs: Extra keywords. Unused.\n    \"\"\"\n\n    base_bands = [\n        \"SWIR\",\n        \"NIR\",\n        \"R\",\n        \"G\",\n        \"B\",\n        \"QA\",\n        \"DEM\",\n    ]\n\n    def __init__(\n        self,\n        dataset_stats: Dict[str, Dict[str, float]],\n        train_images: Optional[List[Path]] = None,\n        train_masks: Optional[List[Path]] = None,\n        val_images: Optional[List[Path]] = None,\n        val_masks: Optional[List[Path]] = None,\n        test_images: Optional[List[Path]] = None,\n        test_masks: Optional[List[Path]] = None,\n        predict_images: Optional[List[Path]] = None,\n        spectral_indices: Optional[List[str]] = None,\n        bands: Optional[List[str]] = None,\n        missing_pixels_fill_value: float = 0.0,\n        batch_size: int = 32,\n        num_workers: int = 0,\n        sahi: bool = False,\n        image_size: int = 352,\n        interpolation: Literal[\"nearest\", \"nearest-exact\", \"bilinear\", \"bicubic\"] = \"nearest\",\n        resize_strategy: Literal[\"pad\", \"resize\"] = \"pad\",\n        normalization_strategy: Literal[\n            \"min-max\",\n            \"quantile\",\n            \"per-sample-min-max\",\n            \"per-sample-quantile\",\n            \"z-score\",\n        ] = \"quantile\",\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n        use_weighted_sampler: bool = False,\n        samples_per_epoch: int = 10240,\n        image_weights: Optional[List[float]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__()  # type: ignore[no-untyped-call]\n        bands = self._guard_against_invalid_bands_config(bands)\n        spectral_indices = self._guard_against_invalid_spectral_indices_config(\n            bands_to_use=bands,\n            spectral_indices=spectral_indices,\n            mask_using_qa=mask_using_qa,\n            mask_using_water_mask=mask_using_water_mask,\n        )\n        self.dataset_stats = dataset_stats\n        self.train_images = train_images or []\n        self.train_masks = train_masks or []\n        self.val_images = val_images or []\n        self.val_masks = val_masks or []\n        self.test_images = test_images or []\n        self.test_masks = test_masks or []\n        self.predict_images = predict_images or []\n        self.spectral_indices = spectral_indices\n        self.bands = bands\n        self.band_order = [self.base_bands.index(band) for band in self.bands]\n        self.bands_to_use = self.bands + self.spectral_indices\n        self.band_index_lookup = {band: idx for idx, band in enumerate(self.bands_to_use)}\n        self.missing_pixels_fill_value = missing_pixels_fill_value\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.sahi = sahi\n        self.image_size = image_size\n        self.interpolation = interpolation\n        self.normalization_strategy = normalization_strategy\n        self.mask_using_qa = mask_using_qa\n        self.mask_using_water_mask = mask_using_water_mask\n        self.use_weighted_sampler = use_weighted_sampler\n        self.samples_per_epoch = samples_per_epoch\n        self.image_weights = image_weights or [1.0 for _ in self.train_images]\n        self.band_stats, self.in_channels = resolve_normalization_stats(\n            dataset_stats=dataset_stats,\n            bands_to_use=self.bands_to_use,\n        )\n        self.normalization_transform = resolve_normalization_transform(\n            band_stats=self.band_stats,\n            normalization_strategy=self.normalization_strategy,\n        )\n        self.train_augmentations = resolve_transforms(\n            spectral_indices=self.spectral_indices,\n            band_index_lookup=self.band_index_lookup,\n            band_stats=self.band_stats,\n            mask_using_qa=self.mask_using_qa,\n            mask_using_water_mask=self.mask_using_water_mask,\n            normalization_transform=self.normalization_transform,\n            stage=\"train\",\n        )\n        self.val_augmentations = resolve_transforms(\n            spectral_indices=self.spectral_indices,\n            band_index_lookup=self.band_index_lookup,\n            band_stats=self.band_stats,\n            mask_using_qa=self.mask_using_qa,\n            mask_using_water_mask=self.mask_using_water_mask,\n            normalization_transform=self.normalization_transform,\n            stage=\"val\",\n        )\n        self.test_augmentations = resolve_transforms(\n            spectral_indices=self.spectral_indices,\n            band_index_lookup=self.band_index_lookup,\n            band_stats=self.band_stats,\n            mask_using_qa=self.mask_using_qa,\n            mask_using_water_mask=self.mask_using_water_mask,\n            normalization_transform=self.normalization_transform,\n            stage=\"test\",\n        )\n        self.predict_augmentations = resolve_transforms(\n            spectral_indices=self.spectral_indices,\n            band_index_lookup=self.band_index_lookup,\n            band_stats=self.band_stats,\n            mask_using_qa=self.mask_using_qa,\n            mask_using_water_mask=self.mask_using_water_mask,\n            normalization_transform=self.normalization_transform,\n            stage=\"predict\",\n        )\n        self.image_resize_tf = resolve_resize_transform(\n            image_or_mask=\"image\",\n            resize_strategy=resize_strategy,\n            image_size=image_size,\n            interpolation=interpolation,\n        )\n        self.mask_resize_tf = resolve_resize_transform(\n            image_or_mask=\"mask\",\n            resize_strategy=resize_strategy,\n            image_size=image_size,\n            interpolation=interpolation,\n        )\n\n    def _guard_against_invalid_bands_config(self, bands: Optional[List[str]]) -&gt; List[str]:\n        if not bands:\n            return self.base_bands\n\n        if set(bands).issubset(set(self.base_bands)):\n            return bands\n\n        raise ValueError(f\"{bands=} should be a subset of {self.base_bands=}\")\n\n    def _guard_against_invalid_spectral_indices_config(\n        self,\n        bands_to_use: List[str],\n        spectral_indices: Optional[List[str]] = None,\n        mask_using_qa: bool = False,\n        mask_using_water_mask: bool = False,\n    ) -&gt; List[str]:\n        if not spectral_indices:\n            return []\n\n        if \"DEM\" not in bands_to_use and \"DEMWM\" in spectral_indices:\n            raise ValueError(\n                f\"You specified 'DEMWM' as one of spectral indices but 'DEM' is not in {bands_to_use=}, \"\n                f\"which corresponds to {bands_to_use=}\"\n            )\n\n        if \"QA\" not in bands_to_use and mask_using_qa:\n            raise ValueError(\n                f\"You specified {mask_using_qa=} but 'QA' is not in {bands_to_use=}, \"\n                f\"which corresponds to {bands_to_use=}\"\n            )\n\n        if mask_using_water_mask and \"DEMWM\" not in spectral_indices:\n            raise ValueError(f\"You specified {mask_using_water_mask=} but 'DEMWM' is not in {spectral_indices=}\")\n\n        return spectral_indices\n\n    def _build_dataset(self, images: List[Path], masks: Optional[List[Path]] = None) -&gt; KelpForestSegmentationDataset:\n        ds = KelpForestSegmentationDataset(\n            image_fps=images,\n            mask_fps=masks,\n            transforms=self._common_transforms,\n            band_order=self.band_order,\n            fill_value=self.missing_pixels_fill_value,\n        )\n        return ds\n\n    def _apply_transform(\n        self,\n        transforms: Callable[[Tensor, Tensor], Tuple[Tensor, Tensor]],\n        batch: Dict[str, Tensor],\n    ) -&gt; Dict[str, Tensor]:\n        x = batch[\"image\"]\n        # Kornia expects masks to be floats with a channel dimension\n        y = batch[\"mask\"].float().unsqueeze(1)\n        x, y = transforms(x, y)\n        batch[\"image\"] = x\n        # torchmetrics expects masks to be longs without a channel dimension\n        batch[\"mask\"] = y.squeeze(1).long()\n        return batch\n\n    def _apply_predict_transform(\n        self,\n        transforms: Callable[[Tensor], Tensor],\n        batch: Dict[str, Tensor],\n    ) -&gt; Dict[str, Tensor]:\n        x = batch[\"image\"]\n        x = transforms(x)\n        batch[\"image\"] = x\n        return batch\n\n    def _common_transforms(self, sample: Dict[str, Tensor]) -&gt; Dict[str, Tensor]:\n        sample[\"image\"] = self.image_resize_tf(sample[\"image\"])\n        if \"mask\" in sample:\n            sample[\"mask\"] = self.mask_resize_tf(sample[\"mask\"].unsqueeze(0)).squeeze()\n        return sample\n\n    def on_after_batch_transfer(self, batch: Dict[str, Any], batch_idx: int) -&gt; Dict[str, Any]:\n        \"\"\"Apply batch augmentations after batch is transferred to the device.\n\n        Args:\n            batch: mini-batch of data\n            batch_idx: batch index\n\n        Returns:\n            augmented mini-batch\n        \"\"\"\n        if (\n            hasattr(self, \"trainer\")\n            and self.trainer is not None\n            and hasattr(self.trainer, \"training\")\n            and self.trainer.training\n        ):\n            batch = self._apply_transform(self.train_augmentations, batch)\n        elif (\n            hasattr(self, \"trainer\")\n            and self.trainer is not None\n            and hasattr(self.trainer, \"predicting\")\n            and self.trainer.predicting\n        ):\n            batch = self._apply_predict_transform(self.predict_augmentations, batch)\n        else:\n            batch = self._apply_transform(self.val_augmentations, batch)\n\n        return batch\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"Initialize the main ``Dataset`` objects.\n\n        This method is called once per GPU per run.\n\n        Args:\n            stage: stage to set up\n        \"\"\"\n        if self.train_images:\n            self.train_dataset = self._build_dataset(self.train_images, self.train_masks)\n        if self.val_images:\n            self.val_dataset = self._build_dataset(self.val_images, self.val_masks)\n        if self.test_images:\n            self.test_dataset = self._build_dataset(self.test_images, self.test_masks)\n        if self.predict_images:\n            self.predict_dataset = self._build_dataset(self.predict_images)\n\n    def train_dataloader(self) -&gt; DataLoader[Any]:\n        \"\"\"Return a DataLoader for training.\n\n        Returns:\n            training data loader\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            sampler=WeightedRandomSampler(\n                weights=self.image_weights,\n                num_samples=self.samples_per_epoch,\n            )\n            if self.use_weighted_sampler\n            else None,\n            shuffle=True if not self.use_weighted_sampler else False,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader[Any]:\n        \"\"\"Return a DataLoader for validation.\n\n        Returns:\n            validation data loader\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=False,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader[Any]:\n        \"\"\"Return a DataLoader for testing.\n\n        Returns:\n            testing data loader\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=False,\n        )\n\n    def predict_dataloader(self) -&gt; DataLoader[Any]:\n        \"\"\"Return a DataLoader for prediction.\n\n        Returns:\n            prediction data loader\n        \"\"\"\n        return DataLoader(\n            self.predict_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=False,\n        )\n\n    def plot_sample(self, *args: Any, **kwargs: Any) -&gt; plt.Figure:\n        \"\"\"Run :meth:`kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_sample`.\"\"\"\n        return self.val_dataset.plot_sample(*args, **kwargs)\n\n    def plot_batch(self, *args: Any, **kwargs: Any) -&gt; FigureGrids:\n        \"\"\"Run :meth:`kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_batch`.\"\"\"\n        return self.val_dataset.plot_batch(*args, **kwargs)\n\n    @classmethod\n    def resolve_file_paths(\n        cls,\n        data_dir: Path,\n        metadata: pd.DataFrame,\n        cv_split: int,\n        split: str,\n        sahi: bool = False,\n    ) -&gt; Tuple[List[Path], List[Path]]:\n        \"\"\"\n        Resolves file paths using specified metadata dataframe.\n\n        Args:\n            data_dir: The data directory.\n            metadata: The metadata dataframe.\n            cv_split: The CV fold to use.\n            split: The split to use (train, val, test).\n            sahi: A flag indicating whether SAHI dataset is used.\n\n        Returns: A tuple with input image paths and target (mask) image paths\n\n        \"\"\"\n        split_data = metadata[metadata[f\"split_{cv_split}\"] == split]\n        img_folder = consts.data.TRAIN if split in [consts.data.TRAIN, consts.data.VAL] else consts.data.TEST\n        image_paths = sorted(\n            split_data.apply(\n                lambda row: data_dir\n                / img_folder\n                / \"images\"\n                / (\n                    f\"{row['tile_id']}_satellite_{row['j']}_{row['i']}.tif\"\n                    if sahi\n                    else f\"{row['tile_id']}_satellite.tif\"\n                ),\n                axis=1,\n            ).tolist()\n        )\n        mask_paths = sorted(\n            split_data.apply(\n                lambda row: data_dir\n                / img_folder\n                / \"masks\"\n                / (f\"{row['tile_id']}_kelp_{row['j']}_{row['i']}.tif\" if sahi else f\"{row['tile_id']}_kelp.tif\"),\n                axis=1,\n            ).tolist()\n        )\n        return image_paths, mask_paths\n\n    @classmethod\n    def _calculate_image_weights(\n        cls,\n        df: pd.DataFrame,\n        has_kelp_importance_factor: float = 1.0,\n        kelp_pixels_pct_importance_factor: float = 1.0,\n        qa_ok_importance_factor: float = 1.0,\n        qa_corrupted_pixels_pct_importance_factor: float = 1.0,\n        almost_all_water_importance_factor: float = -1.0,\n        dem_nan_pixels_pct_importance_factor: float = -1.0,\n        dem_zero_pixels_pct_importance_factor: float = -1.0,\n        sahi: bool = False,\n    ) -&gt; pd.DataFrame:\n        def resolve_weight(row: pd.Series) -&gt; float:\n            if row[\"original_split\"] == \"test\":\n                return 0.0\n\n            has_kelp = int(row[\"kelp_pxls\"] &gt; 0) if sahi else int(row[\"has_kelp\"])\n            kelp_pixels_pct = row[\"kelp_pct\"] if sahi else row[\"kelp_pixels_pct\"]\n            qa_ok = int(row[\"qa_ok\"])\n            water_pixels_pct = row[\"water_pixels_pct\"]\n            qa_corrupted_pixels_pct = row[\"qa_corrupted_pixels_pct\"]\n            dem_nan_pixels_pct = row[\"dem_nan_pixels_pct\"]\n            dem_zero_pixels_pct = row[\"dem_zero_pixels_pct\"]\n\n            weight = (\n                has_kelp_importance_factor * has_kelp\n                + kelp_pixels_pct_importance_factor * (1 - kelp_pixels_pct)\n                + qa_ok_importance_factor * qa_ok\n                + qa_corrupted_pixels_pct_importance_factor * (1 - qa_corrupted_pixels_pct)\n                + almost_all_water_importance_factor * (1 - water_pixels_pct)\n                + dem_nan_pixels_pct_importance_factor * (1 - dem_nan_pixels_pct)\n                + dem_zero_pixels_pct_importance_factor * (1 - dem_zero_pixels_pct)\n            )\n            return weight  # type: ignore[no-any-return]\n\n        df[\"weight\"] = df.apply(resolve_weight, axis=1)\n        min_val = df[\"weight\"].min()\n        max_val = df[\"weight\"].max()\n        df[\"weight\"] = (df[\"weight\"] - min_val) / (max_val - min_val + consts.data.EPS)\n        return df\n\n    @classmethod\n    def _resolve_image_weights(cls, df: pd.DataFrame, image_paths: List[Path]) -&gt; List[float]:\n        tile_ids = [fp.stem.split(\"_\")[0] for fp in image_paths]\n        weights = df[df[\"tile_id\"].isin(tile_ids)].sort_values(\"tile_id\")[\"weight\"].tolist()\n        return weights  # type: ignore[no-any-return]\n\n    @classmethod\n    def from_metadata_file(\n        cls,\n        data_dir: Path,\n        metadata_fp: Path,\n        dataset_stats: Dict[str, Dict[str, float]],\n        cv_split: int,\n        has_kelp_importance_factor: float = 1.0,\n        kelp_pixels_pct_importance_factor: float = 1.0,\n        qa_ok_importance_factor: float = 1.0,\n        almost_all_water_importance_factor: float = 1.0,\n        qa_corrupted_pixels_pct_importance_factor: float = -1.0,\n        dem_nan_pixels_pct_importance_factor: float = -1.0,\n        dem_zero_pixels_pct_importance_factor: float = -1.0,\n        sahi: bool = False,\n        **kwargs: Any,\n    ) -&gt; KelpForestDataModule:\n        \"\"\"\n        Factory method to create the KelpForestDataModule based on metadata file.\n\n        Args:\n            data_dir: The path to the data directory.\n            metadata_fp: The path to the metadata file.\n            dataset_stats: The per-band dataset statistics.\n            cv_split: The CV fold number to use.\n            has_kelp_importance_factor: The importance factor for the has_kelp flag.\n            kelp_pixels_pct_importance_factor: The importance factor for the kelp_pixels_pct value.\n            qa_ok_importance_factor: The importance factor for the has_kelp flag.\n            almost_all_water_importance_factor: The importance factor for the almost_all_water flag.\n            qa_corrupted_pixels_pct_importance_factor: The importance factor for the qa_corrupted_pixels_pct value.\n            dem_nan_pixels_pct_importance_factor: The importance factor for the dem_nan_pixels_pct value.\n            dem_zero_pixels_pct_importance_factor: The importance factor for the dem_zero_pixels_pct value.\n            sahi: A flag indicating whether SAHI dataset is used.\n            **kwargs: Other keyword arguments passed to the KelpForestDataModule constructor.\n\n        Returns: An instance of KelpForestDataModule.\n\n        \"\"\"\n        metadata = cls._calculate_image_weights(\n            df=pd.read_parquet(metadata_fp),\n            has_kelp_importance_factor=has_kelp_importance_factor,\n            kelp_pixels_pct_importance_factor=kelp_pixels_pct_importance_factor,\n            qa_ok_importance_factor=qa_ok_importance_factor,\n            qa_corrupted_pixels_pct_importance_factor=qa_corrupted_pixels_pct_importance_factor,\n            almost_all_water_importance_factor=almost_all_water_importance_factor,\n            dem_nan_pixels_pct_importance_factor=dem_nan_pixels_pct_importance_factor,\n            dem_zero_pixels_pct_importance_factor=dem_zero_pixels_pct_importance_factor,\n            sahi=sahi,\n        )\n        train_images, train_masks = cls.resolve_file_paths(\n            data_dir=data_dir, metadata=metadata, cv_split=cv_split, split=consts.data.TRAIN, sahi=sahi\n        )\n        val_images, val_masks = cls.resolve_file_paths(\n            data_dir=data_dir, metadata=metadata, cv_split=cv_split, split=consts.data.VAL, sahi=sahi\n        )\n        test_images, test_masks = cls.resolve_file_paths(\n            data_dir=data_dir, metadata=metadata, cv_split=cv_split, split=consts.data.VAL, sahi=sahi\n        )\n        image_weights = cls._resolve_image_weights(df=metadata, image_paths=train_images)\n        return cls(\n            train_images=train_images,\n            train_masks=train_masks,\n            val_images=val_images,\n            val_masks=val_masks,\n            test_images=test_images,\n            test_masks=test_masks,\n            predict_images=None,\n            image_weights=image_weights,\n            dataset_stats=dataset_stats,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_folders(\n        cls,\n        dataset_stats: Dict[str, Dict[str, float]],\n        train_data_folder: Optional[Path] = None,\n        val_data_folder: Optional[Path] = None,\n        test_data_folder: Optional[Path] = None,\n        predict_data_folder: Optional[Path] = None,\n        **kwargs: Any,\n    ) -&gt; KelpForestDataModule:\n        \"\"\"\n        Factory method to create the KelpForestDataModule based on folder paths.\n\n        Args:\n            dataset_stats: The per-band dataset statistics.\n            train_data_folder: The path to the training data folder.\n            val_data_folder: The path to the val data folder.\n            test_data_folder: The path to the test data folder.\n            predict_data_folder: The path to the prediction data folder.\n            **kwargs: Other keyword arguments passed to the KelpForestDataModule constructor.\n\n        Returns: An instance of KelpForestDataModule.\n\n        \"\"\"\n        return cls(\n            train_images=sorted(list(train_data_folder.glob(\"images/*.tif\")))\n            if train_data_folder and train_data_folder.exists()\n            else None,\n            train_masks=sorted(list(train_data_folder.glob(\"masks/*.tif\")))\n            if train_data_folder and train_data_folder.exists()\n            else None,\n            val_images=sorted(list(val_data_folder.glob(\"images/*.tif\")))\n            if val_data_folder and val_data_folder.exists()\n            else None,\n            val_masks=sorted(list(val_data_folder.glob(\"masks/*.tif\")))\n            if val_data_folder and val_data_folder.exists()\n            else None,\n            test_images=sorted(list(test_data_folder.glob(\"images/*.tif\")))\n            if test_data_folder and test_data_folder.exists()\n            else None,\n            test_masks=sorted(list(test_data_folder.glob(\"masks/*.tif\")))\n            if test_data_folder and test_data_folder.exists()\n            else None,\n            predict_images=sorted(list(predict_data_folder.rglob(\"*.tif\")))\n            if predict_data_folder and predict_data_folder.exists()\n            else None,\n            dataset_stats=dataset_stats,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_file_paths(\n        cls,\n        dataset_stats: Dict[str, Dict[str, float]],\n        train_images: Optional[List[Path]] = None,\n        train_masks: Optional[List[Path]] = None,\n        val_images: Optional[List[Path]] = None,\n        val_masks: Optional[List[Path]] = None,\n        test_images: Optional[List[Path]] = None,\n        test_masks: Optional[List[Path]] = None,\n        predict_images: Optional[List[Path]] = None,\n        spectral_indices: Optional[List[str]] = None,\n        batch_size: int = 32,\n        image_size: int = 352,\n        num_workers: int = 0,\n        **kwargs: Any,\n    ) -&gt; KelpForestDataModule:\n        \"\"\"\n        Factory method to create the KelpForestDataModule based on file paths.\n\n        Args:\n            dataset_stats: The per-band dataset statistics.\n            train_images: The list of training images.\n            train_masks: The list of training masks.\n            val_images: The list of validation images.\n            val_masks: The list of validation mask.\n            test_images: The list of test images.\n            test_masks: The list of test masks.\n            predict_images: The list of prediction images.\n            spectral_indices: The list of spectral indices to append to the input tensor.\n            batch_size: The batch size.\n            num_workers: The number of workers to use for data loading.\n            image_size: The size of the input image.\n            **kwargs: Other keyword arguments passed to the KelpForestDataModule constructor.\n\n        Returns: An instance of KelpForestDataModule.\n\n        \"\"\"\n        return cls(\n            train_images=train_images,\n            train_masks=train_masks,\n            val_images=val_images,\n            val_masks=val_masks,\n            test_images=test_images,\n            test_masks=test_masks,\n            predict_images=predict_images,\n            dataset_stats=dataset_stats,\n            spectral_indices=spectral_indices,\n            batch_size=batch_size,\n            image_size=image_size,\n            num_workers=num_workers,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.from_file_paths","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.from_file_paths</code>  <code>classmethod</code>","text":"<p>Factory method to create the KelpForestDataModule based on file paths.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_stats</code> <code>Dict[str, Dict[str, float]]</code> <p>The per-band dataset statistics.</p> required <code>train_images</code> <code>Optional[List[Path]]</code> <p>The list of training images.</p> <code>None</code> <code>train_masks</code> <code>Optional[List[Path]]</code> <p>The list of training masks.</p> <code>None</code> <code>val_images</code> <code>Optional[List[Path]]</code> <p>The list of validation images.</p> <code>None</code> <code>val_masks</code> <code>Optional[List[Path]]</code> <p>The list of validation mask.</p> <code>None</code> <code>test_images</code> <code>Optional[List[Path]]</code> <p>The list of test images.</p> <code>None</code> <code>test_masks</code> <code>Optional[List[Path]]</code> <p>The list of test masks.</p> <code>None</code> <code>predict_images</code> <code>Optional[List[Path]]</code> <p>The list of prediction images.</p> <code>None</code> <code>spectral_indices</code> <code>Optional[List[str]]</code> <p>The list of spectral indices to append to the input tensor.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>image_size</code> <code>int</code> <p>The size of the input image.</p> <code>352</code> <code>**kwargs</code> <code>Any</code> <p>Other keyword arguments passed to the KelpForestDataModule constructor.</p> <code>{}</code> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>@classmethod\ndef from_file_paths(\n    cls,\n    dataset_stats: Dict[str, Dict[str, float]],\n    train_images: Optional[List[Path]] = None,\n    train_masks: Optional[List[Path]] = None,\n    val_images: Optional[List[Path]] = None,\n    val_masks: Optional[List[Path]] = None,\n    test_images: Optional[List[Path]] = None,\n    test_masks: Optional[List[Path]] = None,\n    predict_images: Optional[List[Path]] = None,\n    spectral_indices: Optional[List[str]] = None,\n    batch_size: int = 32,\n    image_size: int = 352,\n    num_workers: int = 0,\n    **kwargs: Any,\n) -&gt; KelpForestDataModule:\n    \"\"\"\n    Factory method to create the KelpForestDataModule based on file paths.\n\n    Args:\n        dataset_stats: The per-band dataset statistics.\n        train_images: The list of training images.\n        train_masks: The list of training masks.\n        val_images: The list of validation images.\n        val_masks: The list of validation mask.\n        test_images: The list of test images.\n        test_masks: The list of test masks.\n        predict_images: The list of prediction images.\n        spectral_indices: The list of spectral indices to append to the input tensor.\n        batch_size: The batch size.\n        num_workers: The number of workers to use for data loading.\n        image_size: The size of the input image.\n        **kwargs: Other keyword arguments passed to the KelpForestDataModule constructor.\n\n    Returns: An instance of KelpForestDataModule.\n\n    \"\"\"\n    return cls(\n        train_images=train_images,\n        train_masks=train_masks,\n        val_images=val_images,\n        val_masks=val_masks,\n        test_images=test_images,\n        test_masks=test_masks,\n        predict_images=predict_images,\n        dataset_stats=dataset_stats,\n        spectral_indices=spectral_indices,\n        batch_size=batch_size,\n        image_size=image_size,\n        num_workers=num_workers,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.from_folders","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.from_folders</code>  <code>classmethod</code>","text":"<p>Factory method to create the KelpForestDataModule based on folder paths.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_stats</code> <code>Dict[str, Dict[str, float]]</code> <p>The per-band dataset statistics.</p> required <code>train_data_folder</code> <code>Optional[Path]</code> <p>The path to the training data folder.</p> <code>None</code> <code>val_data_folder</code> <code>Optional[Path]</code> <p>The path to the val data folder.</p> <code>None</code> <code>test_data_folder</code> <code>Optional[Path]</code> <p>The path to the test data folder.</p> <code>None</code> <code>predict_data_folder</code> <code>Optional[Path]</code> <p>The path to the prediction data folder.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Other keyword arguments passed to the KelpForestDataModule constructor.</p> <code>{}</code> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>@classmethod\ndef from_folders(\n    cls,\n    dataset_stats: Dict[str, Dict[str, float]],\n    train_data_folder: Optional[Path] = None,\n    val_data_folder: Optional[Path] = None,\n    test_data_folder: Optional[Path] = None,\n    predict_data_folder: Optional[Path] = None,\n    **kwargs: Any,\n) -&gt; KelpForestDataModule:\n    \"\"\"\n    Factory method to create the KelpForestDataModule based on folder paths.\n\n    Args:\n        dataset_stats: The per-band dataset statistics.\n        train_data_folder: The path to the training data folder.\n        val_data_folder: The path to the val data folder.\n        test_data_folder: The path to the test data folder.\n        predict_data_folder: The path to the prediction data folder.\n        **kwargs: Other keyword arguments passed to the KelpForestDataModule constructor.\n\n    Returns: An instance of KelpForestDataModule.\n\n    \"\"\"\n    return cls(\n        train_images=sorted(list(train_data_folder.glob(\"images/*.tif\")))\n        if train_data_folder and train_data_folder.exists()\n        else None,\n        train_masks=sorted(list(train_data_folder.glob(\"masks/*.tif\")))\n        if train_data_folder and train_data_folder.exists()\n        else None,\n        val_images=sorted(list(val_data_folder.glob(\"images/*.tif\")))\n        if val_data_folder and val_data_folder.exists()\n        else None,\n        val_masks=sorted(list(val_data_folder.glob(\"masks/*.tif\")))\n        if val_data_folder and val_data_folder.exists()\n        else None,\n        test_images=sorted(list(test_data_folder.glob(\"images/*.tif\")))\n        if test_data_folder and test_data_folder.exists()\n        else None,\n        test_masks=sorted(list(test_data_folder.glob(\"masks/*.tif\")))\n        if test_data_folder and test_data_folder.exists()\n        else None,\n        predict_images=sorted(list(predict_data_folder.rglob(\"*.tif\")))\n        if predict_data_folder and predict_data_folder.exists()\n        else None,\n        dataset_stats=dataset_stats,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.from_metadata_file","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.from_metadata_file</code>  <code>classmethod</code>","text":"<p>Factory method to create the KelpForestDataModule based on metadata file.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required <code>metadata_fp</code> <code>Path</code> <p>The path to the metadata file.</p> required <code>dataset_stats</code> <code>Dict[str, Dict[str, float]]</code> <p>The per-band dataset statistics.</p> required <code>cv_split</code> <code>int</code> <p>The CV fold number to use.</p> required <code>has_kelp_importance_factor</code> <code>float</code> <p>The importance factor for the has_kelp flag.</p> <code>1.0</code> <code>kelp_pixels_pct_importance_factor</code> <code>float</code> <p>The importance factor for the kelp_pixels_pct value.</p> <code>1.0</code> <code>qa_ok_importance_factor</code> <code>float</code> <p>The importance factor for the has_kelp flag.</p> <code>1.0</code> <code>almost_all_water_importance_factor</code> <code>float</code> <p>The importance factor for the almost_all_water flag.</p> <code>1.0</code> <code>qa_corrupted_pixels_pct_importance_factor</code> <code>float</code> <p>The importance factor for the qa_corrupted_pixels_pct value.</p> <code>-1.0</code> <code>dem_nan_pixels_pct_importance_factor</code> <code>float</code> <p>The importance factor for the dem_nan_pixels_pct value.</p> <code>-1.0</code> <code>dem_zero_pixels_pct_importance_factor</code> <code>float</code> <p>The importance factor for the dem_zero_pixels_pct value.</p> <code>-1.0</code> <code>sahi</code> <code>bool</code> <p>A flag indicating whether SAHI dataset is used.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Other keyword arguments passed to the KelpForestDataModule constructor.</p> <code>{}</code> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>@classmethod\ndef from_metadata_file(\n    cls,\n    data_dir: Path,\n    metadata_fp: Path,\n    dataset_stats: Dict[str, Dict[str, float]],\n    cv_split: int,\n    has_kelp_importance_factor: float = 1.0,\n    kelp_pixels_pct_importance_factor: float = 1.0,\n    qa_ok_importance_factor: float = 1.0,\n    almost_all_water_importance_factor: float = 1.0,\n    qa_corrupted_pixels_pct_importance_factor: float = -1.0,\n    dem_nan_pixels_pct_importance_factor: float = -1.0,\n    dem_zero_pixels_pct_importance_factor: float = -1.0,\n    sahi: bool = False,\n    **kwargs: Any,\n) -&gt; KelpForestDataModule:\n    \"\"\"\n    Factory method to create the KelpForestDataModule based on metadata file.\n\n    Args:\n        data_dir: The path to the data directory.\n        metadata_fp: The path to the metadata file.\n        dataset_stats: The per-band dataset statistics.\n        cv_split: The CV fold number to use.\n        has_kelp_importance_factor: The importance factor for the has_kelp flag.\n        kelp_pixels_pct_importance_factor: The importance factor for the kelp_pixels_pct value.\n        qa_ok_importance_factor: The importance factor for the has_kelp flag.\n        almost_all_water_importance_factor: The importance factor for the almost_all_water flag.\n        qa_corrupted_pixels_pct_importance_factor: The importance factor for the qa_corrupted_pixels_pct value.\n        dem_nan_pixels_pct_importance_factor: The importance factor for the dem_nan_pixels_pct value.\n        dem_zero_pixels_pct_importance_factor: The importance factor for the dem_zero_pixels_pct value.\n        sahi: A flag indicating whether SAHI dataset is used.\n        **kwargs: Other keyword arguments passed to the KelpForestDataModule constructor.\n\n    Returns: An instance of KelpForestDataModule.\n\n    \"\"\"\n    metadata = cls._calculate_image_weights(\n        df=pd.read_parquet(metadata_fp),\n        has_kelp_importance_factor=has_kelp_importance_factor,\n        kelp_pixels_pct_importance_factor=kelp_pixels_pct_importance_factor,\n        qa_ok_importance_factor=qa_ok_importance_factor,\n        qa_corrupted_pixels_pct_importance_factor=qa_corrupted_pixels_pct_importance_factor,\n        almost_all_water_importance_factor=almost_all_water_importance_factor,\n        dem_nan_pixels_pct_importance_factor=dem_nan_pixels_pct_importance_factor,\n        dem_zero_pixels_pct_importance_factor=dem_zero_pixels_pct_importance_factor,\n        sahi=sahi,\n    )\n    train_images, train_masks = cls.resolve_file_paths(\n        data_dir=data_dir, metadata=metadata, cv_split=cv_split, split=consts.data.TRAIN, sahi=sahi\n    )\n    val_images, val_masks = cls.resolve_file_paths(\n        data_dir=data_dir, metadata=metadata, cv_split=cv_split, split=consts.data.VAL, sahi=sahi\n    )\n    test_images, test_masks = cls.resolve_file_paths(\n        data_dir=data_dir, metadata=metadata, cv_split=cv_split, split=consts.data.VAL, sahi=sahi\n    )\n    image_weights = cls._resolve_image_weights(df=metadata, image_paths=train_images)\n    return cls(\n        train_images=train_images,\n        train_masks=train_masks,\n        val_images=val_images,\n        val_masks=val_masks,\n        test_images=test_images,\n        test_masks=test_masks,\n        predict_images=None,\n        image_weights=image_weights,\n        dataset_stats=dataset_stats,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.on_after_batch_transfer","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.on_after_batch_transfer</code>","text":"<p>Apply batch augmentations after batch is transferred to the device.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>mini-batch of data</p> required <code>batch_idx</code> <code>int</code> <p>batch index</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>augmented mini-batch</p> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>def on_after_batch_transfer(self, batch: Dict[str, Any], batch_idx: int) -&gt; Dict[str, Any]:\n    \"\"\"Apply batch augmentations after batch is transferred to the device.\n\n    Args:\n        batch: mini-batch of data\n        batch_idx: batch index\n\n    Returns:\n        augmented mini-batch\n    \"\"\"\n    if (\n        hasattr(self, \"trainer\")\n        and self.trainer is not None\n        and hasattr(self.trainer, \"training\")\n        and self.trainer.training\n    ):\n        batch = self._apply_transform(self.train_augmentations, batch)\n    elif (\n        hasattr(self, \"trainer\")\n        and self.trainer is not None\n        and hasattr(self.trainer, \"predicting\")\n        and self.trainer.predicting\n    ):\n        batch = self._apply_predict_transform(self.predict_augmentations, batch)\n    else:\n        batch = self._apply_transform(self.val_augmentations, batch)\n\n    return batch\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.plot_batch","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.plot_batch</code>","text":"<p>Run :meth:<code>kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_batch</code>.</p> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>def plot_batch(self, *args: Any, **kwargs: Any) -&gt; FigureGrids:\n    \"\"\"Run :meth:`kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_batch`.\"\"\"\n    return self.val_dataset.plot_batch(*args, **kwargs)\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.plot_sample","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.plot_sample</code>","text":"<p>Run :meth:<code>kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_sample</code>.</p> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>def plot_sample(self, *args: Any, **kwargs: Any) -&gt; plt.Figure:\n    \"\"\"Run :meth:`kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_sample`.\"\"\"\n    return self.val_dataset.plot_sample(*args, **kwargs)\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.predict_dataloader","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.predict_dataloader</code>","text":"<p>Return a DataLoader for prediction.</p> <p>Returns:</p> Type Description <code>DataLoader[Any]</code> <p>prediction data loader</p> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader[Any]:\n    \"\"\"Return a DataLoader for prediction.\n\n    Returns:\n        prediction data loader\n    \"\"\"\n    return DataLoader(\n        self.predict_dataset,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        shuffle=False,\n    )\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.resolve_file_paths","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.resolve_file_paths</code>  <code>classmethod</code>","text":"<p>Resolves file paths using specified metadata dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The data directory.</p> required <code>metadata</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required <code>cv_split</code> <code>int</code> <p>The CV fold to use.</p> required <code>split</code> <code>str</code> <p>The split to use (train, val, test).</p> required <code>sahi</code> <code>bool</code> <p>A flag indicating whether SAHI dataset is used.</p> <code>False</code> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>@classmethod\ndef resolve_file_paths(\n    cls,\n    data_dir: Path,\n    metadata: pd.DataFrame,\n    cv_split: int,\n    split: str,\n    sahi: bool = False,\n) -&gt; Tuple[List[Path], List[Path]]:\n    \"\"\"\n    Resolves file paths using specified metadata dataframe.\n\n    Args:\n        data_dir: The data directory.\n        metadata: The metadata dataframe.\n        cv_split: The CV fold to use.\n        split: The split to use (train, val, test).\n        sahi: A flag indicating whether SAHI dataset is used.\n\n    Returns: A tuple with input image paths and target (mask) image paths\n\n    \"\"\"\n    split_data = metadata[metadata[f\"split_{cv_split}\"] == split]\n    img_folder = consts.data.TRAIN if split in [consts.data.TRAIN, consts.data.VAL] else consts.data.TEST\n    image_paths = sorted(\n        split_data.apply(\n            lambda row: data_dir\n            / img_folder\n            / \"images\"\n            / (\n                f\"{row['tile_id']}_satellite_{row['j']}_{row['i']}.tif\"\n                if sahi\n                else f\"{row['tile_id']}_satellite.tif\"\n            ),\n            axis=1,\n        ).tolist()\n    )\n    mask_paths = sorted(\n        split_data.apply(\n            lambda row: data_dir\n            / img_folder\n            / \"masks\"\n            / (f\"{row['tile_id']}_kelp_{row['j']}_{row['i']}.tif\" if sahi else f\"{row['tile_id']}_kelp.tif\"),\n            axis=1,\n        ).tolist()\n    )\n    return image_paths, mask_paths\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.setup","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.setup</code>","text":"<p>Initialize the main <code>Dataset</code> objects.</p> <p>This method is called once per GPU per run.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>stage to set up</p> <code>None</code> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"Initialize the main ``Dataset`` objects.\n\n    This method is called once per GPU per run.\n\n    Args:\n        stage: stage to set up\n    \"\"\"\n    if self.train_images:\n        self.train_dataset = self._build_dataset(self.train_images, self.train_masks)\n    if self.val_images:\n        self.val_dataset = self._build_dataset(self.val_images, self.val_masks)\n    if self.test_images:\n        self.test_dataset = self._build_dataset(self.test_images, self.test_masks)\n    if self.predict_images:\n        self.predict_dataset = self._build_dataset(self.predict_images)\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.test_dataloader","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.test_dataloader</code>","text":"<p>Return a DataLoader for testing.</p> <p>Returns:</p> Type Description <code>DataLoader[Any]</code> <p>testing data loader</p> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader[Any]:\n    \"\"\"Return a DataLoader for testing.\n\n    Returns:\n        testing data loader\n    \"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        shuffle=False,\n    )\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.train_dataloader","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.train_dataloader</code>","text":"<p>Return a DataLoader for training.</p> <p>Returns:</p> Type Description <code>DataLoader[Any]</code> <p>training data loader</p> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader[Any]:\n    \"\"\"Return a DataLoader for training.\n\n    Returns:\n        training data loader\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        sampler=WeightedRandomSampler(\n            weights=self.image_weights,\n            num_samples=self.samples_per_epoch,\n        )\n        if self.use_weighted_sampler\n        else None,\n        shuffle=True if not self.use_weighted_sampler else False,\n    )\n</code></pre>"},{"location":"api_ref/nn/data/datamodule/#kelp.nn.data.datamodule.KelpForestDataModule.val_dataloader","title":"<code>kelp.nn.data.datamodule.KelpForestDataModule.val_dataloader</code>","text":"<p>Return a DataLoader for validation.</p> <p>Returns:</p> Type Description <code>DataLoader[Any]</code> <p>validation data loader</p> Source code in <code>kelp/nn/data/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader[Any]:\n    \"\"\"Return a DataLoader for validation.\n\n    Returns:\n        validation data loader\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        shuffle=False,\n    )\n</code></pre>"},{"location":"api_ref/nn/data/dataset/","title":"dataset","text":"<p>The Kelp Forest Dataset.</p>"},{"location":"api_ref/nn/data/dataset/#kelp.nn.data.dataset.FigureGrids","title":"<code>kelp.nn.data.dataset.FigureGrids</code>  <code>dataclass</code>","text":"<p>A dataclass for holding figure grids.</p> Source code in <code>kelp/nn/data/dataset.py</code> <pre><code>@dataclass\nclass FigureGrids:\n    \"\"\"\n    A dataclass for holding figure grids.\n    \"\"\"\n\n    true_color: Optional[plt.Figure] = None\n    color_infrared: Optional[plt.Figure] = None\n    short_wave_infrared: Optional[plt.Figure] = None\n    mask: Optional[plt.Figure] = None\n    prediction: Optional[plt.Figure] = None\n    qa: Optional[plt.Figure] = None\n    dem: Optional[plt.Figure] = None\n    spectral_indices: Optional[Dict[str, plt.Figure]] = None\n</code></pre>"},{"location":"api_ref/nn/data/dataset/#kelp.nn.data.dataset.KelpForestSegmentationDataset","title":"<code>kelp.nn.data.dataset.KelpForestSegmentationDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>The KelpForestSegmentationDataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_fps</code> <code>List[Path]</code> <p>The input image paths.</p> required <code>mask_fps</code> <code>Optional[List[Path]]</code> <p>The mask image paths.</p> <code>None</code> <code>transforms</code> <code>Optional[Callable[[Dict[str, Tensor]], Dict[str, Tensor]]]</code> <p>The transforms to apply to the input images and masks.</p> <code>None</code> <code>band_order</code> <code>Optional[List[int]]</code> <p>The order of bands to use.</p> <code>None</code> <code>fill_value</code> <code>float</code> <p>The fill value for missing pixels.</p> <code>0.0</code> Source code in <code>kelp/nn/data/dataset.py</code> <pre><code>class KelpForestSegmentationDataset(Dataset):\n    \"\"\"\n    The KelpForestSegmentationDataset.\n\n    Args:\n        image_fps: The input image paths.\n        mask_fps: The mask image paths.\n        transforms: The transforms to apply to the input images and masks.\n        band_order: The order of bands to use.\n        fill_value: The fill value for missing pixels.\n\n    \"\"\"\n\n    classes = consts.data.CLASSES\n    cmap = ListedColormap([\"black\", \"lightseagreen\"])\n\n    def __init__(\n        self,\n        image_fps: List[Path],\n        mask_fps: Optional[List[Path]] = None,\n        transforms: Optional[Callable[[Dict[str, Tensor]], Dict[str, Tensor]]] = None,\n        band_order: Optional[List[int]] = None,\n        fill_value: float = 0.0,\n    ) -&gt; None:\n        self.image_fps = image_fps\n        self.mask_fps = mask_fps\n        self.transforms = transforms\n        self.fill_value = fill_value\n        self.band_order = [band_idx + 1 for band_idx in band_order] if band_order else list(range(1, 8))\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns The number of images in the dataset.\n\n        Returns: The number of images in the dataset.\n\n        \"\"\"\n        return len(self.image_fps)\n\n    def __getitem__(self, index: int) -&gt; Dict[str, Tensor]:\n        \"\"\"\n        Loads a single image and mask from the dataset.\n\n        Args:\n            index: The index of the image to load.\n\n        Returns: A dictionary with the image and mask tensor pair.\n\n        \"\"\"\n        src: DatasetReader\n        with rasterio.open(self.image_fps[index]) as src:\n            # we need to replace values to account for corrupted pixels\n            img = torch.from_numpy(src.read(self.band_order)).float()\n            img = torch.where(img == -32768, self.fill_value, img)\n\n        sample = {\"image\": img, \"tile_id\": self.image_fps[index].stem.split(\"_\")[0]}\n\n        if self.mask_fps:\n            with rasterio.open(self.mask_fps[index]) as src:\n                target = torch.from_numpy(src.read(1))\n                sample[\"mask\"] = target\n\n        if self.transforms:\n            sample = self.transforms(sample)\n\n        sample = self._ensure_proper_sample_format(sample)\n\n        return sample\n\n    @staticmethod\n    def _ensure_proper_sample_format(sample: Dict[str, Tensor]) -&gt; Dict[str, Tensor]:\n        \"\"\"Transform a single sample from the Dataset.\n\n        Args:\n            sample: dictionary containing image and mask\n\n        Returns:\n            preprocessed sample\n        \"\"\"\n        sample[\"image\"] = sample[\"image\"].float()\n\n        if \"mask\" in sample:\n            sample[\"mask\"] = sample[\"mask\"].long()\n\n        return sample\n\n    @staticmethod\n    def plot_sample(\n        sample: Dict[str, Tensor],\n        show_titles: bool = True,\n        suptitle: Optional[str] = None,\n    ) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            show_titles: flag indicating whether to show titles above each panel\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        \"\"\"\n        image = sample[\"image\"].numpy()\n        mask = sample[\"mask\"].squeeze().numpy() if \"mask\" in sample else None\n        predictions = sample[\"prediction\"].numpy() if \"prediction\" in sample else None\n\n        fig = plot_sample(\n            input_arr=image,\n            target_arr=mask,\n            predictions_arr=predictions,\n            show_titles=show_titles,\n            suptitle=suptitle or f\"Tile ID: {sample['tile_id']}\",\n        )\n        return fig\n\n    @staticmethod\n    def _plot_tensor(\n        tensor: Tensor,\n        interpolation: Literal[\"antialiased\", \"none\"] = \"antialiased\",\n        cmap: Optional[str] = None,\n    ) -&gt; plt.Figure:\n        \"\"\"\n        Plot a single tensor.\n\n        Args:\n            tensor: The tensor.\n            interpolation: The interpolation mode.\n            cmap: An optional colormap to use.\n\n        Returns: A matplotlib Figure with the rendered tensor.\n\n        \"\"\"\n        tensor = tensor.float()\n        h, w = tensor.shape[-2], tensor.shape[-1]\n        fig: plt.Figure\n        axes: Axes\n        fig, axes = plt.subplots(ncols=1, nrows=1, squeeze=True, figsize=(w / 100, h / 100))\n        img = tensor.detach()\n        img = F.to_pil_image(img)\n        axes.imshow(np.asarray(img), interpolation=interpolation, cmap=cmap)\n        axes.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n        plt.tight_layout(pad=0)\n        return fig\n\n    @staticmethod\n    def plot_batch(\n        batch: Dict[str, Tensor],\n        band_index_lookup: Dict[str, int],\n        samples_per_row: int = 8,\n        plot_true_color: bool = False,\n        plot_color_infrared_grid: bool = False,\n        plot_short_wave_infrared_grid: bool = False,\n        plot_spectral_indices: bool = False,\n        plot_qa_grid: bool = False,\n        plot_dem_grid: bool = False,\n        plot_mask_grid: bool = False,\n        plot_prediction_grid: bool = False,\n        dem_cmap: str = \"viridis\",\n        spectral_indices_cmap: str = \"viridis\",\n        qa_mask_cmap: str = \"gray\",\n        mask_cmap: str = consts.data.CMAP,\n    ) -&gt; FigureGrids:\n        \"\"\"\n        Plots a batch of images generated using this dataset.\n\n        Args:\n            batch: The dictionary containing a batch of images with optional masks and predictions.\n            band_index_lookup: The dictionary containing a lookup that matches band name to its index in the tensor.\n            samples_per_row: The number of samples per row to plot in a grid.\n            plot_true_color: A flag indicating whether to plot the True Color composite.\n            plot_color_infrared_grid: A flag indicating whether to plot the Color Infrared composite.\n            plot_short_wave_infrared_grid: A flag indicating whether to plot the Shortwave Infrared composite.\n            plot_spectral_indices: A flag indicating whether to plot the spectral indices.\n            plot_qa_grid: A flag indicating whether to plot the QA band.\n            plot_dem_grid: A flag indicating whether to plot the DEM band.\n            plot_mask_grid: A flag indicating whether to plot the mask grid.\n            plot_prediction_grid: A flag indicating whether to plot the prediction grid.\n            dem_cmap: The matplotlib colormap to use for the DEM band.\n            spectral_indices_cmap: The matplotlib colormap to use for the spectral indices.\n            qa_mask_cmap: The matplotlib colormap to use for the QA band.\n            mask_cmap: The matplotlib colormap to use for the masks and predictions.\n\n        Returns: A FigureGrid instance with plotted grids.\n\n        \"\"\"\n        if plot_mask_grid and \"mask\" not in batch:\n            raise ValueError(\n                \"Mask grid cannot be plotted. No 'mask' key is present in the batch. \"\n                f\"Found following keys: {list(batch.keys())}\"\n            )\n\n        if plot_prediction_grid and \"prediction\" not in batch:\n            raise ValueError(\n                \"Prediction grid cannot be plotted. No 'prediction' key is present in the batch. \"\n                f\"Found following keys: {list(batch.keys())}\"\n            )\n\n        image = batch[\"image\"]\n        vmin = torch.amin(image, dim=(2, 3)).unsqueeze(2).unsqueeze(3)\n        vmax = torch.amax(image, dim=(2, 3)).unsqueeze(2).unsqueeze(3)\n        normalized = (image - vmin) / (vmax - vmin + consts.data.EPS)\n\n        image_grid = make_grid(normalized, nrow=samples_per_row)\n\n        return FigureGrids(\n            true_color=KelpForestSegmentationDataset._plot_tensor(\n                tensor=image_grid[(band_index_lookup[\"R\"], band_index_lookup[\"G\"], band_index_lookup[\"B\"]), :, :],\n            )\n            if plot_true_color\n            else None,\n            color_infrared=KelpForestSegmentationDataset._plot_tensor(\n                tensor=image_grid[(band_index_lookup[\"NIR\"], band_index_lookup[\"R\"], band_index_lookup[\"G\"]), :, :],\n            )\n            if plot_color_infrared_grid\n            else None,\n            short_wave_infrared=KelpForestSegmentationDataset._plot_tensor(\n                tensor=image_grid[(band_index_lookup[\"SWIR\"], band_index_lookup[\"NIR\"], band_index_lookup[\"R\"]), :, :],\n            )\n            if plot_short_wave_infrared_grid\n            else None,\n            mask=KelpForestSegmentationDataset._plot_tensor(\n                tensor=make_grid(batch[\"mask\"].unsqueeze(1), nrow=samples_per_row)[0, :, :],\n                interpolation=\"none\",\n                cmap=mask_cmap,\n            )\n            if plot_mask_grid\n            else None,\n            prediction=KelpForestSegmentationDataset._plot_tensor(\n                tensor=make_grid(batch[\"prediction\"].unsqueeze(1), nrow=samples_per_row)[0, :, :],\n                interpolation=\"none\",\n                cmap=mask_cmap,\n            )\n            if plot_prediction_grid\n            else None,\n            qa=KelpForestSegmentationDataset._plot_tensor(\n                tensor=image_grid[band_index_lookup[\"QA\"], :, :],\n                interpolation=\"none\",\n                cmap=qa_mask_cmap,\n            )\n            if plot_qa_grid\n            else None,\n            dem=KelpForestSegmentationDataset._plot_tensor(\n                tensor=image_grid[band_index_lookup[\"DEM\"], :, :],\n                cmap=dem_cmap,\n            )\n            if plot_dem_grid\n            else None,\n            spectral_indices={\n                band_name: KelpForestSegmentationDataset._plot_tensor(\n                    tensor=image_grid[band_index_lookup[band_name], :, :],\n                    interpolation=\"none\" if band_name.endswith(\"WM\") else \"antialiased\",\n                    cmap=qa_mask_cmap if band_name.endswith(\"WM\") else spectral_indices_cmap,\n                )\n                for band_name, band_number in band_index_lookup.items()\n                if band_name not in consts.data.ORIGINAL_BANDS\n            }\n            if plot_spectral_indices\n            else None,\n        )\n</code></pre>"},{"location":"api_ref/nn/data/dataset/#kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_batch","title":"<code>kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_batch</code>  <code>staticmethod</code>","text":"<p>Plots a batch of images generated using this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>The dictionary containing a batch of images with optional masks and predictions.</p> required <code>band_index_lookup</code> <code>Dict[str, int]</code> <p>The dictionary containing a lookup that matches band name to its index in the tensor.</p> required <code>samples_per_row</code> <code>int</code> <p>The number of samples per row to plot in a grid.</p> <code>8</code> <code>plot_true_color</code> <code>bool</code> <p>A flag indicating whether to plot the True Color composite.</p> <code>False</code> <code>plot_color_infrared_grid</code> <code>bool</code> <p>A flag indicating whether to plot the Color Infrared composite.</p> <code>False</code> <code>plot_short_wave_infrared_grid</code> <code>bool</code> <p>A flag indicating whether to plot the Shortwave Infrared composite.</p> <code>False</code> <code>plot_spectral_indices</code> <code>bool</code> <p>A flag indicating whether to plot the spectral indices.</p> <code>False</code> <code>plot_qa_grid</code> <code>bool</code> <p>A flag indicating whether to plot the QA band.</p> <code>False</code> <code>plot_dem_grid</code> <code>bool</code> <p>A flag indicating whether to plot the DEM band.</p> <code>False</code> <code>plot_mask_grid</code> <code>bool</code> <p>A flag indicating whether to plot the mask grid.</p> <code>False</code> <code>plot_prediction_grid</code> <code>bool</code> <p>A flag indicating whether to plot the prediction grid.</p> <code>False</code> <code>dem_cmap</code> <code>str</code> <p>The matplotlib colormap to use for the DEM band.</p> <code>'viridis'</code> <code>spectral_indices_cmap</code> <code>str</code> <p>The matplotlib colormap to use for the spectral indices.</p> <code>'viridis'</code> <code>qa_mask_cmap</code> <code>str</code> <p>The matplotlib colormap to use for the QA band.</p> <code>'gray'</code> <code>mask_cmap</code> <code>str</code> <p>The matplotlib colormap to use for the masks and predictions.</p> <code>CMAP</code> Source code in <code>kelp/nn/data/dataset.py</code> <pre><code>@staticmethod\ndef plot_batch(\n    batch: Dict[str, Tensor],\n    band_index_lookup: Dict[str, int],\n    samples_per_row: int = 8,\n    plot_true_color: bool = False,\n    plot_color_infrared_grid: bool = False,\n    plot_short_wave_infrared_grid: bool = False,\n    plot_spectral_indices: bool = False,\n    plot_qa_grid: bool = False,\n    plot_dem_grid: bool = False,\n    plot_mask_grid: bool = False,\n    plot_prediction_grid: bool = False,\n    dem_cmap: str = \"viridis\",\n    spectral_indices_cmap: str = \"viridis\",\n    qa_mask_cmap: str = \"gray\",\n    mask_cmap: str = consts.data.CMAP,\n) -&gt; FigureGrids:\n    \"\"\"\n    Plots a batch of images generated using this dataset.\n\n    Args:\n        batch: The dictionary containing a batch of images with optional masks and predictions.\n        band_index_lookup: The dictionary containing a lookup that matches band name to its index in the tensor.\n        samples_per_row: The number of samples per row to plot in a grid.\n        plot_true_color: A flag indicating whether to plot the True Color composite.\n        plot_color_infrared_grid: A flag indicating whether to plot the Color Infrared composite.\n        plot_short_wave_infrared_grid: A flag indicating whether to plot the Shortwave Infrared composite.\n        plot_spectral_indices: A flag indicating whether to plot the spectral indices.\n        plot_qa_grid: A flag indicating whether to plot the QA band.\n        plot_dem_grid: A flag indicating whether to plot the DEM band.\n        plot_mask_grid: A flag indicating whether to plot the mask grid.\n        plot_prediction_grid: A flag indicating whether to plot the prediction grid.\n        dem_cmap: The matplotlib colormap to use for the DEM band.\n        spectral_indices_cmap: The matplotlib colormap to use for the spectral indices.\n        qa_mask_cmap: The matplotlib colormap to use for the QA band.\n        mask_cmap: The matplotlib colormap to use for the masks and predictions.\n\n    Returns: A FigureGrid instance with plotted grids.\n\n    \"\"\"\n    if plot_mask_grid and \"mask\" not in batch:\n        raise ValueError(\n            \"Mask grid cannot be plotted. No 'mask' key is present in the batch. \"\n            f\"Found following keys: {list(batch.keys())}\"\n        )\n\n    if plot_prediction_grid and \"prediction\" not in batch:\n        raise ValueError(\n            \"Prediction grid cannot be plotted. No 'prediction' key is present in the batch. \"\n            f\"Found following keys: {list(batch.keys())}\"\n        )\n\n    image = batch[\"image\"]\n    vmin = torch.amin(image, dim=(2, 3)).unsqueeze(2).unsqueeze(3)\n    vmax = torch.amax(image, dim=(2, 3)).unsqueeze(2).unsqueeze(3)\n    normalized = (image - vmin) / (vmax - vmin + consts.data.EPS)\n\n    image_grid = make_grid(normalized, nrow=samples_per_row)\n\n    return FigureGrids(\n        true_color=KelpForestSegmentationDataset._plot_tensor(\n            tensor=image_grid[(band_index_lookup[\"R\"], band_index_lookup[\"G\"], band_index_lookup[\"B\"]), :, :],\n        )\n        if plot_true_color\n        else None,\n        color_infrared=KelpForestSegmentationDataset._plot_tensor(\n            tensor=image_grid[(band_index_lookup[\"NIR\"], band_index_lookup[\"R\"], band_index_lookup[\"G\"]), :, :],\n        )\n        if plot_color_infrared_grid\n        else None,\n        short_wave_infrared=KelpForestSegmentationDataset._plot_tensor(\n            tensor=image_grid[(band_index_lookup[\"SWIR\"], band_index_lookup[\"NIR\"], band_index_lookup[\"R\"]), :, :],\n        )\n        if plot_short_wave_infrared_grid\n        else None,\n        mask=KelpForestSegmentationDataset._plot_tensor(\n            tensor=make_grid(batch[\"mask\"].unsqueeze(1), nrow=samples_per_row)[0, :, :],\n            interpolation=\"none\",\n            cmap=mask_cmap,\n        )\n        if plot_mask_grid\n        else None,\n        prediction=KelpForestSegmentationDataset._plot_tensor(\n            tensor=make_grid(batch[\"prediction\"].unsqueeze(1), nrow=samples_per_row)[0, :, :],\n            interpolation=\"none\",\n            cmap=mask_cmap,\n        )\n        if plot_prediction_grid\n        else None,\n        qa=KelpForestSegmentationDataset._plot_tensor(\n            tensor=image_grid[band_index_lookup[\"QA\"], :, :],\n            interpolation=\"none\",\n            cmap=qa_mask_cmap,\n        )\n        if plot_qa_grid\n        else None,\n        dem=KelpForestSegmentationDataset._plot_tensor(\n            tensor=image_grid[band_index_lookup[\"DEM\"], :, :],\n            cmap=dem_cmap,\n        )\n        if plot_dem_grid\n        else None,\n        spectral_indices={\n            band_name: KelpForestSegmentationDataset._plot_tensor(\n                tensor=image_grid[band_index_lookup[band_name], :, :],\n                interpolation=\"none\" if band_name.endswith(\"WM\") else \"antialiased\",\n                cmap=qa_mask_cmap if band_name.endswith(\"WM\") else spectral_indices_cmap,\n            )\n            for band_name, band_number in band_index_lookup.items()\n            if band_name not in consts.data.ORIGINAL_BANDS\n        }\n        if plot_spectral_indices\n        else None,\n    )\n</code></pre>"},{"location":"api_ref/nn/data/dataset/#kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_sample","title":"<code>kelp.nn.data.dataset.KelpForestSegmentationDataset.plot_sample</code>  <code>staticmethod</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>show_titles</code> <code>bool</code> <p>flag indicating whether to show titles above each panel</p> <code>True</code> <code>suptitle</code> <code>Optional[str]</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> Source code in <code>kelp/nn/data/dataset.py</code> <pre><code>@staticmethod\ndef plot_sample(\n    sample: Dict[str, Tensor],\n    show_titles: bool = True,\n    suptitle: Optional[str] = None,\n) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        show_titles: flag indicating whether to show titles above each panel\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    \"\"\"\n    image = sample[\"image\"].numpy()\n    mask = sample[\"mask\"].squeeze().numpy() if \"mask\" in sample else None\n    predictions = sample[\"prediction\"].numpy() if \"prediction\" in sample else None\n\n    fig = plot_sample(\n        input_arr=image,\n        target_arr=mask,\n        predictions_arr=predictions,\n        show_titles=show_titles,\n        suptitle=suptitle or f\"Tile ID: {sample['tile_id']}\",\n    )\n    return fig\n</code></pre>"},{"location":"api_ref/nn/data/transforms/","title":"transforms","text":"<p>The augmentation transforms related classes and helpers.</p>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.MinMaxNormalize","title":"<code>kelp.nn.data.transforms.MinMaxNormalize</code>","text":"<p>             Bases: <code>Module</code></p> <p>Min-Max normalization transform that uses provided min and max per-channel values for image transformation.</p> <p>Parameters:</p> Name Type Description Default <code>min_vals</code> <code>Tensor</code> <p>A Tensor of min values per-channel.</p> required <code>max_vals</code> <code>Tensor</code> <p>A Tensor of max values per-channel.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>class MinMaxNormalize(Module):\n    \"\"\"\n    Min-Max normalization transform that uses provided min and max per-channel values for image transformation.\n\n    Args:\n        min_vals: A Tensor of min values per-channel.\n        max_vals: A Tensor of max values per-channel.\n    \"\"\"\n\n    def __init__(self, min_vals: Tensor, max_vals: Tensor) -&gt; None:\n        super().__init__()\n        self.mins = min_vals.view(1, -1, 1, 1)\n        self.maxs = max_vals.view(1, -1, 1, 1)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        mins = torch.as_tensor(self.mins, device=x.device, dtype=x.dtype)\n        maxs = torch.as_tensor(self.maxs, device=x.device, dtype=x.dtype)\n        x = x.clamp(mins, maxs)\n        x = (x - mins) / (maxs - mins + consts.data.EPS)\n        return x\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.PerSampleMinMaxNormalize","title":"<code>kelp.nn.data.transforms.PerSampleMinMaxNormalize</code>","text":"<p>             Bases: <code>Module</code></p> <p>A per-sample normalization transform that will calculate min and max per-channel on the fly.</p> Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>class PerSampleMinMaxNormalize(Module):\n    \"\"\"\n    A per-sample normalization transform that will calculate min and max per-channel on the fly.\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Runs the normalization transform for specified batch of images.\n\n        Args:\n            x: The batch of images.\n\n        Returns: A batch of normalized images.\n\n        \"\"\"\n        vmin = torch.amin(x, dim=(2, 3)).unsqueeze(2).unsqueeze(3)\n        vmax = torch.amax(x, dim=(2, 3)).unsqueeze(2).unsqueeze(3)\n        return (x - vmin) / (vmax - vmin + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.PerSampleMinMaxNormalize.forward","title":"<code>kelp.nn.data.transforms.PerSampleMinMaxNormalize.forward</code>","text":"<p>Runs the normalization transform for specified batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The batch of images.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Runs the normalization transform for specified batch of images.\n\n    Args:\n        x: The batch of images.\n\n    Returns: A batch of normalized images.\n\n    \"\"\"\n    vmin = torch.amin(x, dim=(2, 3)).unsqueeze(2).unsqueeze(3)\n    vmax = torch.amax(x, dim=(2, 3)).unsqueeze(2).unsqueeze(3)\n    return (x - vmin) / (vmax - vmin + consts.data.EPS)\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.PerSampleQuantileNormalize","title":"<code>kelp.nn.data.transforms.PerSampleQuantileNormalize</code>","text":"<p>             Bases: <code>Module</code></p> <p>A per-sample normalization transform that will calculate min and max per-channel on the fly using provided quantile values.</p> <p>Parameters:</p> Name Type Description Default <code>q_low</code> <code>float</code> <p>The lower quantile value.</p> required <code>q_high</code> <code>float</code> <p>The upper quantile value.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>class PerSampleQuantileNormalize(Module):\n    \"\"\"\n    A per-sample normalization transform that will calculate min and max per-channel on the fly\n    using provided quantile values.\n\n    Args:\n        q_low: The lower quantile value.\n        q_high: The upper quantile value.\n\n    \"\"\"\n\n    def __init__(self, q_low: float, q_high: float) -&gt; None:\n        super().__init__()\n        self.q_low = q_low\n        self.q_high = q_high\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Runs the normalization transform for specified batch of images.\n\n        Args:\n            x: The batch of images.\n\n        Returns: A batch of normalized images.\n\n        \"\"\"\n        flattened_sample = x.view(x.shape[0], x.shape[1], -1)\n        vmin = torch.quantile(flattened_sample, self.q_low, dim=2).unsqueeze(2).unsqueeze(3)\n        vmax = torch.quantile(flattened_sample, self.q_high, dim=2).unsqueeze(2).unsqueeze(3)\n        x = x.clamp(vmin, vmax)\n        x = (x - vmin) / (vmax - vmin + consts.data.EPS)\n        return x\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.PerSampleQuantileNormalize.forward","title":"<code>kelp.nn.data.transforms.PerSampleQuantileNormalize.forward</code>","text":"<p>Runs the normalization transform for specified batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The batch of images.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Runs the normalization transform for specified batch of images.\n\n    Args:\n        x: The batch of images.\n\n    Returns: A batch of normalized images.\n\n    \"\"\"\n    flattened_sample = x.view(x.shape[0], x.shape[1], -1)\n    vmin = torch.quantile(flattened_sample, self.q_low, dim=2).unsqueeze(2).unsqueeze(3)\n    vmax = torch.quantile(flattened_sample, self.q_high, dim=2).unsqueeze(2).unsqueeze(3)\n    x = x.clamp(vmin, vmax)\n    x = (x - vmin) / (vmax - vmin + consts.data.EPS)\n    return x\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.RemoveNaNs","title":"<code>kelp.nn.data.transforms.RemoveNaNs</code>","text":"<p>             Bases: <code>Module</code></p> <p>Removes NaN values from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>min_vals</code> <code>Tensor</code> <p>The min values per-channel to use when removing NaNs and neg-Inf.</p> required <code>max_vals</code> <code>Tensor</code> <p>The min values per-channel to use when removing positive-Inf.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>class RemoveNaNs(Module):\n    \"\"\"\n    Removes NaN values from the input tensor.\n\n    Args:\n        min_vals: The min values per-channel to use when removing NaNs and neg-Inf.\n        max_vals: The min values per-channel to use when removing positive-Inf.\n\n    \"\"\"\n\n    def __init__(self, min_vals: Tensor, max_vals: Tensor) -&gt; None:\n        super().__init__()\n        self.mins = min_vals.view(1, -1, 1, 1)\n        self.maxs = max_vals.view(1, -1, 1, 1)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Runs the transform for specified batch of images.\n\n        Args:\n            x: The batch of images.\n\n        Returns: A batch of normalized images.\n\n        \"\"\"\n        mins = torch.as_tensor(self.mins, device=x.device, dtype=x.dtype)\n        maxs = torch.as_tensor(self.maxs, device=x.device, dtype=x.dtype)\n        x = torch.where(torch.isnan(x), mins, x)\n        x = torch.where(torch.isneginf(x), mins, x)\n        x = torch.where(torch.isinf(x), maxs, x)\n        return x\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.RemoveNaNs.forward","title":"<code>kelp.nn.data.transforms.RemoveNaNs.forward</code>","text":"<p>Runs the transform for specified batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The batch of images.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Runs the transform for specified batch of images.\n\n    Args:\n        x: The batch of images.\n\n    Returns: A batch of normalized images.\n\n    \"\"\"\n    mins = torch.as_tensor(self.mins, device=x.device, dtype=x.dtype)\n    maxs = torch.as_tensor(self.maxs, device=x.device, dtype=x.dtype)\n    x = torch.where(torch.isnan(x), mins, x)\n    x = torch.where(torch.isneginf(x), mins, x)\n    x = torch.where(torch.isinf(x), maxs, x)\n    return x\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.RemovePadding","title":"<code>kelp.nn.data.transforms.RemovePadding</code>","text":"<p>             Bases: <code>Module</code></p> <p>Removes specified padding from the input tensors.</p> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>The size of the target image after padding removal.</p> required <code>padded_image_size</code> <code>int</code> <p>The size of the padded image before padding removal.</p> required <code>args</code> <code>Any</code> <p>Arguments passed to super class.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments passed to super class.</p> <code>{}</code> Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>class RemovePadding(nn.Module):\n    \"\"\"\n    Removes specified padding from the input tensors.\n\n    Args:\n        image_size: The size of the target image after padding removal.\n        padded_image_size: The size of the padded image before padding removal.\n        args: Arguments passed to super class.\n        kwargs: Keyword arguments passed to super class.\n\n    \"\"\"\n\n    def __init__(self, image_size: int, padded_image_size: int, *args: Any, **kwargs: Any) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.padding_to_trim = (padded_image_size - image_size) // 2\n        self.crop_upper_bound = image_size + self.padding_to_trim\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Runs the transform for specified batch of images.\n\n        Args:\n            x: The batch of images.\n\n        Returns: A batch of normalized images.\n\n        \"\"\"\n        x = x.squeeze()\n        x = x[self.padding_to_trim : self.crop_upper_bound, self.padding_to_trim : self.crop_upper_bound]\n        return x\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.RemovePadding.forward","title":"<code>kelp.nn.data.transforms.RemovePadding.forward</code>","text":"<p>Runs the transform for specified batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The batch of images.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Runs the transform for specified batch of images.\n\n    Args:\n        x: The batch of images.\n\n    Returns: A batch of normalized images.\n\n    \"\"\"\n    x = x.squeeze()\n    x = x[self.padding_to_trim : self.crop_upper_bound, self.padding_to_trim : self.crop_upper_bound]\n    return x\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.build_append_index_transforms","title":"<code>kelp.nn.data.transforms.build_append_index_transforms</code>","text":"<p>Build an append index transforms based on specified spectral indices.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_indices</code> <code>List[str]</code> <p>A list of spectral indices to use.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def build_append_index_transforms(spectral_indices: List[str]) -&gt; Callable[[Tensor], Tensor]:\n    \"\"\"\n    Build an append index transforms based on specified spectral indices.\n\n    Args:\n        spectral_indices: A list of spectral indices to use.\n\n    Returns: A callable that can be used to transform batch of images.\n\n    \"\"\"\n    transforms = K.AugmentationSequential(\n        AppendDEMWM(  # type: ignore\n            index_dem=BAND_INDEX_LOOKUP[\"DEM\"],\n            index_qa=BAND_INDEX_LOOKUP[\"QA\"],\n        ),\n        *[\n            SPECTRAL_INDEX_LOOKUP[index_name](\n                index_swir=BAND_INDEX_LOOKUP[\"SWIR\"],\n                index_nir=BAND_INDEX_LOOKUP[\"NIR\"],\n                index_red=BAND_INDEX_LOOKUP[\"R\"],\n                index_green=BAND_INDEX_LOOKUP[\"G\"],\n                index_blue=BAND_INDEX_LOOKUP[\"B\"],\n                index_dem=BAND_INDEX_LOOKUP[\"DEM\"],\n                index_qa=BAND_INDEX_LOOKUP[\"QA\"],\n                index_water_mask=BAND_INDEX_LOOKUP[\"DEMWM\"],\n                mask_using_qa=not index_name.endswith(\"WM\"),\n                mask_using_water_mask=not index_name.endswith(\"WM\"),\n                fill_val=torch.nan,\n            )\n            for index_name in spectral_indices\n            if index_name != \"DEMWM\"\n        ],\n        data_keys=[\"input\"],\n    ).to(DEVICE)\n    return transforms  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.min_max_normalize","title":"<code>kelp.nn.data.transforms.min_max_normalize</code>","text":"<p>Runs min-max normalization on the input array by calculating min and max per-channel values on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The array to normalize.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def min_max_normalize(arr: np.ndarray) -&gt; np.ndarray:  # type: ignore[type-arg]\n    \"\"\"\n    Runs min-max normalization on the input array by calculating min and max per-channel values on the fly.\n\n    Args:\n        arr: The array to normalize.\n\n    Returns: Normalized array.\n\n    \"\"\"\n    vmin = np.nanmin(arr, axis=(0, 1))\n    vmax = np.nanmax(arr, axis=(0, 1))\n    arr = arr.clip(0, vmax)\n    arr = (arr - vmin) / (vmax - vmin + consts.data.EPS)\n    arr = arr.clip(0, 1)\n    return arr\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.quantile_min_max_normalize","title":"<code>kelp.nn.data.transforms.quantile_min_max_normalize</code>","text":"<p>Runs min-max quantile normalization on the input array by calculating min and max per-channel values on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The array to normalize.</p> required <code>q_lower</code> <code>float</code> <p>The lower quantile.</p> <code>0.01</code> <code>q_upper</code> <code>float</code> <p>The upper quantile.</p> <code>0.99</code> Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def quantile_min_max_normalize(\n    x: np.ndarray,  # type: ignore[type-arg]\n    q_lower: float = 0.01,\n    q_upper: float = 0.99,\n) -&gt; np.ndarray:  # type: ignore[type-arg]\n    \"\"\"\n    Runs min-max quantile normalization on the input array by calculating min and max per-channel values on the fly.\n\n    Args:\n        x: The array to normalize.\n        q_lower: The lower quantile.\n        q_upper: The upper quantile.\n\n    Returns: Normalized array.\n\n    \"\"\"\n    vmin = np.expand_dims(np.expand_dims(np.quantile(x, q=q_lower, axis=(1, 2)), 1), 2)\n    vmax = np.expand_dims(np.expand_dims(np.quantile(x, q=q_upper, axis=(1, 2)), 1), 2)\n    return (x - vmin) / (vmax - vmin + consts.data.EPS)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.resolve_normalization_stats","title":"<code>kelp.nn.data.transforms.resolve_normalization_stats</code>","text":"<p>Resolves normalization stats based on specified bands to use.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_stats</code> <code>Dict[str, Dict[str, float]]</code> <p>The full per-band dataset statistics.</p> required <code>bands_to_use</code> <code>List[str]</code> <p>The list of band names to use.</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def resolve_normalization_stats(\n    dataset_stats: Dict[str, Dict[str, float]],\n    bands_to_use: List[str],\n) -&gt; Tuple[BandStats, int]:\n    \"\"\"\n    Resolves normalization stats based on specified bands to use.\n\n    Args:\n        dataset_stats: The full per-band dataset statistics.\n        bands_to_use: The list of band names to use.\n\n    Returns: A tuple of stats and the number of bands to use.\n\n    \"\"\"\n    band_stats = {band: dataset_stats[band] for band in bands_to_use}\n    mean = [val[\"mean\"] for val in band_stats.values()]\n    std = [val[\"std\"] for val in band_stats.values()]\n    vmin = [val[\"min\"] for val in band_stats.values()]\n    vmax = [val[\"max\"] for val in band_stats.values()]\n    q01 = [val[\"q01\"] for val in band_stats.values()]\n    q99 = [val[\"q99\"] for val in band_stats.values()]\n    stats = BandStats(\n        mean=torch.tensor(mean),\n        std=torch.tensor(std),\n        min=torch.tensor(vmin),\n        max=torch.tensor(vmax),\n        q01=torch.tensor(q01),\n        q99=torch.tensor(q99),\n    )\n    return stats, len(band_stats)\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.resolve_normalization_transform","title":"<code>kelp.nn.data.transforms.resolve_normalization_transform</code>","text":"<p>Resolves the normalization transform.</p> <p>Parameters:</p> Name Type Description Default <code>band_stats</code> <code>BandStats</code> <p>The band statistics.</p> required <code>normalization_strategy</code> <code>Literal['min-max', 'quantile', 'per-sample-min-max', 'per-sample-quantile', 'z-score']</code> <p>The normalization strategy.</p> <code>'quantile'</code> Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def resolve_normalization_transform(\n    band_stats: BandStats,\n    normalization_strategy: Literal[\n        \"min-max\",\n        \"quantile\",\n        \"per-sample-min-max\",\n        \"per-sample-quantile\",\n        \"z-score\",\n    ] = \"quantile\",\n) -&gt; Union[_AugmentationBase, nn.Module]:\n    \"\"\"\n    Resolves the normalization transform.\n\n    Args:\n        band_stats: The band statistics.\n        normalization_strategy: The normalization strategy.\n\n    Returns: A normalization transform to use for the image batch.\n\n    \"\"\"\n    if normalization_strategy == \"z-score\":\n        return K.Normalize(band_stats.mean, band_stats.std)  # type: ignore[no-any-return]\n    elif normalization_strategy == \"min-max\":\n        return MinMaxNormalize(min_vals=band_stats.min, max_vals=band_stats.max)\n    elif normalization_strategy == \"quantile\":\n        return MinMaxNormalize(min_vals=band_stats.q01, max_vals=band_stats.q99)\n    elif normalization_strategy == \"per-sample-quantile\":\n        return PerSampleQuantileNormalize(q_low=0.01, q_high=0.99)\n    elif normalization_strategy == \"per-sample-min-max\":\n        return PerSampleMinMaxNormalize()\n    else:\n        raise ValueError(f\"{normalization_strategy} is not supported!\")\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.resolve_resize_transform","title":"<code>kelp.nn.data.transforms.resolve_resize_transform</code>","text":"<p>Resolves the input image and mask resize transform.</p> <p>Parameters:</p> Name Type Description Default <code>image_or_mask</code> <code>Literal['image', 'mask']</code> <p>Indicates if the transform is for an image or a mask.</p> required <code>resize_strategy</code> <code>Literal['pad', 'resize']</code> <p>The resize strategy to use.</p> <code>'pad'</code> <code>image_size</code> <code>int</code> <p>The size of the resized image.</p> <code>352</code> <code>interpolation</code> <code>Literal['nearest', 'nearest-exact', 'bilinear', 'bicubic']</code> <p>The interpolation method to use for the \"resize\" strategy.</p> <code>'nearest'</code> Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def resolve_resize_transform(\n    image_or_mask: Literal[\"image\", \"mask\"],\n    resize_strategy: Literal[\"pad\", \"resize\"] = \"pad\",\n    image_size: int = 352,\n    interpolation: Literal[\"nearest\", \"nearest-exact\", \"bilinear\", \"bicubic\"] = \"nearest\",\n) -&gt; Callable[[Tensor], Tensor]:\n    \"\"\"\n    Resolves the input image and mask resize transform.\n\n    Args:\n        image_or_mask: Indicates if the transform is for an image or a mask.\n        resize_strategy: The resize strategy to use.\n        image_size: The size of the resized image.\n        interpolation: The interpolation method to use for the \"resize\" strategy.\n\n    Returns:\n\n    \"\"\"\n    interpolation_lookup = {\n        \"nearest\": InterpolationMode.NEAREST,\n        \"nearest-exact\": InterpolationMode.NEAREST_EXACT,\n        \"bilinear\": InterpolationMode.BILINEAR,\n        \"bicubic\": InterpolationMode.BICUBIC,\n    }\n    if resize_strategy == \"pad\":\n        if image_size &lt; 352:\n            raise ValueError(\"Invalid resize strategy. Padding is only applicable when image size is greater than 352.\")\n        return T.Pad(  # type: ignore[no-any-return]\n            padding=[\n                (image_size - consts.data.TILE_SIZE) // 2,\n            ],\n            fill=0,\n            padding_mode=\"constant\",\n        )\n    elif resize_strategy == \"resize\":\n        return T.Resize(  # type: ignore[no-any-return]\n            size=(image_size, image_size),\n            interpolation=interpolation_lookup[interpolation]\n            if image_or_mask == \"image\"\n            else InterpolationMode.NEAREST,\n            antialias=False,\n        )\n    else:\n        raise ValueError(f\"{resize_strategy=} is not supported!\")\n</code></pre>"},{"location":"api_ref/nn/data/transforms/#kelp.nn.data.transforms.resolve_transforms","title":"<code>kelp.nn.data.transforms.resolve_transforms</code>","text":"<p>Resolves batch augmentation transformations to be used based on specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_indices</code> <code>List[str]</code> <p>The list of spectral indices to use.</p> required <code>band_index_lookup</code> <code>Dict[str, int]</code> <p>The dictionary mapping band name to index in the input tensor.</p> required <code>band_stats</code> <code>BandStats</code> <p>The band statistics to use.</p> required <code>mask_using_qa</code> <code>bool</code> <p>A flag indicating whether to mask spectral indices with QA band.</p> required <code>mask_using_water_mask</code> <code>bool</code> <p>A flag indicating whether to mask spectral indices with DEM Water Mask.</p> required <code>normalization_transform</code> <code>Union[_AugmentationBase, Module]</code> <p>A normalization transformation.</p> required <code>stage</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>A literal indicating the stage to use. One of [\"train\", \"val\", \"test\", \"predict\"].</p> required Source code in <code>kelp/nn/data/transforms.py</code> <pre><code>def resolve_transforms(\n    spectral_indices: List[str],\n    band_index_lookup: Dict[str, int],\n    band_stats: BandStats,\n    mask_using_qa: bool,\n    mask_using_water_mask: bool,\n    normalization_transform: Union[_AugmentationBase, nn.Module],\n    stage: Literal[\"train\", \"val\", \"test\", \"predict\"],\n) -&gt; K.AugmentationSequential:\n    \"\"\"\n    Resolves batch augmentation transformations to be used based on specified configuration.\n\n    Args:\n        spectral_indices: The list of spectral indices to use.\n        band_index_lookup: The dictionary mapping band name to index in the input tensor.\n        band_stats: The band statistics to use.\n        mask_using_qa: A flag indicating whether to mask spectral indices with QA band.\n        mask_using_water_mask: A flag indicating whether to mask spectral indices with DEM Water Mask.\n        normalization_transform: A normalization transformation.\n        stage: A literal indicating the stage to use. One of [\"train\", \"val\", \"test\", \"predict\"].\n\n    Returns: An instance of AugmentationSequential.\n\n    \"\"\"\n    common_transforms = []\n\n    for index_name in spectral_indices:\n        common_transforms.append(\n            SPECTRAL_INDEX_LOOKUP[index_name](\n                index_swir=band_index_lookup.get(\"SWIR\", -1),\n                index_nir=band_index_lookup.get(\"NIR\", -1),\n                index_red=band_index_lookup.get(\"R\", -1),\n                index_green=band_index_lookup.get(\"G\", -1),\n                index_blue=band_index_lookup.get(\"B\", -1),\n                index_dem=band_index_lookup.get(\"DEM\", -1),\n                index_qa=band_index_lookup.get(\"QA\", -1),\n                index_water_mask=band_index_lookup.get(\"DEMWM\", -1),\n                mask_using_qa=False if index_name.endswith(\"WM\") else mask_using_qa,\n                mask_using_water_mask=False if index_name.endswith(\"WM\") else mask_using_water_mask,\n                fill_val=torch.nan,\n            )\n        )\n\n    common_transforms.extend(\n        [\n            RemoveNaNs(min_vals=band_stats.min, max_vals=band_stats.max),\n            normalization_transform,\n        ]\n    )\n\n    if stage == \"train\":\n        return K.AugmentationSequential(\n            *common_transforms,\n            K.RandomRotation(p=0.5, degrees=90),\n            K.RandomHorizontalFlip(p=0.5),\n            K.RandomVerticalFlip(p=0.5),\n            data_keys=[\"input\", \"mask\"],\n        )\n    else:\n        return K.AugmentationSequential(\n            *common_transforms,\n            data_keys=[\"input\"] if stage == \"predict\" else [\"input\", \"mask\"],\n        )\n</code></pre>"},{"location":"api_ref/nn/data/utils/","title":"utils","text":"<p>The data utils.</p>"},{"location":"api_ref/nn/data/utils/#kelp.nn.data.utils.unbind_samples","title":"<code>kelp.nn.data.utils.unbind_samples</code>","text":"<p>Reverse of :func:<code>stack_samples</code>.</p> <p>Useful for turning a mini-batch of samples into a list of samples. These individual samples can then be plotted using a dataset's <code>plot</code> method.</p> <p>Taken from <code>torchgeo</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Dict[Any, Sequence[Any]]</code> <p>a mini-batch of samples</p> required <p>Returns:</p> Type Description <code>List[Dict[Any, Any]]</code> <p>list of samples</p> Source code in <code>kelp/nn/data/utils.py</code> <pre><code>def unbind_samples(sample: Dict[Any, Sequence[Any]]) -&gt; List[Dict[Any, Any]]:\n    \"\"\"Reverse of :func:`stack_samples`.\n\n    Useful for turning a mini-batch of samples into a list of samples. These individual\n    samples can then be plotted using a dataset's ``plot`` method.\n\n    Taken from `torchgeo`.\n\n    Args:\n        sample: a mini-batch of samples\n\n    Returns:\n         list of samples\n\n    \"\"\"\n    for key, values in sample.items():\n        if isinstance(values, Tensor):\n            sample[key] = torch.unbind(values)\n    return _dict_list_to_list_dict(sample)\n</code></pre>"},{"location":"api_ref/nn/inference/average_predictions/","title":"average_predictions","text":"<p>Inference logic for averaging predictions from multiple folds.</p>"},{"location":"api_ref/nn/inference/average_predictions/#kelp.nn.inference.average_predictions.AveragePredictionsConfig","title":"<code>kelp.nn.inference.average_predictions.AveragePredictionsConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>Config for running prediction averaging logic.</p> Source code in <code>kelp/nn/inference/average_predictions.py</code> <pre><code>class AveragePredictionsConfig(ConfigBase):\n    \"\"\"Config for running prediction averaging logic.\"\"\"\n\n    predictions_dirs: List[Path]\n    output_dir: Path\n    decision_threshold: float = 0.5\n    weights: List[float]\n    preview_submission: bool = False\n    test_data_dir: Optional[Path] = None\n    preview_first_n: int = 10\n\n    @model_validator(mode=\"before\")\n    def validate_cfg(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n        if values[\"preview_submission\"] and values.get(\"test_data_dir\", None) is None:\n            raise ValueError(\"Please provide test_data_dir param if running submission preview!\")\n        return values\n</code></pre>"},{"location":"api_ref/nn/inference/average_predictions/#kelp.nn.inference.average_predictions.average_predictions","title":"<code>kelp.nn.inference.average_predictions.average_predictions</code>","text":"<p>Average predictions given a list of directories with predictions from single models.</p> <p>Parameters:</p> Name Type Description Default <code>preds_dirs</code> <code>List[Path]</code> <p>The list of directories with predictions from single model.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required <code>weights</code> <code>List[float]</code> <p>The list of weights for each fold (prediction directory).</p> required <code>decision_threshold</code> <code>float</code> <p>The final decision threshold.</p> <code>0.5</code> Source code in <code>kelp/nn/inference/average_predictions.py</code> <pre><code>def average_predictions(\n    preds_dirs: List[Path],\n    output_dir: Path,\n    weights: List[float],\n    decision_threshold: float = 0.5,\n) -&gt; None:\n    \"\"\"\n    Average predictions given a list of directories with predictions from single models.\n\n    Args:\n        preds_dirs: The list of directories with predictions from single model.\n        output_dir: The output directory.\n        weights: The list of weights for each fold (prediction directory).\n        decision_threshold: The final decision threshold.\n\n    \"\"\"\n    if len(weights) != len(preds_dirs):\n        raise ValueError(\"Number of weights must match the number prediction dirs!\")\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n    predictions: Dict[str, Dict[str, Union[np.ndarray, float, int]]] = {}  # type: ignore[type-arg]\n\n    for preds_dir, weight in zip(preds_dirs, weights):\n        if weight == 0.0:\n            _logger.info(f\"Weight for {preds_dir.name} == 0.0. Skipping this fold.\")\n            continue\n        for pred_file in tqdm(\n            sorted(list(preds_dir.glob(\"*.tif\"))),\n            desc=f\"Processing files for {preds_dir.name}, {weight=}\",\n        ):\n            file_name = pred_file.name\n            with rasterio.open(pred_file) as src:\n                pred_array = src.read(1) * weight\n                if file_name not in predictions:\n                    predictions[file_name] = {\n                        \"data\": np.zeros_like(pred_array, dtype=np.float32),\n                        \"count\": 1,\n                        \"weight_sum\": weight,\n                    }\n                predictions[file_name][\"data\"] += pred_array\n                predictions[file_name][\"count\"] += 1\n                predictions[file_name][\"weight_sum\"] += weight\n\n    for file_name, content in tqdm(predictions.items(), desc=\"Saving predictions\"):\n        content[\"data\"] = content[\"data\"] / content[\"weight_sum\"]\n        content[\"data\"] = np.where(content[\"data\"] &gt;= decision_threshold, 1, 0).astype(np.uint8)\n        output_file = output_dir / file_name\n        with rasterio.open(output_file, \"w\", **META) as dst:\n            dst.write(content[\"data\"].astype(rasterio.uint8), 1)  # type: ignore[union-attr]\n</code></pre>"},{"location":"api_ref/nn/inference/average_predictions/#kelp.nn.inference.average_predictions.main","title":"<code>kelp.nn.inference.average_predictions.main</code>","text":"<p>Main entrypoint for averaging the predictions and creating a submission file.</p> Source code in <code>kelp/nn/inference/average_predictions.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for averaging the predictions and creating a submission file.\"\"\"\n    cfg = parse_args()\n    now = datetime.utcnow().isoformat()\n    out_dir = cfg.output_dir / now\n    preds_dir = cfg.output_dir / now / \"predictions\"\n    preds_dir.mkdir(exist_ok=False, parents=True)\n    avg_preds_config = cfg.model_dump(mode=\"json\")\n    (out_dir / \"predict_config.yaml\").write_text(yaml.dump(avg_preds_config))\n    average_predictions(\n        preds_dirs=cfg.predictions_dirs,\n        output_dir=preds_dir,\n        weights=cfg.weights,\n        decision_threshold=cfg.decision_threshold,\n    )\n    create_submission_tar(\n        preds_dir=preds_dir,\n        output_dir=out_dir,\n    )\n    if cfg.preview_submission:\n        plot_first_n_samples(\n            data_dir=cfg.test_data_dir,  # type: ignore[arg-type]\n            submission_dir=out_dir,\n            output_dir=out_dir / \"previews\",\n            n=cfg.preview_first_n,\n        )\n</code></pre>"},{"location":"api_ref/nn/inference/average_predictions/#kelp.nn.inference.average_predictions.parse_args","title":"<code>kelp.nn.inference.average_predictions.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of AveragePredictionsConfig.</p> Source code in <code>kelp/nn/inference/average_predictions.py</code> <pre><code>def parse_args() -&gt; AveragePredictionsConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of AveragePredictionsConfig.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--predictions_dirs\", nargs=\"*\", required=True)\n    parser.add_argument(\"--weights\", nargs=\"*\", required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    parser.add_argument(\"--decision_threshold\", type=float, default=0.5)\n    parser.add_argument(\"--preview_submission\", action=\"store_true\")\n    parser.add_argument(\"--test_data_dir\", type=str)\n    parser.add_argument(\"--preview_first_n\", type=int, default=10)\n    args = parser.parse_args()\n    cfg = AveragePredictionsConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/nn/inference/fold_weights/","title":"fold_weights","text":"<p>Logic for calculating fold weights proportional to individual LB scores.</p>"},{"location":"api_ref/nn/inference/fold_weights/#kelp.nn.inference.fold_weights.main","title":"<code>kelp.nn.inference.fold_weights.main</code>","text":"<p>Main entrypoint for calculating fold weights.</p> Source code in <code>kelp/nn/inference/fold_weights.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for calculating fold weights.\"\"\"\n    fold_scores = [\n        (\"fold=0\", 0.7110),\n        (\"fold=1\", 0.7086),\n        (\"fold=2\", 0.7110),\n        (\"fold=3\", 0.7139),\n        (\"fold=4\", 0.7106),\n        (\"fold=5\", 0.7100),\n        (\"fold=6\", 0.7119),\n        (\"fold=7\", 0.7105),\n        (\"fold=8\", 0.7155),\n        (\"fold=9\", 0.7047),\n        (\"fold=0v2\", 0.7135),\n        (\"fold=1v2\", 0.7114),\n        (\"fold=2v2\", 0.7094),\n        (\"fold=3v2\", 0.7133),\n        (\"fold=4v2\", 0.7106),\n    ]\n    df = pd.DataFrame(fold_scores, columns=[\"fold\", \"score\"])\n\n    scaler = MinMaxScaler(feature_range=(0.2, 1.0))\n    norm_scores = scaler.fit_transform(df[[\"score\"]].values)\n    df[\"score_norm\"] = norm_scores\n    df.to_parquet(\"scores.parquet\")\n</code></pre>"},{"location":"api_ref/nn/inference/predict/","title":"predict","text":"<p>Single model prediction logic.</p>"},{"location":"api_ref/nn/inference/predict/#kelp.nn.inference.predict.PredictConfig","title":"<code>kelp.nn.inference.predict.PredictConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>The prediction config</p> Source code in <code>kelp/nn/inference/predict.py</code> <pre><code>class PredictConfig(ConfigBase):\n    \"\"\"The prediction config\"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n\n    data_dir: Path\n    dataset_stats_dir: Path\n    original_training_config_fp: Path\n    model_checkpoint: Path\n    use_checkpoint: Literal[\"best\", \"latest\"] = \"best\"\n    run_dir: Path\n    output_dir: Path\n    tta: bool = False\n    soft_labels: bool = False\n    tta_merge_mode: str = \"max\"\n    decision_threshold: Optional[float] = None\n    precision: Optional[\n        Literal[\n            \"16-true\",\n            \"16-mixed\",\n            \"bf16-true\",\n            \"bf16-mixed\",\n            \"32-true\",\n        ]\n    ] = None\n    sahi_tile_size: int = 128\n    sahi_overlap: int = 64\n\n    @model_validator(mode=\"before\")\n    def validate_inputs(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        run_dir = Path(data[\"run_dir\"])\n        if (run_dir / \"model\").exists():\n            artifacts_dir = run_dir\n        elif (run_dir / \"artifacts\").exists():\n            artifacts_dir = run_dir / \"artifacts\"\n        else:\n            raise ValueError(\"Could not find nor model dir nor artifacts folder in the specified run_dir\")\n\n        model_checkpoint = artifacts_dir / \"model\"\n\n        if (checkpoints_root := (artifacts_dir / \"model\" / \"checkpoints\")).exists():\n            if data[\"use_checkpoint\"] == \"latest\":\n                model_checkpoint = checkpoints_root / \"last\" / \"last.ckpt\"\n            else:\n                for checkpoint_dir in sorted(list(checkpoints_root.iterdir())):\n                    aliases = (checkpoint_dir / \"aliases.txt\").read_text()\n                    if \"'best'\" in aliases:\n                        model_checkpoint = checkpoints_root / checkpoint_dir.name / f\"{checkpoint_dir.name}.ckpt\"\n                        break\n\n        config_fp = artifacts_dir / \"config.yaml\"\n        data[\"model_checkpoint\"] = model_checkpoint\n        data[\"original_training_config_fp\"] = config_fp\n        return data\n\n    @property\n    def training_config(self) -&gt; TrainConfig:\n        with open(self.original_training_config_fp, \"r\") as f:\n            cfg = TrainConfig(**yaml.safe_load(f))\n        cfg.data_dir = self.data_dir\n        cfg.dataset_stats_fp = self.dataset_stats_dir / cfg.dataset_stats_fp.name.replace(\"%3A\", \":\")\n        cfg.output_dir = self.output_dir\n        if self.precision is not None:\n            cfg.precision = self.precision\n        return cfg\n\n    @property\n    def use_mlflow(self) -&gt; bool:\n        return self.model_checkpoint.is_dir()\n</code></pre>"},{"location":"api_ref/nn/inference/predict/#kelp.nn.inference.predict.build_prediction_arg_parser","title":"<code>kelp.nn.inference.predict.build_prediction_arg_parser</code>","text":"<p>Builds a base prediction argument parser.</p> <p>Returns: An instance of :argparse.ArgumentParser.</p> Source code in <code>kelp/nn/inference/predict.py</code> <pre><code>def build_prediction_arg_parser() -&gt; argparse.ArgumentParser:\n    \"\"\"\n    Builds a base prediction argument parser.\n\n    Returns: An instance of :argparse.ArgumentParser.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    parser.add_argument(\"--dataset_stats_dir\", type=str, required=True)\n    parser.add_argument(\"--run_dir\", type=str, required=True)\n    parser.add_argument(\"--use_checkpoint\", choices=[\"latest\", \"best\"], type=str, default=\"best\")\n    parser.add_argument(\"--tta\", action=\"store_true\")\n    parser.add_argument(\"--soft_labels\", action=\"store_true\")\n    parser.add_argument(\"--tta_merge_mode\", type=str, default=\"max\")\n    parser.add_argument(\"--decision_threshold\", type=float)\n    parser.add_argument(\"--sahi_tile_size\", type=int, default=128)\n    parser.add_argument(\"--sahi_overlap\", type=int, default=64)\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        choices=[\n            \"16-true\",\n            \"16-mixed\",\n            \"bf16-true\",\n            \"bf16-mixed\",\n            \"32-true\",\n        ],\n    )\n    return parser\n</code></pre>"},{"location":"api_ref/nn/inference/predict/#kelp.nn.inference.predict.main","title":"<code>kelp.nn.inference.predict.main</code>","text":"<p>Main entry point for performing model prediction. Will automatically use SAHI if model was trained with this flag.</p> Source code in <code>kelp/nn/inference/predict.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"\n    Main entry point for performing model prediction.\n    Will automatically use SAHI if model was trained with this flag.\n    \"\"\"\n    cfg = parse_args()\n    (cfg.output_dir / \"predict_config.yaml\").write_text(yaml.dump(cfg.model_dump(mode=\"json\")))\n    if cfg.training_config.sahi:\n        run_sahi_prediction(\n            data_dir=cfg.data_dir,\n            output_dir=cfg.output_dir,\n            model_checkpoint=cfg.model_checkpoint,\n            use_mlflow=cfg.use_mlflow,\n            train_cfg=cfg.training_config,\n            tta=cfg.tta,\n            soft_labels=cfg.soft_labels,\n            tta_merge_mode=cfg.tta_merge_mode,\n            decision_threshold=cfg.decision_threshold,\n            sahi_tile_size=cfg.sahi_tile_size,\n            sahi_overlap=cfg.sahi_overlap,\n        )\n    else:\n        run_prediction(\n            data_dir=cfg.data_dir,\n            output_dir=cfg.output_dir,\n            model_checkpoint=cfg.model_checkpoint,\n            use_mlflow=cfg.use_mlflow,\n            train_cfg=cfg.training_config,\n            tta=cfg.tta,\n            soft_labels=cfg.soft_labels,\n            tta_merge_mode=cfg.tta_merge_mode,\n            decision_threshold=cfg.decision_threshold,\n        )\n</code></pre>"},{"location":"api_ref/nn/inference/predict/#kelp.nn.inference.predict.parse_args","title":"<code>kelp.nn.inference.predict.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of PredictConfig.</p> Source code in <code>kelp/nn/inference/predict.py</code> <pre><code>def parse_args() -&gt; PredictConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of PredictConfig.\n\n    \"\"\"\n    parser = build_prediction_arg_parser()\n    args = parser.parse_args()\n    cfg = PredictConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/nn/inference/predict/#kelp.nn.inference.predict.predict","title":"<code>kelp.nn.inference.predict.predict</code>","text":"<p>Runs prediction using specified datamodule and model.</p> <p>Parameters:</p> Name Type Description Default <code>dm</code> <code>LightningDataModule</code> <p>The datamodule to use for prediction.</p> required <code>model</code> <code>LightningModule</code> <p>The model.</p> required <code>train_cfg</code> <code>TrainConfig</code> <p>The original training configuration.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required <code>resize_tf</code> <code>Callable[[Tensor], Tensor]</code> <p>The resize transform for post-prediction adjustment.</p> required Source code in <code>kelp/nn/inference/predict.py</code> <pre><code>@torch.inference_mode()\ndef predict(\n    dm: pl.LightningDataModule,\n    model: pl.LightningModule,\n    train_cfg: TrainConfig,\n    output_dir: Path,\n    resize_tf: Callable[[Tensor], Tensor],\n) -&gt; None:\n    \"\"\"\n    Runs prediction using specified datamodule and model.\n\n    Args:\n        dm: The datamodule to use for prediction.\n        model: The model.\n        train_cfg: The original training configuration.\n        output_dir: The output directory.\n        resize_tf: The resize transform for post-prediction adjustment.\n\n    \"\"\"\n    with torch.no_grad():\n        trainer = pl.Trainer(**train_cfg.trainer_kwargs, logger=False)\n        preds: List[Dict[str, Union[Tensor, str]]] = trainer.predict(model=model, datamodule=dm)\n        for prediction_batch in tqdm(preds, \"Saving prediction batches\"):\n            individual_samples = unbind_samples(prediction_batch)\n            for sample in individual_samples:\n                tile_id = sample[\"tile_id\"]\n                prediction = sample[\"prediction\"]\n                if model.hyperparams.get(\"soft_labels\", False):\n                    META[\"dtype\"] = \"float32\"\n                dest: DatasetWriter\n                with rasterio.open(output_dir / f\"{tile_id}_kelp.tif\", \"w\", **META) as dest:\n                    prediction_arr = resize_tf(prediction.unsqueeze(0)).detach().cpu().numpy().squeeze()\n                    dest.write(prediction_arr, 1)\n</code></pre>"},{"location":"api_ref/nn/inference/predict/#kelp.nn.inference.predict.resolve_post_predict_resize_transform","title":"<code>kelp.nn.inference.predict.resolve_post_predict_resize_transform</code>","text":"<p>Resolves the post-predict resize transform.</p> <p>Parameters:</p> Name Type Description Default <code>resize_strategy</code> <code>Literal['resize', 'pad']</code> <p>The resize strategy.</p> required <code>source_image_size</code> <code>int</code> <p>The source image size.</p> required <code>target_image_size</code> <code>int</code> <p>The target image size.</p> required Source code in <code>kelp/nn/inference/predict.py</code> <pre><code>def resolve_post_predict_resize_transform(\n    resize_strategy: Literal[\"resize\", \"pad\"],\n    source_image_size: int,\n    target_image_size: int,\n) -&gt; Callable[[Tensor], Tensor]:\n    \"\"\"\n    Resolves the post-predict resize transform.\n\n    Args:\n        resize_strategy: The resize strategy.\n        source_image_size: The source image size.\n        target_image_size: The target image size.\n\n    Returns: The transform to be called on predictions.\n\n    \"\"\"\n    if resize_strategy == \"resize\":\n        resize_tf = T.Resize(\n            size=(target_image_size, target_image_size),\n            interpolation=InterpolationMode.NEAREST,\n            antialias=False,\n        )\n    elif resize_strategy == \"pad\":\n        resize_tf = RemovePadding(image_size=target_image_size, padded_image_size=source_image_size)\n    else:\n        raise ValueError(f\"{resize_strategy=} is not supported\")\n    return resize_tf  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/nn/inference/predict/#kelp.nn.inference.predict.run_prediction","title":"<code>kelp.nn.inference.predict.run_prediction</code>","text":"<p>Runs the prediction logic for a single model checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required <code>output_dir</code> <code>Path</code> <p>The path to the output directory.</p> required <code>model_checkpoint</code> <code>Path</code> <p>The model checkpoint.</p> required <code>use_mlflow</code> <code>bool</code> <p>A flag indicating whether to use MLflow to load the model.</p> required <code>train_cfg</code> <code>TrainConfig</code> <p>The original training config used to train the model.</p> required <code>tta</code> <code>bool</code> <p>A flag indicating whether to use TTA for prediction.</p> <code>False</code> <code>soft_labels</code> <code>bool</code> <p>A flag indicating whether to use soft labels for prediction.</p> <code>False</code> <code>tta_merge_mode</code> <code>str</code> <p>The TTA merge mode.</p> <code>'max'</code> <code>decision_threshold</code> <code>Optional[float]</code> <p>An optional decision threshold for prediction. torch.argmax will be used by default.</p> <code>None</code> Source code in <code>kelp/nn/inference/predict.py</code> <pre><code>def run_prediction(\n    data_dir: Path,\n    output_dir: Path,\n    model_checkpoint: Path,\n    use_mlflow: bool,\n    train_cfg: TrainConfig,\n    tta: bool = False,\n    soft_labels: bool = False,\n    tta_merge_mode: str = \"max\",\n    decision_threshold: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Runs the prediction logic for a single model checkpoint.\n\n    Args:\n        data_dir: The path to the data directory.\n        output_dir: The path to the output directory.\n        model_checkpoint: The model checkpoint.\n        use_mlflow: A flag indicating whether to use MLflow to load the model.\n        train_cfg: The original training config used to train the model.\n        tta: A flag indicating whether to use TTA for prediction.\n        soft_labels: A flag indicating whether to use soft labels for prediction.\n        tta_merge_mode: The TTA merge mode.\n        decision_threshold: An optional decision threshold for prediction. torch.argmax will be used by default.\n\n    \"\"\"\n    dm = KelpForestDataModule.from_folders(predict_data_folder=data_dir, **train_cfg.data_module_kwargs)\n    model = load_model(\n        model_path=model_checkpoint,\n        use_mlflow=use_mlflow,\n        tta=tta,\n        soft_labels=soft_labels,\n        tta_merge_mode=tta_merge_mode,\n        decision_threshold=decision_threshold,\n    )\n    resize_tf = resolve_post_predict_resize_transform(\n        resize_strategy=train_cfg.resize_strategy,\n        source_image_size=train_cfg.image_size,\n        target_image_size=consts.data.TILE_SIZE,\n    )\n    predict(\n        dm=dm,\n        model=model,\n        train_cfg=train_cfg,\n        output_dir=output_dir,\n        resize_tf=resize_tf,\n    )\n</code></pre>"},{"location":"api_ref/nn/inference/predict/#kelp.nn.inference.predict.run_sahi_prediction","title":"<code>kelp.nn.inference.predict.run_sahi_prediction</code>","text":"<p>Runs SAHI (Sliced Aided Hyper Inference) using specified model checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the data directory.</p> required <code>output_dir</code> <code>Path</code> <p>The path to the output directory.</p> required <code>model_checkpoint</code> <code>Path</code> <p>The model checkpoint.</p> required <code>use_mlflow</code> <code>bool</code> <p>A flag indicating whether to use MLflow to load the model.</p> required <code>train_cfg</code> <code>TrainConfig</code> <p>The original training config used to train the model.</p> required <code>tta</code> <code>bool</code> <p>A flag indicating whether to use TTA for prediction.</p> <code>False</code> <code>soft_labels</code> <code>bool</code> <p>A flag indicating whether to use soft labels for prediction.</p> <code>False</code> <code>tta_merge_mode</code> <code>str</code> <p>The TTA merge mode.</p> <code>'max'</code> <code>decision_threshold</code> <code>Optional[float]</code> <p>An optional decision threshold for prediction. torch.argmax will be used by default.</p> <code>None</code> <code>sahi_tile_size</code> <code>int</code> <p>The size of the tiles to use when performing SAHI.</p> <code>128</code> <code>sahi_overlap</code> <code>int</code> <p>The size of the overlap between tiles to use when performing SAHI</p> <code>64</code> Source code in <code>kelp/nn/inference/predict.py</code> <pre><code>def run_sahi_prediction(\n    data_dir: Path,\n    output_dir: Path,\n    model_checkpoint: Path,\n    use_mlflow: bool,\n    train_cfg: TrainConfig,\n    tta: bool = False,\n    soft_labels: bool = False,\n    tta_merge_mode: str = \"max\",\n    decision_threshold: Optional[float] = None,\n    sahi_tile_size: int = 128,\n    sahi_overlap: int = 64,\n) -&gt; None:\n    \"\"\"\n    Runs SAHI (Sliced Aided Hyper Inference) using specified model checkpoint.\n\n    Args:\n        data_dir: The path to the data directory.\n        output_dir: The path to the output directory.\n        model_checkpoint: The model checkpoint.\n        use_mlflow: A flag indicating whether to use MLflow to load the model.\n        train_cfg: The original training config used to train the model.\n        tta: A flag indicating whether to use TTA for prediction.\n        soft_labels: A flag indicating whether to use soft labels for prediction.\n        tta_merge_mode: The TTA merge mode.\n        decision_threshold: An optional decision threshold for prediction. torch.argmax will be used by default.\n        sahi_tile_size: The size of the tiles to use when performing SAHI.\n        sahi_overlap: The size of the overlap between tiles to use when performing SAHI\n\n    \"\"\"\n    model = load_model(\n        model_path=model_checkpoint,\n        use_mlflow=use_mlflow,\n        tta=tta,\n        soft_labels=soft_labels,\n        tta_merge_mode=tta_merge_mode,\n        decision_threshold=decision_threshold,\n    )\n    band_order = [consts.data.ORIGINAL_BANDS.index(band) + 1 for band in train_cfg.bands]\n    bands_to_use = train_cfg.bands + train_cfg.spectral_indices\n    band_index_lookup = {band: idx for idx, band in enumerate(bands_to_use)}\n    band_stats, in_channels = resolve_normalization_stats(\n        dataset_stats=train_cfg.dataset_stats,\n        bands_to_use=bands_to_use,\n    )\n    normalization_tf = resolve_normalization_transform(\n        band_stats=band_stats,\n        normalization_strategy=train_cfg.normalization_strategy,\n    )\n    predict_sahi(\n        file_paths=sorted(list(data_dir.glob(\"*.tif\"))),\n        model=model,\n        tta=tta,\n        soft_labels=soft_labels,\n        tta_merge_mode=tta_merge_mode,\n        decision_threshold=decision_threshold,\n        output_dir=output_dir,\n        overlap=sahi_overlap,\n        tile_size=(sahi_tile_size, sahi_tile_size),\n        band_order=band_order,\n        fill_value=train_cfg.fill_value,\n        resize_tf=resolve_resize_transform(\n            resize_strategy=train_cfg.resize_strategy,\n            interpolation=train_cfg.interpolation,\n            image_size=train_cfg.image_size,\n            image_or_mask=\"image\",\n        ),\n        input_transforms=resolve_transforms(\n            normalization_transform=normalization_tf,\n            spectral_indices=train_cfg.spectral_indices,\n            band_stats=band_stats,\n            band_index_lookup=band_index_lookup,\n            mask_using_qa=train_cfg.mask_using_qa,\n            mask_using_water_mask=train_cfg.mask_using_water_mask,\n            stage=\"predict\",\n        ),\n        post_predict_transforms=T.Resize(\n            size=(sahi_tile_size, sahi_tile_size),\n            interpolation=InterpolationMode.NEAREST,\n            antialias=False,\n        ),\n    )\n</code></pre>"},{"location":"api_ref/nn/inference/predict_and_submit/","title":"predict_and_submit","text":"<p>Logic for predicting and creating submission file for a single model.</p>"},{"location":"api_ref/nn/inference/predict_and_submit/#kelp.nn.inference.predict_and_submit.PredictAndSubmitConfig","title":"<code>kelp.nn.inference.predict_and_submit.PredictAndSubmitConfig</code>","text":"<p>             Bases: <code>PredictConfig</code></p> <p>Config for running prediction and submission file generation in a single pass.</p> Source code in <code>kelp/nn/inference/predict_and_submit.py</code> <pre><code>class PredictAndSubmitConfig(PredictConfig):\n    \"\"\"Config for running prediction and submission file generation in a single pass.\"\"\"\n\n    preview_submission: bool = False\n    preview_first_n: int = 10\n</code></pre>"},{"location":"api_ref/nn/inference/predict_and_submit/#kelp.nn.inference.predict_and_submit.copy_run_artifacts","title":"<code>kelp.nn.inference.predict_and_submit.copy_run_artifacts</code>","text":"<p>Copies run artifacts from run_dir to output_dir.</p> <p>Parameters:</p> Name Type Description Default <code>run_dir</code> <code>Path</code> <p>The directory to copy run artifacts from.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required Source code in <code>kelp/nn/inference/predict_and_submit.py</code> <pre><code>def copy_run_artifacts(run_dir: Path, output_dir: Path) -&gt; None:\n    \"\"\"\n    Copies run artifacts from run_dir to output_dir.\n\n    Args:\n        run_dir: The directory to copy run artifacts from.\n        output_dir: The output directory.\n\n    \"\"\"\n    shutil.copytree(run_dir, output_dir / run_dir.name, dirs_exist_ok=True)\n</code></pre>"},{"location":"api_ref/nn/inference/predict_and_submit/#kelp.nn.inference.predict_and_submit.main","title":"<code>kelp.nn.inference.predict_and_submit.main</code>","text":"<p>The main entry point for running prediction and submission file generation.</p> Source code in <code>kelp/nn/inference/predict_and_submit.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"The main entry point for running prediction and submission file generation.\"\"\"\n    cfg = parse_args()\n    now = datetime.utcnow().isoformat()\n    out_dir = cfg.output_dir / now\n    preds_dir = cfg.output_dir / now / \"predictions\"\n    preds_dir.mkdir(exist_ok=False, parents=True)\n    (out_dir / \"predict_config.yaml\").write_text(yaml.dump(cfg.model_dump(mode=\"json\")))\n    if cfg.training_config.sahi:\n        run_sahi_prediction(\n            data_dir=cfg.data_dir,\n            output_dir=preds_dir,\n            model_checkpoint=cfg.model_checkpoint,\n            use_mlflow=cfg.use_mlflow,\n            train_cfg=cfg.training_config,\n            tta=cfg.tta,\n            soft_labels=cfg.soft_labels,\n            tta_merge_mode=cfg.tta_merge_mode,\n            decision_threshold=cfg.decision_threshold,\n            sahi_tile_size=cfg.sahi_tile_size,\n            sahi_overlap=cfg.sahi_overlap,\n        )\n    else:\n        run_prediction(\n            data_dir=cfg.data_dir,\n            output_dir=preds_dir,\n            model_checkpoint=cfg.model_checkpoint,\n            use_mlflow=cfg.use_mlflow,\n            train_cfg=cfg.training_config,\n            tta=cfg.tta,\n            tta_merge_mode=cfg.tta_merge_mode,\n            decision_threshold=cfg.decision_threshold,\n        )\n    create_submission_tar(\n        preds_dir=preds_dir,\n        output_dir=out_dir,\n    )\n    copy_run_artifacts(\n        run_dir=cfg.run_dir,  # type: ignore[arg-type]\n        output_dir=out_dir,\n    )\n    if cfg.preview_submission:\n        plot_first_n_samples(\n            data_dir=cfg.data_dir,\n            submission_dir=out_dir,\n            output_dir=out_dir / \"previews\",\n            n=cfg.preview_first_n,\n        )\n</code></pre>"},{"location":"api_ref/nn/inference/predict_and_submit/#kelp.nn.inference.predict_and_submit.parse_args","title":"<code>kelp.nn.inference.predict_and_submit.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of PredictAndSubmitConfig.</p> Source code in <code>kelp/nn/inference/predict_and_submit.py</code> <pre><code>def parse_args() -&gt; PredictAndSubmitConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of PredictAndSubmitConfig.\n\n    \"\"\"\n    parser = build_prediction_arg_parser()\n    parser.add_argument(\"--preview_submission\", action=\"store_true\")\n    parser.add_argument(\"--preview_first_n\", type=int, default=10)\n    args = parser.parse_args()\n    cfg = PredictAndSubmitConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/nn/inference/preview_submission/","title":"preview_submission","text":"<p>Logic to preview submission predictions.</p>"},{"location":"api_ref/nn/inference/preview_submission/#kelp.nn.inference.preview_submission.PreviewSubmissionConfig","title":"<code>kelp.nn.inference.preview_submission.PreviewSubmissionConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>Config for previewing the submission files.</p> Source code in <code>kelp/nn/inference/preview_submission.py</code> <pre><code>class PreviewSubmissionConfig(ConfigBase):\n    \"\"\"Config for previewing the submission files.\"\"\"\n\n    test_data_dir: Path\n    output_dir: Path\n    submission_dir: Path\n    first_n: int = 10\n\n    @field_validator(\"submission_dir\", mode=\"before\")\n    def validate_submission_dir(cls, value: Union[str, Path]) -&gt; Union[str, Path]:\n        if value == \"latest\":\n            submission_dir = sorted([d for d in Path(\"data/submissions\").iterdir() if d.is_dir()])[-1]\n            return submission_dir\n        return value\n</code></pre>"},{"location":"api_ref/nn/inference/preview_submission/#kelp.nn.inference.preview_submission.main","title":"<code>kelp.nn.inference.preview_submission.main</code>","text":"<p>Main entrypoint for plotting sample predictions from submission directory.</p> Source code in <code>kelp/nn/inference/preview_submission.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for plotting sample predictions from submission directory.\"\"\"\n    cfg = parse_args()\n    plot_first_n_samples(\n        data_dir=cfg.test_data_dir,\n        submission_dir=cfg.submission_dir,\n        output_dir=cfg.output_dir / cfg.submission_dir.name,\n        n=cfg.first_n,\n    )\n</code></pre>"},{"location":"api_ref/nn/inference/preview_submission/#kelp.nn.inference.preview_submission.parse_args","title":"<code>kelp.nn.inference.preview_submission.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of PreviewSubmissionConfig.</p> Source code in <code>kelp/nn/inference/preview_submission.py</code> <pre><code>def parse_args() -&gt; PreviewSubmissionConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of PreviewSubmissionConfig.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--test_data_dir\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    parser.add_argument(\"--submission_dir\", type=str, required=True)\n    parser.add_argument(\"--first_n\", type=int, default=10)\n    args = parser.parse_args()\n    cfg = PreviewSubmissionConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/nn/inference/preview_submission/#kelp.nn.inference.preview_submission.plot_first_n_samples","title":"<code>kelp.nn.inference.preview_submission.plot_first_n_samples</code>","text":"<p>Plots first N samples from the submission directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the data directory with files used to generate predictions in the submission dir.</p> required <code>submission_dir</code> <code>Path</code> <p>The path to the submission directory.</p> required <code>output_dir</code> <code>Path</code> <p>The path to the output directory.</p> required <code>n</code> <code>int</code> <p>The number of samples to plot.</p> <code>10</code> Source code in <code>kelp/nn/inference/preview_submission.py</code> <pre><code>def plot_first_n_samples(data_dir: Path, submission_dir: Path, output_dir: Path, n: int = 10) -&gt; None:\n    \"\"\"\n    Plots first N samples from the submission directory.\n\n    Args:\n        data_dir: The path to the data directory with files used to generate predictions in the submission dir.\n        submission_dir: The path to the submission directory.\n        output_dir: The path to the output directory.\n        n: The number of samples to plot.\n\n    \"\"\"\n    output_dir.mkdir(exist_ok=True, parents=True)\n    for fp in tqdm(sorted(list(data_dir.glob(\"*.tif\")))[:n], \"Plotting predictions\"):\n        tile = fp.stem.split(\"_\")[0]\n        with rasterio.open(data_dir / f\"{tile}_satellite.tif\") as src:\n            input = src.read()\n        with rasterio.open(submission_dir / \"predictions\" / f\"{tile}_kelp.tif\") as src:\n            prediction = src.read(1)\n        fig = plot_sample(\n            input_arr=input,\n            predictions_arr=prediction,\n            suptitle=tile,\n        )\n        fig.savefig(output_dir / f\"{fp.stem}.png\")\n        plt.close(fig)\n</code></pre>"},{"location":"api_ref/nn/inference/sahi/","title":"sahi","text":"<p>SAHI inference logic.</p>"},{"location":"api_ref/nn/inference/sahi/#kelp.nn.inference.sahi.inference_model","title":"<code>kelp.nn.inference.sahi.inference_model</code>","text":"<p>Runs inference on a batch of image tiles.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The batch of image tiles.</p> required <code>model</code> <code>Module</code> <p>The model to use for inference.</p> required <code>soft_labels</code> <code>bool</code> <p>A flag indicating whether to use soft-labels.</p> <code>False</code> <code>tta</code> <code>bool</code> <p>A flag indicating whether to use TTA.</p> <code>False</code> <code>tta_merge_mode</code> <code>str</code> <p>The TTA merge mode.</p> <code>'mean'</code> <code>decision_threshold</code> <code>Optional[float]</code> <p>An optional decision threshold to use. Will use torch.argmax by default.</p> <code>None</code> Source code in <code>kelp/nn/inference/sahi.py</code> <pre><code>@torch.inference_mode()\ndef inference_model(\n    x: Tensor,\n    model: nn.Module,\n    soft_labels: bool = False,\n    tta: bool = False,\n    tta_merge_mode: str = \"mean\",\n    decision_threshold: Optional[float] = None,\n) -&gt; Tensor:\n    \"\"\"\n    Runs inference on a batch of image tiles.\n\n    Args:\n        x: The batch of image tiles.\n        model: The model to use for inference.\n        soft_labels: A flag indicating whether to use soft-labels.\n        tta: A flag indicating whether to use TTA.\n        tta_merge_mode: The TTA merge mode.\n        decision_threshold: An optional decision threshold to use. Will use torch.argmax by default.\n\n    Returns: A tensor with predictions.\n\n    \"\"\"\n    x = x.to(model.device)\n    with torch.no_grad():\n        if tta:\n            tta_model = ttach.SegmentationTTAWrapper(\n                model=model,\n                transforms=_test_time_transforms,\n                merge_mode=tta_merge_mode,\n            )\n            y_hat = tta_model(x)\n        else:\n            y_hat = model(x)\n        if soft_labels:\n            y_hat = y_hat.sigmoid()[:, 1, :, :].float()\n        elif decision_threshold is not None:\n            y_hat = (y_hat.sigmoid()[:, 1, :, :] &gt;= decision_threshold).long()  # type: ignore[attr-defined]\n        else:\n            y_hat = y_hat.argmax(dim=1)\n    return y_hat\n</code></pre>"},{"location":"api_ref/nn/inference/sahi/#kelp.nn.inference.sahi.load_image","title":"<code>kelp.nn.inference.sahi.load_image</code>","text":"<p>Helper function to load a satellite image and fill out missing pixels.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Path</code> <p>The path to the image.</p> required <code>band_order</code> <code>List[int]</code> <p>The band order to load.</p> required <code>fill_value</code> <code>nan</code> <p>The fill value for missing pixels.</p> required Source code in <code>kelp/nn/inference/sahi.py</code> <pre><code>def load_image(\n    image_path: Path,\n    band_order: List[int],\n    fill_value: torch.nan,\n) -&gt; np.ndarray:  # type: ignore[type-arg]\n    \"\"\"\n    Helper function to load a satellite image and fill out missing pixels.\n\n    Args:\n        image_path: The path to the image.\n        band_order: The band order to load.\n        fill_value: The fill value for missing pixels.\n\n    Returns: An array with the image.\n\n    \"\"\"\n    with rasterio.open(image_path) as src:\n        img = src.read(band_order).astype(np.float32)\n        img = np.where(img == -32768.0, fill_value, img)\n    return img  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/nn/inference/sahi/#kelp.nn.inference.sahi.merge_predictions","title":"<code>kelp.nn.inference.sahi.merge_predictions</code>","text":"<p>Merges the prediction tiles into a single image by averaging the predictions in the overlapping sections.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>List[ndarray]</code> <p>A list of tiles to merge back into one image.</p> required <code>original_shape</code> <code>Tuple[int, int, int]</code> <p>The shape of the original image.</p> required <code>tile_size</code> <code>Tuple[int, int]</code> <p>The tile size used to generate crops.</p> required <code>overlap</code> <code>int</code> <p>The overlap between the tiles.</p> required <code>decision_threshold</code> <code>Optional[float]</code> <p>An optional decision threshold.</p> <code>None</code> Source code in <code>kelp/nn/inference/sahi.py</code> <pre><code>def merge_predictions(\n    tiles: List[np.ndarray],  # type: ignore[type-arg]\n    original_shape: Tuple[int, int, int],\n    tile_size: Tuple[int, int],\n    overlap: int,\n    decision_threshold: Optional[float] = None,\n) -&gt; np.ndarray:  # type: ignore[type-arg]\n    \"\"\"\n    Merges the prediction tiles into a single image by averaging the predictions in the overlapping sections.\n\n    Args:\n        tiles: A list of tiles to merge back into one image.\n        original_shape: The shape of the original image.\n        tile_size: The tile size used to generate crops.\n        overlap: The overlap between the tiles.\n        decision_threshold: An optional decision threshold.\n\n    Returns: A numpy array representing merged tiles.\n\n    \"\"\"\n    step = tile_size[0] - overlap\n    prediction = np.zeros(original_shape, dtype=np.float32)\n    counts = np.zeros(original_shape, dtype=np.float32)\n\n    idx = 0\n    for y in range(0, original_shape[0], step):\n        for x in range(0, original_shape[1], step):\n            h, w = prediction[y : y + tile_size[1], x : x + tile_size[0]].shape\n            prediction[y : y + tile_size[1], x : x + tile_size[0]] += tiles[idx][:h, :w].astype(np.float32)\n            counts[y : y + tile_size[1], x : x + tile_size[0]] += 1\n            idx += 1\n\n    # Avoid division by zero\n    counts[counts == 0] = 1\n    prediction /= counts\n\n    if decision_threshold is not None:\n        prediction = np.where(prediction &gt; decision_threshold, 1, 0)\n\n    return prediction.astype(np.int64)\n</code></pre>"},{"location":"api_ref/nn/inference/sahi/#kelp.nn.inference.sahi.predict_sahi","title":"<code>kelp.nn.inference.sahi.predict_sahi</code>","text":"<p>Runs SAHI on specified image list.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to use for prediction.</p> required <code>file_paths</code> <code>List[Path]</code> <p>The input image paths.</p> required <code>output_dir</code> <code>Path</code> <p>The path to the output directory.</p> required <code>tile_size</code> <code>Tuple[int, int]</code> <p>The tile size to use for SAHI.</p> required <code>overlap</code> <code>int</code> <p>The overlap between tiles.</p> required <code>band_order</code> <code>List[int]</code> <p>The band order.</p> required <code>resize_tf</code> <code>Callable[[Tensor], Tensor]</code> <p>The resize transform to use for resizing the tiles.</p> required <code>input_transforms</code> <code>Callable[[Tensor], Tensor]</code> <p>The input transform to use for input image before passing it to the model.</p> required <code>post_predict_transforms</code> <code>Callable[[Tensor], Tensor]</code> <p>The post-predict transform to use for predictions.</p> required <code>fill_value</code> <code>float</code> <p>The fill value for missing pixels.</p> <code>0.0</code> <code>soft_labels</code> <code>bool</code> <p>A flag indicating whether to use soft-labels.</p> <code>False</code> <code>tta</code> <code>bool</code> <p>A flag indicating whether to use TTA.</p> <code>False</code> <code>tta_merge_mode</code> <code>str</code> <p>The TTA merge mode.</p> <code>'mean'</code> <code>decision_threshold</code> <code>Optional[float]</code> <p>An optional decision threshold.</p> <code>None</code> Source code in <code>kelp/nn/inference/sahi.py</code> <pre><code>def predict_sahi(\n    model: nn.Module,\n    file_paths: List[Path],\n    output_dir: Path,\n    tile_size: Tuple[int, int],\n    overlap: int,\n    band_order: List[int],\n    resize_tf: Callable[[Tensor], Tensor],\n    input_transforms: Callable[[Tensor], Tensor],\n    post_predict_transforms: Callable[[Tensor], Tensor],\n    soft_labels: bool = False,\n    fill_value: float = 0.0,\n    tta: bool = False,\n    tta_merge_mode: str = \"mean\",\n    decision_threshold: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Runs SAHI on specified image list.\n\n    Args:\n        model: The model to use for prediction.\n        file_paths: The input image paths.\n        output_dir: The path to the output directory.\n        tile_size: The tile size to use for SAHI.\n        overlap: The overlap between tiles.\n        band_order: The band order.\n        resize_tf: The resize transform to use for resizing the tiles.\n        input_transforms: The input transform to use for input image before passing it to the model.\n        post_predict_transforms: The post-predict transform to use for predictions.\n        fill_value: The fill value for missing pixels.\n        soft_labels: A flag indicating whether to use soft-labels.\n        tta: A flag indicating whether to use TTA.\n        tta_merge_mode: The TTA merge mode.\n        decision_threshold: An optional decision threshold.\n\n    \"\"\"\n    model.eval()\n    for file_path in tqdm(file_paths, desc=\"Processing files\"):\n        tile_id = file_path.name.split(\"_\")[0]\n        pred = process_image(\n            image_path=file_path,\n            model=model,\n            tile_size=tile_size,\n            overlap=overlap,\n            input_transforms=input_transforms,\n            post_predict_transforms=post_predict_transforms,\n            soft_labels=soft_labels,\n            tta=tta,\n            tta_merge_mode=tta_merge_mode,\n            decision_threshold=decision_threshold,\n            band_order=band_order,\n            resize_tf=resize_tf,\n            fill_value=fill_value,\n        )\n        if soft_labels and decision_threshold is None:\n            META[\"dtype\"] = \"float32\"\n        dest: DatasetWriter\n        with rasterio.open(output_dir / f\"{tile_id}_kelp.tif\", \"w\", **META) as dest:\n            dest.write(pred, 1)\n</code></pre>"},{"location":"api_ref/nn/inference/sahi/#kelp.nn.inference.sahi.process_image","title":"<code>kelp.nn.inference.sahi.process_image</code>","text":"<p>Runs SAHI on a single image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Path</code> <p>The path to the image.</p> required <code>model</code> <code>Module</code> <p>The model to use for prediction.</p> required <code>tile_size</code> <code>Tuple[int, int]</code> <p>The tile size to use for SAHI.</p> required <code>overlap</code> <code>int</code> <p>The overlap between tiles.</p> required <code>band_order</code> <code>List[int]</code> <p>The band order.</p> required <code>resize_tf</code> <code>Callable[[Tensor], Tensor]</code> <p>The resize transform to use for resizing the tiles.</p> required <code>input_transforms</code> <code>Callable[[Tensor], Tensor]</code> <p>The input transform to use for input image before passing it to the model.</p> required <code>post_predict_transforms</code> <code>Callable[[Tensor], Tensor]</code> <p>The post-predict transform to use for predictions.</p> required <code>fill_value</code> <code>float</code> <p>The fill value for missing pixels.</p> <code>0.0</code> <code>soft_labels</code> <code>bool</code> <p>A flag indicating whether to use soft-labels.</p> <code>False</code> <code>tta</code> <code>bool</code> <p>A flag indicating whether to use TTA.</p> <code>False</code> <code>tta_merge_mode</code> <code>str</code> <p>The TTA merge mode.</p> <code>'mean'</code> <code>decision_threshold</code> <code>Optional[float]</code> <p>An optional decision threshold.</p> <code>None</code> Source code in <code>kelp/nn/inference/sahi.py</code> <pre><code>def process_image(\n    image_path: Path,\n    model: nn.Module,\n    tile_size: Tuple[int, int],\n    overlap: int,\n    band_order: List[int],\n    resize_tf: Callable[[Tensor], Tensor],\n    input_transforms: Callable[[Tensor], Tensor],\n    post_predict_transforms: Callable[[Tensor], Tensor],\n    fill_value: float = 0.0,\n    soft_labels: bool = False,\n    tta: bool = False,\n    tta_merge_mode: str = \"mean\",\n    decision_threshold: Optional[float] = None,\n) -&gt; np.ndarray:  # type: ignore[type-arg]\n    \"\"\"\n    Runs SAHI on a single image.\n\n    Args:\n        image_path: The path to the image.\n        model: The model to use for prediction.\n        tile_size: The tile size to use for SAHI.\n        overlap: The overlap between tiles.\n        band_order: The band order.\n        resize_tf: The resize transform to use for resizing the tiles.\n        input_transforms: The input transform to use for input image before passing it to the model.\n        post_predict_transforms: The post-predict transform to use for predictions.\n        fill_value: The fill value for missing pixels.\n        soft_labels: A flag indicating whether to use soft-labels.\n        tta: A flag indicating whether to use TTA.\n        tta_merge_mode: The TTA merge mode.\n        decision_threshold: An optional decision threshold.\n\n    Returns: An array with post-processed and merged tiles as final prediction.\n\n    \"\"\"\n    image = load_image(\n        image_path=image_path,\n        fill_value=fill_value,\n        band_order=band_order,\n    )\n    tiles = slice_image(image, tile_size, overlap)\n    predictions = []\n    img_batch = []\n    for tile in tiles:\n        x = resize_tf(torch.from_numpy(tile)).unsqueeze(0)\n        img_batch.append(x)\n\n    x = torch.cat(img_batch, dim=0).to(DEVICE)\n    x = input_transforms(x)\n    y_hat = inference_model(\n        x=x,\n        model=model,\n        soft_labels=soft_labels,\n        tta=tta,\n        tta_merge_mode=tta_merge_mode,\n        decision_threshold=decision_threshold,\n    )\n    prediction = post_predict_transforms(y_hat).detach().cpu().numpy()\n    predictions.extend([tensor for tensor in prediction])\n\n    merged_prediction = merge_predictions(\n        tiles=predictions,\n        original_shape=image.shape[1:],  # type: ignore[arg-type]\n        tile_size=tile_size,\n        overlap=overlap,\n        decision_threshold=decision_threshold,\n    )\n    return merged_prediction\n</code></pre>"},{"location":"api_ref/nn/inference/sahi/#kelp.nn.inference.sahi.slice_image","title":"<code>kelp.nn.inference.sahi.slice_image</code>","text":"<p>Helper function to slice an image into smaller tiles with a given overlap.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The image to slice.</p> required <code>tile_size</code> <code>Tuple[int, int]</code> <p>The size of the tile.</p> required <code>overlap</code> <code>int</code> <p>The overlap between tiles.</p> required Source code in <code>kelp/nn/inference/sahi.py</code> <pre><code>def slice_image(\n    image: np.ndarray,  # type: ignore[type-arg]\n    tile_size: Tuple[int, int],\n    overlap: int,\n) -&gt; List[np.ndarray]:  # type: ignore[type-arg]\n    \"\"\"\n    Helper function to slice an image into smaller tiles with a given overlap.\n\n    Args:\n        image: The image to slice.\n        tile_size: The size of the tile.\n        overlap: The overlap between tiles.\n\n    Returns: A list of sliced images.\n\n    \"\"\"\n    tiles = []\n    height, width = image.shape[1], image.shape[2]\n    step = tile_size[0] - overlap\n\n    for y in range(0, height, step):\n        for x in range(0, width, step):\n            tile = image[:, y : y + tile_size[1], x : x + tile_size[0]]\n            # Padding the tile if it's smaller than the expected size (at edges)\n            if tile.shape[1] &lt; tile_size[1] or tile.shape[2] &lt; tile_size[0]:\n                tile = np.pad(\n                    tile,\n                    ((0, 0), (0, max(0, tile_size[1] - tile.shape[1])), (0, max(0, tile_size[0] - tile.shape[2]))),\n                    mode=\"constant\",\n                    constant_values=0,\n                )\n            tiles.append(tile)\n    return tiles\n</code></pre>"},{"location":"api_ref/nn/models/efficientunetplusplus/","title":"efficientunet++","text":"<p>The EfficientUnet++.</p>"},{"location":"api_ref/nn/models/efficientunetplusplus/#decoder","title":"Decoder","text":"<p>Code credit: https://github.com/jlcsilva/segmentation_models.pytorch</p>"},{"location":"api_ref/nn/models/efficientunetplusplus/#kelp.nn.models.efficientunetplusplus.decoder.EfficientUnetPlusPlusDecoder","title":"<code>kelp.nn.models.efficientunetplusplus.decoder.EfficientUnetPlusPlusDecoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>EfficientUnet++ Decoder.</p> Source code in <code>kelp/nn/models/efficientunetplusplus/decoder.py</code> <pre><code>class EfficientUnetPlusPlusDecoder(nn.Module):\n    \"\"\"\n    EfficientUnet++ Decoder.\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_channels: List[int],\n        decoder_channels: List[int],\n        n_blocks: int = 5,\n        squeeze_ratio: int = 1,\n        expansion_ratio: int = 1,\n    ) -&gt; None:\n        super().__init__()\n        if n_blocks != len(decoder_channels):\n            raise ValueError(\n                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                    n_blocks, len(decoder_channels)\n                )\n            )\n\n        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n        # computing blocks input and output channels\n        head_channels = encoder_channels[0]\n        self.in_channels = [head_channels] + list(decoder_channels[:-1])\n        self.skip_channels = list(encoder_channels[1:]) + [0]\n        self.out_channels = decoder_channels\n\n        # combine decoder keyword arguments\n        kwargs = dict(squeeze_ratio=squeeze_ratio, expansion_ratio=expansion_ratio)\n\n        blocks = {}\n        for layer_idx in range(len(self.in_channels) - 1):\n            for depth_idx in range(layer_idx + 1):\n                if depth_idx == 0:\n                    in_ch = self.in_channels[layer_idx]\n                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1)\n                    out_ch = self.out_channels[layer_idx]\n                else:\n                    out_ch = self.skip_channels[layer_idx]\n                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1 - depth_idx)\n                    in_ch = self.skip_channels[layer_idx - 1]\n                blocks[f\"x_{depth_idx}_{layer_idx}\"] = DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n        blocks[f\"x_{0}_{len(self.in_channels) - 1}\"] = DecoderBlock(\n            self.in_channels[-1], 0, self.out_channels[-1], **kwargs\n        )\n        self.blocks = nn.ModuleDict(blocks)\n        self.depth = len(self.in_channels) - 1\n\n    def forward(self, *features: Any) -&gt; Tensor:\n        features = features[1:]  # remove first skip with same spatial resolution\n        features = features[::-1]  # reverse channels to start from head of encoder\n        # start building dense connections\n        dense_x = {}\n        for layer_idx in range(len(self.in_channels) - 1):\n            for depth_idx in range(self.depth - layer_idx):\n                if layer_idx == 0:\n                    output = self.blocks[f\"x_{depth_idx}_{depth_idx}\"](features[depth_idx], features[depth_idx + 1])\n                    dense_x[f\"x_{depth_idx}_{depth_idx}\"] = output\n                else:\n                    dense_l_i = depth_idx + layer_idx\n                    cat_features = [dense_x[f\"x_{idx}_{dense_l_i}\"] for idx in range(depth_idx + 1, dense_l_i + 1)]\n                    cat_features = torch.cat(cat_features + [features[dense_l_i + 1]], dim=1)\n                    dense_x[f\"x_{depth_idx}_{dense_l_i}\"] = self.blocks[f\"x_{depth_idx}_{dense_l_i}\"](\n                        dense_x[f\"x_{depth_idx}_{dense_l_i - 1}\"], cat_features\n                    )\n        dense_x[f\"x_{0}_{self.depth}\"] = self.blocks[f\"x_{0}_{self.depth}\"](dense_x[f\"x_{0}_{self.depth - 1}\"])\n        return dense_x[f\"x_{0}_{self.depth}\"]\n</code></pre>"},{"location":"api_ref/nn/models/efficientunetplusplus/#kelp.nn.models.efficientunetplusplus.decoder.InvertedResidual","title":"<code>kelp.nn.models.efficientunetplusplus.decoder.InvertedResidual</code>","text":"<p>             Bases: <code>Module</code></p> <p>Inverted bottleneck residual block with an scSE block embedded into the residual layer, after the depth-wise convolution. By default, uses batch normalization and Hardswish activation.</p> Source code in <code>kelp/nn/models/efficientunetplusplus/decoder.py</code> <pre><code>class InvertedResidual(nn.Module):\n    \"\"\"\n    Inverted bottleneck residual block with an scSE block embedded into the residual layer, after the\n    depth-wise convolution. By default, uses batch normalization and Hardswish activation.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int = 3,\n        stride: int = 1,\n        expansion_ratio: int = 1,\n        squeeze_ratio: int = 1,\n        activation: Optional[nn.Module] = None,\n        normalization: Optional[Type[nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if activation is None:\n            activation = nn.Hardswish(True)\n        if normalization is None:\n            normalization = nn.BatchNorm2d\n\n        self.same_shape = in_channels == out_channels\n        self.mid_channels = expansion_ratio * in_channels\n        self.block = nn.Sequential(\n            PointWiseConv2d(in_channels, self.mid_channels),\n            normalization(self.mid_channels),\n            activation,\n            DepthWiseConv2d(self.mid_channels, kernel_size=kernel_size, stride=stride),\n            normalization(self.mid_channels),\n            activation,\n            # md.sSEModule(self.mid_channels),\n            SCSEModule(self.mid_channels, reduction=squeeze_ratio),\n            # md.SEModule(self.mid_channels, reduction = squeeze_ratio),\n            PointWiseConv2d(self.mid_channels, out_channels),\n            normalization(out_channels),\n        )\n\n        if not self.same_shape:\n            # 1x1 convolution used to match the number of channels in the skip feature maps with that\n            # of the residual feature maps\n            self.skip_conv = nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n                normalization(out_channels),\n            )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        residual = self.block(x)\n        if not self.same_shape:\n            x = self.skip_conv(x)\n        return x + residual\n</code></pre>"},{"location":"api_ref/nn/models/efficientunetplusplus/#model","title":"Model","text":"<p>Code credit: https://github.com/jlcsilva/segmentation_models.pytorch</p>"},{"location":"api_ref/nn/models/efficientunetplusplus/#kelp.nn.models.efficientunetplusplus.model.EfficientUnetPlusPlus","title":"<code>kelp.nn.models.efficientunetplusplus.model.EfficientUnetPlusPlus</code>","text":"<p>             Bases: <code>SegmentationModel</code></p> <p>The EfficientUNet++ is a fully convolutional neural network for ordinary and medical image semantic segmentation. Consists of an encoder and a decoder, connected by skip connections. The encoder extracts features of different spatial resolutions, which are fed to the decoder through skip connections. The decoder combines its own feature maps with the ones from skip connections to produce accurate segmentations masks.  The EfficientUNet++ decoder architecture is based on the UNet++, a model composed of nested U-Net-like decoder sub-networks. To increase performance and computational efficiency, the EfficientUNet++ replaces the UNet++'s blocks with inverted residual blocks with depthwise convolutions and embedded spatial and channel attention mechanisms. Synergizes well with EfficientNet encoders. Due to their efficient visual representations (i.e., using few channels to represent extracted features), EfficientNet encoders require few computation from the decoder.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone) to extract features of different spatial resolution</p> <code>'timm-efficientnet-b0'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>decoder_channels</code> <code>Optional[List[int]]</code> <p>List of integers which specify in_channels parameter for convolutions used in decoder. Length of the list should be the same as encoder_depth</p> <code>None</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[Union[str, Callable[[Any], Any]]]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\",     callable and None. Default is None</p> <code>None</code> <code>aux_params</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"         (could be None to return logits)</p> <code>None</code> Reference <p>Silva et al. 2021</p> Source code in <code>kelp/nn/models/efficientunetplusplus/model.py</code> <pre><code>class EfficientUnetPlusPlus(SegmentationModel):\n    \"\"\"The EfficientUNet++ is a fully convolutional neural network for ordinary and medical image semantic segmentation.\n    Consists of an *encoder* and a *decoder*, connected by *skip connections*. The encoder extracts features of\n    different spatial resolutions, which are fed to the decoder through skip connections. The decoder combines its\n    own feature maps with the ones from skip connections to produce accurate segmentations masks.  The EfficientUNet++\n    decoder architecture is based on the UNet++, a model composed of nested U-Net-like decoder sub-networks. To\n    increase performance and computational efficiency, the EfficientUNet++ replaces the UNet++'s blocks with\n    inverted residual blocks with depthwise convolutions and embedded spatial and channel attention mechanisms.\n    Synergizes well with EfficientNet encoders. Due to their efficient visual representations (i.e., using few channels\n    to represent extracted features), EfficientNet encoders require few computation from the decoder.\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n            Length of the list should be the same as **encoder_depth**\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n                **callable** and **None**.\n            Default is **None**\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n                    (could be **None** to return logits)\n\n    Reference:\n        [Silva et al. 2021](https://arxiv.org/abs/2106.11447)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"timm-efficientnet-b0\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_channels: Optional[List[int]] = None,\n        squeeze_ratio: int = 1,\n        expansion_ratio: int = 1,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, Callable[[Any], Any]]] = None,\n        aux_params: Optional[Dict[str, Any]] = None,\n    ):\n        super().__init__()\n        if decoder_channels is None:\n            decoder_channels = [256, 128, 64, 32, 16]\n\n        self.classes = classes\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = EfficientUnetPlusPlusDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            squeeze_ratio=squeeze_ratio,\n            expansion_ratio=expansion_ratio,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=classes,\n            activation=activation,\n            kernel_size=3,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)\n        else:\n            self.classification_head = None\n\n        self.name = \"EfficientUNet++-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"api_ref/nn/models/factories/","title":"factories","text":"<p>The factory methods.</p>"},{"location":"api_ref/nn/models/factories/#kelp.nn.models.factories.resolve_loss","title":"<code>kelp.nn.models.factories.resolve_loss</code>","text":"<p>Resolves the loss function based on provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>str</code> <p>The loss function name.</p> required <code>objective</code> <code>str</code> <p>The objective.</p> required <code>device</code> <code>device</code> <p>The device.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes.</p> <code>NUM_CLASSES</code> <code>ce_smooth_factor</code> <code>float</code> <p>The smoothing factor for Cross Entropy Loss.</p> <code>0.0</code> <code>ce_class_weights</code> <code>Optional[List[float]]</code> <p>The class weights for Cross Entropy Loss.</p> <code>None</code> <code>ignore_index</code> <code>Optional[int]</code> <p>The index to ignore.</p> <code>None</code> Source code in <code>kelp/nn/models/factories.py</code> <pre><code>def resolve_loss(\n    loss_fn: str,\n    objective: str,\n    device: torch.device,\n    num_classes: int = consts.data.NUM_CLASSES,\n    ce_smooth_factor: float = 0.0,\n    ce_class_weights: Optional[List[float]] = None,\n    ignore_index: Optional[int] = None,\n) -&gt; nn.Module:\n    \"\"\"\n    Resolves the loss function based on provided arguments.\n\n    Args:\n        loss_fn: The loss function name.\n        objective: The objective.\n        device: The device.\n        num_classes: The number of classes.\n        ce_smooth_factor: The smoothing factor for Cross Entropy Loss.\n        ce_class_weights: The class weights for Cross Entropy Loss.\n        ignore_index: The index to ignore.\n\n    Returns: Resolved Loss Function module.\n\n    \"\"\"\n    if loss_fn not in LOSS_REGISTRY:\n        raise ValueError(f\"{loss_fn=} is not supported.\")\n\n    loss_kwargs: Dict[str, Any]\n    if loss_fn in [\"jaccard\", \"dice\"]:\n        loss_kwargs = {\n            \"mode\": \"multiclass\",\n            \"classes\": list(range(num_classes)) if objective != \"binary\" else None,\n        }\n    elif loss_fn == \"ce\":\n        loss_kwargs = {\n            \"weight\": torch.tensor(ce_class_weights, device=device),\n            \"ignore_index\": ignore_index or -100,\n        }\n    elif loss_fn == \"soft_ce\":\n        loss_kwargs = {\n            \"ignore_index\": ignore_index,\n            \"smooth_factor\": ce_smooth_factor,\n        }\n    elif loss_fn == \"xedice\":\n        loss_kwargs = {\n            \"mode\": \"multiclass\",\n            \"ce_class_weights\": torch.tensor(ce_class_weights, device=device),\n        }\n    elif loss_fn in [\n        \"focal_tversky\",\n        \"log_cosh_dice\",\n        \"hausdorff\",\n        \"combo\",\n        \"soft_dice\",\n        \"batch_soft_dice\",\n        \"sens_spec_loss\",\n    ]:\n        loss_kwargs = {}\n    elif loss_fn == \"t_loss\":\n        loss_kwargs = {\n            \"device\": device,\n        }\n    elif loss_fn == \"exp_log_loss\":\n        loss_kwargs = {\n            \"class_weights\": torch.tensor(ce_class_weights, device=device),\n        }\n    else:\n        loss_kwargs = {\n            \"mode\": \"multiclass\",\n            \"ignore_index\": ignore_index,\n        }\n\n    return LOSS_REGISTRY[loss_fn](**loss_kwargs)\n</code></pre>"},{"location":"api_ref/nn/models/factories/#kelp.nn.models.factories.resolve_lr_scheduler","title":"<code>kelp.nn.models.factories.resolve_lr_scheduler</code>","text":"<p>Resolves the learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer.</p> required <code>num_training_steps</code> <code>int</code> <p>The number of training steps.</p> required <code>steps_per_epoch</code> <code>int</code> <p>The number of training steps per epoch.</p> required <code>hyperparams</code> <code>Dict[str, Any]</code> <p>The hyperparameters.</p> required Source code in <code>kelp/nn/models/factories.py</code> <pre><code>def resolve_lr_scheduler(\n    optimizer: torch.optim.Optimizer,\n    num_training_steps: int,\n    steps_per_epoch: int,\n    hyperparams: Dict[str, Any],\n) -&gt; Optional[torch.optim.lr_scheduler.LRScheduler]:\n    \"\"\"\n    Resolves the learning rate scheduler.\n\n    Args:\n        optimizer: The optimizer.\n        num_training_steps: The number of training steps.\n        steps_per_epoch: The number of training steps per epoch.\n        hyperparams: The hyperparameters.\n\n    Returns: Resolved optimizer if requested, None otherwise.\n\n    \"\"\"\n\n    if (lr_scheduler := hyperparams[\"lr_scheduler\"]) is None:\n        return None\n    elif lr_scheduler == \"onecycle\":\n        scheduler = OneCycleLR(\n            optimizer,\n            max_lr=hyperparams[\"lr\"],\n            total_steps=num_training_steps,\n            pct_start=hyperparams[\"onecycle_pct_start\"],\n            div_factor=hyperparams[\"onecycle_div_factor\"],\n            final_div_factor=hyperparams[\"onecycle_final_div_factor\"],\n        )\n    elif lr_scheduler == \"cosine\":\n        scheduler = CosineAnnealingLR(\n            optimizer=optimizer,\n            T_max=hyperparams[\"epochs\"],\n            eta_min=hyperparams[\"cosine_eta_min\"],\n        )\n    elif lr_scheduler == \"cyclic\":\n        scheduler = CyclicLR(\n            optimizer=optimizer,\n            max_lr=hyperparams[\"lr\"],\n            base_lr=hyperparams[\"cyclic_base_lr\"],\n            step_size_up=steps_per_epoch,\n            step_size_down=steps_per_epoch,\n            mode=hyperparams[\"cyclic_mode\"],\n        )\n    elif lr_scheduler == \"cosine_with_warm_restarts\":\n        scheduler = CosineAnnealingWarmRestarts(\n            optimizer=optimizer,\n            T_0=steps_per_epoch,\n            T_mult=hyperparams[\"cosine_T_mult\"],\n            eta_min=hyperparams[\"cosine_eta_min\"],\n        )\n    elif lr_scheduler == \"reduce_lr_on_plateau\":\n        scheduler = ReduceLROnPlateau(\n            optimizer,\n            mode=\"min\",\n            factor=hyperparams[\"reduce_lr_on_plateau_factor\"],\n            patience=hyperparams[\"reduce_lr_on_plateau_patience\"],\n            threshold=hyperparams[\"reduce_lr_on_plateau_threshold\"],\n            min_lr=hyperparams[\"reduce_lr_on_plateau_min_lr\"],\n            verbose=True,\n        )\n    else:\n        raise ValueError(f\"LR Scheduler: {lr_scheduler} is not supported.\")\n    return scheduler\n</code></pre>"},{"location":"api_ref/nn/models/factories/#kelp.nn.models.factories.resolve_model","title":"<code>kelp.nn.models.factories.resolve_model</code>","text":"<p>Resolves the model based on provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>architecture</code> <code>str</code> <p>The architecture.</p> required <code>encoder</code> <code>str</code> <p>The encoder.</p> required <code>classes</code> <code>int</code> <p>The number of classes.</p> required <code>in_channels</code> <code>int</code> <p>The number of input channels.</p> required <code>encoder_weights</code> <code>Optional[str]</code> <p>Optional pre-trained encoder weights.</p> <code>None</code> <code>decoder_channels</code> <code>Optional[List[int]]</code> <p>Optional decoder channels.</p> <code>None</code> <code>decoder_attention_type</code> <code>Optional[str]</code> <p>Optional decoder attention type.</p> <code>None</code> <code>pretrained</code> <code>bool</code> <p>A flag indicating whether to use pre-trained model weights.</p> <code>False</code> <code>compile</code> <code>bool</code> <p>A flag indicating whether to compile the model using torch.compile.</p> <code>False</code> <code>compile_mode</code> <code>str</code> <p>The compile mode.</p> <code>'default'</code> <code>compile_dynamic</code> <code>Optional[bool]</code> <p>A flag indicating whether to use dynamic compile.</p> <code>None</code> <code>ort</code> <code>bool</code> <p>A flag indicating whether to use torch ORT compilation.</p> <code>False</code> Source code in <code>kelp/nn/models/factories.py</code> <pre><code>def resolve_model(\n    architecture: str,\n    encoder: str,\n    classes: int,\n    in_channels: int,\n    encoder_weights: Optional[str] = None,\n    decoder_channels: Optional[List[int]] = None,\n    decoder_attention_type: Optional[str] = None,\n    pretrained: bool = False,\n    compile: bool = False,\n    compile_mode: str = \"default\",\n    compile_dynamic: Optional[bool] = None,\n    ort: bool = False,\n) -&gt; nn.Module:\n    \"\"\"\n    Resolves the model based on provided parameters.\n\n    Args:\n        architecture: The architecture.\n        encoder: The encoder.\n        classes: The number of classes.\n        in_channels: The number of input channels.\n        encoder_weights: Optional pre-trained encoder weights.\n        decoder_channels: Optional decoder channels.\n        decoder_attention_type: Optional decoder attention type.\n        pretrained: A flag indicating whether to use pre-trained model weights.\n        compile: A flag indicating whether to compile the model using torch.compile.\n        compile_mode: The compile mode.\n        compile_dynamic: A flag indicating whether to use dynamic compile.\n        ort: A flag indicating whether to use torch ORT compilation.\n\n    Returns: Resolved model.\n\n    \"\"\"\n    if decoder_channels is None:\n        decoder_channels = [256, 128, 64, 32, 16]\n\n    if architecture in _MODEL_LOOKUP:\n        model_kwargs = {\n            \"encoder_name\": encoder,\n            \"encoder_weights\": encoder_weights if pretrained else None,\n            \"in_channels\": in_channels,\n            \"classes\": classes,\n            \"encoder_depth\": len(decoder_channels),\n            \"decoder_channels\": decoder_channels,\n            \"decoder_attention_type\": decoder_attention_type,\n        }\n        if \"unet\" not in architecture or architecture == \"efficientunet++\":\n            model_kwargs.pop(\"decoder_attention_type\")\n        if architecture == \"fcn\":\n            model_kwargs.pop(\"encoder_name\")\n            model_kwargs.pop(\"encoder_weights\")\n        if architecture not in [\"efficinentunet++\", \"manet\", \"resunet\", \"resunet++\", \"unet\", \"unet++\"]:\n            model_kwargs.pop(\"decoder_channels\")\n            model_kwargs.pop(\"encoder_depth\")\n        model = _MODEL_LOOKUP[architecture](**model_kwargs)\n    else:\n        raise ValueError(f\"{architecture=} is not supported.\")\n\n    if compile:\n        model = torch.compile(\n            model,\n            mode=compile_mode,\n            dynamic=compile_dynamic,\n        )\n\n    if ort:\n        if module_available(\"torch_ort\"):\n            from torch_ort import ORTModule  # noqa\n\n            model = ORTModule(model)\n        else:\n            raise MisconfigurationException(\n                \"Torch ORT is required to use ORT. See here for installation: https://github.com/pytorch/ort\"\n            )\n\n    return model\n</code></pre>"},{"location":"api_ref/nn/models/factories/#kelp.nn.models.factories.resolve_optimizer","title":"<code>kelp.nn.models.factories.resolve_optimizer</code>","text":"<p>Resolves the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterator[Parameter]</code> <p>The model parameters.</p> required <code>hyperparams</code> <code>Dict[str, Any]</code> <p>A dictionary of hyperparameters.</p> required Source code in <code>kelp/nn/models/factories.py</code> <pre><code>def resolve_optimizer(params: Iterator[Parameter], hyperparams: Dict[str, Any]) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Resolves the optimizer.\n\n    Args:\n        params: The model parameters.\n        hyperparams: A dictionary of hyperparameters.\n\n    Returns: Resolved optimizer.\n\n    \"\"\"\n    if (optimizer := hyperparams[\"optimizer\"]) == \"adam\":\n        optimizer = Adam(params, lr=hyperparams[\"lr\"], weight_decay=hyperparams[\"weight_decay\"])\n    elif optimizer == \"adamw\":\n        optimizer = AdamW(params, lr=hyperparams[\"lr\"], weight_decay=hyperparams[\"weight_decay\"])\n    elif optimizer == \"sgd\":\n        optimizer = SGD(params, lr=hyperparams[\"lr\"], weight_decay=hyperparams[\"weight_decay\"])\n    else:\n        raise ValueError(f\"Optimizer: {optimizer} is not supported.\")\n    return optimizer\n</code></pre>"},{"location":"api_ref/nn/models/fcn/","title":"fcn","text":"<p>The FCN.</p> <p>Simple fully convolutional neural network (FCN) implementations. Code credit: torchgeo</p>"},{"location":"api_ref/nn/models/fcn/#kelp.nn.models.fcn.model.FCN","title":"<code>kelp.nn.models.fcn.model.FCN</code>","text":"<p>             Bases: <code>Module</code></p> <p>A simple 5 layer FCN with leaky relus and 'same' padding.</p> Source code in <code>kelp/nn/models/fcn/model.py</code> <pre><code>class FCN(nn.Module):\n    \"\"\"A simple 5 layer FCN with leaky relus and 'same' padding.\"\"\"\n\n    def __init__(self, in_channels: int, classes: int, num_filters: int = 256) -&gt; None:\n        \"\"\"Initializes the 5 layer FCN model.\n\n        Args:\n            in_channels: Number of input channels that the model will expect\n            classes: Number of filters in the final layer\n            num_filters: Number of filters in each convolutional layer\n        \"\"\"\n        super().__init__()\n\n        conv1 = nn.Conv2d(in_channels, num_filters, kernel_size=3, stride=1, padding=1)\n        conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1)\n        conv3 = nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1)\n        conv4 = nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1)\n        conv5 = nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1)\n\n        self.backbone = nn.Sequential(\n            conv1,\n            nn.LeakyReLU(inplace=True),\n            conv2,\n            nn.LeakyReLU(inplace=True),\n            conv3,\n            nn.LeakyReLU(inplace=True),\n            conv4,\n            nn.LeakyReLU(inplace=True),\n            conv5,\n            nn.LeakyReLU(inplace=True),\n        )\n\n        self.last = nn.Conv2d(num_filters, classes, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass of the model.\"\"\"\n        x = self.backbone(x)\n        x = self.last(x)\n        return x\n</code></pre>"},{"location":"api_ref/nn/models/fcn/#kelp.nn.models.fcn.model.FCN.forward","title":"<code>kelp.nn.models.fcn.model.FCN.forward</code>","text":"<p>Forward pass of the model.</p> Source code in <code>kelp/nn/models/fcn/model.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass of the model.\"\"\"\n    x = self.backbone(x)\n    x = self.last(x)\n    return x\n</code></pre>"},{"location":"api_ref/nn/models/losses/","title":"losses","text":"<p>The loss functions.</p>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.BatchSoftDice","title":"<code>kelp.nn.models.losses.BatchSoftDice</code>","text":"<p>             Bases: <code>Module</code></p> <p>This is the variance of SoftDiceLoss, it in introduced in this paper</p> References <p>Segmentation of Head and Neck Organs at Risk Using CNN with Batch Dice Loss</p> Source code in <code>kelp/nn/models/losses.py</code> <pre><code>class BatchSoftDice(nn.Module):\n    \"\"\"\n    This is the variance of SoftDiceLoss, it in introduced in this [paper](https://arxiv.org/pdf/1812.02427.pdf)\n\n    References:\n        [Segmentation of Head and Neck Organs at Risk Using CNN with\n        Batch Dice Loss](https://arxiv.org/pdf/1812.02427.pdf)\n    \"\"\"\n\n    def __init__(self, use_square: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            use_square: If use square then the denominator will the sum of square\n        \"\"\"\n        super(BatchSoftDice, self).__init__()\n        self._use_square = use_square\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -&gt; Tensor:\n        \"\"\"\n        Calculates batch soft-dice loss.\n\n        Args:\n            y_pred: Tensor shape (N, N_Class, H, W), torch.float\n            y_true: Tensor shape (N, H, W)\n        Returns:\n        \"\"\"\n        num_classes = y_pred.shape[1]\n        batch_size = y_pred.shape[0]\n        axes = (-2, -1)\n        y_pred = F.softmax(y_pred, dim=1)\n        one_hot_target = F.one_hot(y_true.to(torch.int64), num_classes=num_classes).permute((0, 3, 1, 2))\n        assert y_pred.shape == one_hot_target.shape\n        numerator = 2.0 * torch.sum(y_pred * one_hot_target, dim=axes)\n        if self._use_square:\n            denominator = torch.sum(torch.square(y_pred) + torch.square(one_hot_target), dim=axes)\n        else:\n            denominator = torch.sum(y_pred + one_hot_target, dim=axes)\n        return (1 - torch.mean((numerator + consts.data.EPS) / (denominator + consts.data.EPS))) * batch_size\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.BatchSoftDice.forward","title":"<code>kelp.nn.models.losses.BatchSoftDice.forward</code>","text":"<p>Calculates batch soft-dice loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Tensor shape (N, N_Class, H, W), torch.float</p> required <code>y_true</code> <code>Tensor</code> <p>Tensor shape (N, H, W)</p> required Source code in <code>kelp/nn/models/losses.py</code> <pre><code>def forward(self, y_pred: Tensor, y_true: Tensor) -&gt; Tensor:\n    \"\"\"\n    Calculates batch soft-dice loss.\n\n    Args:\n        y_pred: Tensor shape (N, N_Class, H, W), torch.float\n        y_true: Tensor shape (N, H, W)\n    Returns:\n    \"\"\"\n    num_classes = y_pred.shape[1]\n    batch_size = y_pred.shape[0]\n    axes = (-2, -1)\n    y_pred = F.softmax(y_pred, dim=1)\n    one_hot_target = F.one_hot(y_true.to(torch.int64), num_classes=num_classes).permute((0, 3, 1, 2))\n    assert y_pred.shape == one_hot_target.shape\n    numerator = 2.0 * torch.sum(y_pred * one_hot_target, dim=axes)\n    if self._use_square:\n        denominator = torch.sum(torch.square(y_pred) + torch.square(one_hot_target), dim=axes)\n    else:\n        denominator = torch.sum(y_pred + one_hot_target, dim=axes)\n    return (1 - torch.mean((numerator + consts.data.EPS) / (denominator + consts.data.EPS))) * batch_size\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.ComboLoss","title":"<code>kelp.nn.models.losses.ComboLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>It is defined as a weighted sum of Dice loss and a modified cross entropy. It attempts to leverage the flexibility of Dice loss of class imbalance and at same time use cross-entropy for curve smoothing.</p> <p>This loss will look like \"batch bce-loss\" when we consider all pixels flattened are predicted as correct or not</p> <p>This loss is perfect loss when the training loss come to -0.5 (with the default config)</p> References <p>Paper. See the original paper at formula (3) Author's implementation in Keras</p> Source code in <code>kelp/nn/models/losses.py</code> <pre><code>class ComboLoss(nn.Module):\n    \"\"\"\n    It is defined as a weighted sum of Dice loss and a modified cross entropy. It attempts to leverage the\n    flexibility of Dice loss of class imbalance and at same time use cross-entropy for curve smoothing.\n\n    This loss will look like \"batch bce-loss\" when we consider all pixels flattened are predicted as correct or not\n\n    This loss is perfect loss when the training loss come to -0.5 (with the default config)\n\n    References:\n        [Paper](https://arxiv.org/pdf/1805.02798.pdf). See the original paper at formula (3)\n        [Author's implementation in Keras](https://github.com/asgsaeid/ComboLoss/blob/master/combo_loss.py)\n\n    \"\"\"\n\n    def __init__(self, use_softmax: bool = True, ce_w: float = 0.5, ce_d_w: float = 0.5) -&gt; None:\n        super(ComboLoss, self).__init__()\n        self.use_softmax = use_softmax\n        self.ce_w = ce_w\n        self.ce_d_w = ce_d_w\n        self.eps = 1e-12\n        self.smooth = 1\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -&gt; Tensor:\n        # Apply softmax to the output to present it in probability.\n        if self.use_softmax:\n            y_pred = F.softmax(y_pred, dim=1)\n\n        one_hot_target = F.one_hot(y_true.to(torch.int64), num_classes=2).permute((0, 3, 1, 2)).to(torch.float)\n\n        # At this time, the output and one_hot_target have the same shape\n        y_true_f = torch.flatten(one_hot_target)\n        y_pred_f = torch.flatten(y_pred)\n        intersection = torch.sum(y_true_f * y_pred_f)\n        d = (2.0 * intersection + self.smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + self.smooth)\n\n        out = -(\n            self.ce_w * y_true_f * torch.log(y_pred_f + self.eps)\n            + (1 - self.ce_w) * (1.0 - y_true_f) * torch.log(1.0 - y_pred_f + self.eps)\n        )\n        weighted_ce = torch.mean(out, dim=-1)\n\n        combo = (self.ce_d_w * weighted_ce) - ((1 - self.ce_d_w) * d)\n        return combo\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.ExponentialLogarithmicLoss","title":"<code>kelp.nn.models.losses.ExponentialLogarithmicLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>This loss is focuses on less accurately predicted structures using the combination of Dice Loss ans Cross Entropy Loss</p> References <p>Original paper</p> <p>See the paper at 2.2 w_l = ((Sum k f_k) / f_l) ** 0.5 is the label weight</p> Note <ul> <li>Input for CrossEntropyLoss is the logits - Raw output from the model</li> </ul> Source code in <code>kelp/nn/models/losses.py</code> <pre><code>class ExponentialLogarithmicLoss(nn.Module):\n    \"\"\"\n    This loss is focuses on less accurately predicted structures using the combination of Dice Loss ans Cross Entropy\n    Loss\n\n    References:\n        [Original paper](https://arxiv.org/pdf/1809.00076.pdf)\n\n        See the paper at 2.2 w_l = ((Sum k f_k) / f_l) ** 0.5 is the label weight\n\n    Note:\n        - Input for CrossEntropyLoss is the logits - Raw output from the model\n    \"\"\"\n\n    def __init__(\n        self,\n        class_weights: Tensor,\n        w_dice: float = 0.5,\n        w_cross: float = 0.5,\n        gamma: float = 0.3,\n        use_softmax: bool = True,\n    ) -&gt; None:\n        super(ExponentialLogarithmicLoss, self).__init__()\n        self.w_dice = w_dice\n        self.gamma = gamma\n        self.w_cross = w_cross\n        self.use_softmax = use_softmax\n        self.class_weights = class_weights\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -&gt; Tensor:\n        self.class_weights = self.class_weights.to(y_true.device)\n        weight_map = self.class_weights[y_true]\n\n        y_true = F.one_hot(y_true.to(torch.int64), num_classes=2).permute((0, 3, 1, 2)).to(torch.float)\n        if self.use_softmax:\n            y_pred = F.softmax(y_pred, dim=1)\n\n        l_dice = torch.mean(torch.pow(-torch.log(soft_dice_loss(y_pred, y_true)), self.gamma))  # mean w.r.t to label\n        l_cross = torch.mean(\n            torch.mul(weight_map, torch.pow(F.cross_entropy(y_pred, y_true, reduction=\"none\"), self.gamma))\n        )\n        return self.w_dice * l_dice + self.w_cross * l_cross\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.FocalTverskyLoss","title":"<code>kelp.nn.models.losses.FocalTverskyLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Focal-Tversky Loss.</p> <p>This loss is similar to Tversky Loss, but with a small adjustment With input shape (batch, n_classes, h, w) then TI has shape [batch, n_classes] In their paper TI_c is the tensor w.r.t to n_classes index</p> References <p>This paper</p> <p>FTL = Sum_index_c(1 - TI_c)^gamma</p> Source code in <code>kelp/nn/models/losses.py</code> <pre><code>class FocalTverskyLoss(nn.Module):\n    \"\"\"\n    Focal-Tversky Loss.\n\n    This loss is similar to Tversky Loss, but with a small adjustment\n    With input shape (batch, n_classes, h, w) then TI has shape [batch, n_classes]\n    In their paper TI_c is the tensor w.r.t to n_classes index\n\n    References:\n        [This paper](https://arxiv.org/pdf/1810.07842.pdf)\n\n        FTL = Sum_index_c(1 - TI_c)^gamma\n    \"\"\"\n\n    def __init__(self, gamma: float = 1.0, beta: float = 0.5, use_softmax: bool = True) -&gt; None:\n        super(FocalTverskyLoss, self).__init__()\n        self.gamma = gamma\n        self.beta = beta\n        self.use_softmax = use_softmax\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -&gt; Tensor:\n        num_classes = y_pred.shape[1]\n        if self.use_softmax:\n            y_pred = F.softmax(y_pred, dim=1)  # predicted value\n        y_true = F.one_hot(y_true.to(torch.int64), num_classes=num_classes).permute((0, 3, 1, 2)).to(torch.float)\n        assert y_pred.shape == y_true.shape\n        numerator = torch.sum(y_pred * y_true, dim=(-2, -1))\n        denominator = (\n            numerator\n            + self.beta * torch.sum((1 - y_true) * y_pred, dim=(-2, -1))\n            + (1 - self.beta) * torch.sum(y_true * (1 - y_pred), dim=(-2, -1))\n        )\n        TI = torch.mean((numerator + consts.data.EPS) / (denominator + consts.data.EPS), dim=0)\n        return torch.sum(torch.pow(1.0 - TI, self.gamma))\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.HausdorffLoss","title":"<code>kelp.nn.models.losses.HausdorffLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>The Hausdorff loss.</p> Source code in <code>kelp/nn/models/losses.py</code> <pre><code>class HausdorffLoss(nn.Module):\n    \"\"\"\n    The Hausdorff loss.\n    \"\"\"\n\n    def __init__(self, use_softmax: bool = True) -&gt; None:\n        super().__init__()\n        self.hausdorfer = HausdorffERLoss()\n        self.use_softmax = use_softmax\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n        # Apply softmax to the output to present it in probability.\n        if self.use_softmax:\n            y_pred = F.softmax(y_pred, dim=1)\n        y_true = y_true.unsqueeze(1)\n        return self.hausdorfer(y_pred, y_true)\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.LogCoshDiceLoss","title":"<code>kelp.nn.models.losses.LogCoshDiceLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>LogCoshDice Loss.</p> <p>L_{lc-dce} = log(cosh(DiceLoss)</p> Source code in <code>kelp/nn/models/losses.py</code> <pre><code>class LogCoshDiceLoss(nn.Module):\n    \"\"\"\n    LogCoshDice Loss.\n\n    L_{lc-dce} = log(cosh(DiceLoss)\n    \"\"\"\n\n    def __init__(self, use_softmax: bool = True) -&gt; None:\n        super(LogCoshDiceLoss, self).__init__()\n        self.use_softmax = use_softmax\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -&gt; Tensor:\n        # Apply softmax to the output to present it in probability.\n        if self.use_softmax:\n            y_pred = nn.Softmax(dim=1)(y_pred)\n        one_hot_target = F.one_hot(y_true.to(torch.int64), num_classes=2).permute((0, 3, 1, 2)).to(torch.float)\n        assert y_pred.shape == one_hot_target.shape\n        numerator = 2.0 * torch.sum(y_pred * one_hot_target, dim=(-2, -1))\n        denominator = torch.sum(y_pred + one_hot_target, dim=(-2, -1))\n        return torch.log(torch.cosh(1 - torch.mean((numerator + consts.data.EPS) / (denominator + consts.data.EPS))))\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.SoftDiceLoss","title":"<code>kelp.nn.models.losses.SoftDiceLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>SoftDice loss.</p> References <p>JeremyJordan's Implementation</p> <p>Paper related to this function:</p> <p>Formula for binary segmentation case - A survey of loss functions for semantic segmentation</p> <p>Formula for multiclass segmentation cases - Segmentation of Head and Neck Organs at Risk Using CNN with Batch Dice Loss</p> Source code in <code>kelp/nn/models/losses.py</code> <pre><code>class SoftDiceLoss(nn.Module):\n    \"\"\"\n    SoftDice loss.\n\n    References:\n        [JeremyJordan's\n        Implementation](https://gist.github.com/jeremyjordan/9ea3032a32909f71dd2ab35fe3bacc08#file-soft_dice_loss-py)\n\n        Paper related to this function:\n\n        [Formula for binary segmentation case -\n        A survey of loss functions for semantic segmentation](https://arxiv.org/pdf/2006.14822.pdf)\n\n        [Formula for multiclass segmentation cases - Segmentation of Head and Neck Organs at Risk Using CNN with Batch\n        Dice Loss](https://arxiv.org/pdf/1812.02427.pdf)\n    \"\"\"\n\n    def __init__(self, reduction: str = \"mean\", use_softmax: bool = True) -&gt; None:\n        \"\"\"\n        Args:\n            use_softmax: Set it to False when use the function for testing purpose\n        \"\"\"\n        super(SoftDiceLoss, self).__init__()\n        self.use_softmax = use_softmax\n        self.reduction = reduction\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -&gt; Tensor:\n        \"\"\"\n        Calculate SoftDice loss.\n\n        Args:\n            y_pred: Tensor shape (N, N_Class, H, W), torch.float\n            y_true: Tensor shape (N, H, W)\n\n        Returns:\n\n        \"\"\"\n        num_classes = y_pred.shape[1]\n        # Apply softmax to the output to present it in probability.\n        if self.use_softmax:\n            y_pred = F.softmax(y_pred, dim=1)\n        one_hot_target = (\n            F.one_hot(y_true.to(torch.int64), num_classes=num_classes).permute((0, 3, 1, 2)).to(torch.float)\n        )\n        assert y_pred.shape == one_hot_target.shape\n        if self.reduction == \"none\":\n            return 1.0 - soft_dice_loss(y_pred, one_hot_target)\n        elif self.reduction == \"mean\":\n            return 1.0 - torch.mean(soft_dice_loss(y_pred, one_hot_target))\n        else:\n            raise NotImplementedError(f\"Invalid reduction mode: {self.reduction}\")\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.SoftDiceLoss.forward","title":"<code>kelp.nn.models.losses.SoftDiceLoss.forward</code>","text":"<p>Calculate SoftDice loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Tensor shape (N, N_Class, H, W), torch.float</p> required <code>y_true</code> <code>Tensor</code> <p>Tensor shape (N, H, W)</p> required Source code in <code>kelp/nn/models/losses.py</code> <pre><code>def forward(self, y_pred: Tensor, y_true: Tensor) -&gt; Tensor:\n    \"\"\"\n    Calculate SoftDice loss.\n\n    Args:\n        y_pred: Tensor shape (N, N_Class, H, W), torch.float\n        y_true: Tensor shape (N, H, W)\n\n    Returns:\n\n    \"\"\"\n    num_classes = y_pred.shape[1]\n    # Apply softmax to the output to present it in probability.\n    if self.use_softmax:\n        y_pred = F.softmax(y_pred, dim=1)\n    one_hot_target = (\n        F.one_hot(y_true.to(torch.int64), num_classes=num_classes).permute((0, 3, 1, 2)).to(torch.float)\n    )\n    assert y_pred.shape == one_hot_target.shape\n    if self.reduction == \"none\":\n        return 1.0 - soft_dice_loss(y_pred, one_hot_target)\n    elif self.reduction == \"mean\":\n        return 1.0 - torch.mean(soft_dice_loss(y_pred, one_hot_target))\n    else:\n        raise NotImplementedError(f\"Invalid reduction mode: {self.reduction}\")\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.TLoss","title":"<code>kelp.nn.models.losses.TLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Implementation of the TLoss.</p> Source code in <code>kelp/nn/models/losses.py</code> <pre><code>class TLoss(nn.Module):\n    \"\"\"Implementation of the TLoss.\"\"\"\n\n    def __init__(\n        self,\n        device: torch.device,\n        image_size: int = 352,\n        nu: float = 1.0,\n        epsilon: float = 1e-8,\n        reduction: str = \"mean\",\n        use_softmax: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.image_size = image_size\n        self.D = torch.tensor(\n            (self.image_size * self.image_size),\n            dtype=torch.float,\n            device=device,\n        )\n\n        self.lambdas = torch.ones(\n            (self.image_size, self.image_size),\n            dtype=torch.float,\n            device=device,\n        )\n        self.nu = nn.Parameter(torch.tensor(nu, dtype=torch.float, device=device))\n        self.epsilon = torch.tensor(epsilon, dtype=torch.float, device=device)\n        self.reduction = reduction\n        self.use_softmax = use_softmax\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -&gt; torch.Tensor:\n        if self.use_softmax:\n            y_pred = nn.Softmax(dim=1)(y_pred)[:, 1, ...]\n        delta_i = y_pred - y_true\n        sum_nu_epsilon = torch.exp(self.nu) + self.epsilon\n        first_term = -torch.lgamma((sum_nu_epsilon + self.D) / 2)\n        second_term = torch.lgamma(sum_nu_epsilon / 2)\n        third_term = -0.5 * torch.sum(self.lambdas + self.epsilon)\n        fourth_term = (self.D / 2) * torch.log(torch.tensor(np.pi))\n        fifth_term = (self.D / 2) * (self.nu + self.epsilon)\n\n        delta_squared = torch.pow(delta_i, 2)\n        lambdas_exp = torch.exp(self.lambdas + self.epsilon).to(delta_squared.device)\n        numerator = delta_squared * lambdas_exp\n        numerator = torch.sum(numerator, dim=(1, 2))\n\n        fraction = numerator / sum_nu_epsilon\n        sixth_term = ((sum_nu_epsilon + self.D) / 2) * torch.log(1 + fraction)\n\n        total_losses = first_term + second_term + third_term + fourth_term + fifth_term + sixth_term\n\n        if self.reduction == \"mean\":\n            return total_losses.mean()\n        elif self.reduction == \"sum\":\n            return total_losses.sum()\n        elif self.reduction == \"none\":\n            return total_losses\n        else:\n            raise ValueError(f\"The reduction method '{self.reduction}' is not implemented.\")\n</code></pre>"},{"location":"api_ref/nn/models/losses/#kelp.nn.models.losses.XEDiceLoss","title":"<code>kelp.nn.models.losses.XEDiceLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Computes (0.5 * CrossEntropyLoss) + (0.5 * DiceLoss).</p> Source code in <code>kelp/nn/models/losses.py</code> <pre><code>class XEDiceLoss(nn.Module):\n    \"\"\"\n    Computes (0.5 * CrossEntropyLoss) + (0.5 * DiceLoss).\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str,\n        weight_ce: float = 0.5,\n        weight_dice: float = 0.5,\n        ce_class_weights: Optional[Tensor] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.xe = nn.CrossEntropyLoss(weight=ce_class_weights)\n        self.dice = smp.losses.DiceLoss(mode=mode)\n        self.weight_ce = weight_ce\n        self.weight_dice = weight_dice\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -&gt; Tensor:\n        return self.weight_ce * self.xe(y_pred, y_true) + self.weight_dice * self.dice(y_pred, y_true)\n</code></pre>"},{"location":"api_ref/nn/models/modules/","title":"modules","text":"<p>Building blocks for NN models.</p>"},{"location":"api_ref/nn/models/modules/#kelp.nn.models.modules.DepthWiseConv2d","title":"<code>kelp.nn.models.modules.DepthWiseConv2d</code>","text":"<p>             Bases: <code>Conv2d</code></p> <p>Depth-wise convolution operation</p> Source code in <code>kelp/nn/models/modules.py</code> <pre><code>class DepthWiseConv2d(nn.Conv2d):\n    \"\"\"Depth-wise convolution operation\"\"\"\n\n    def __init__(self, channels: int, kernel_size: int = 3, stride: int = 1) -&gt; None:\n        super().__init__(channels, channels, kernel_size, stride=stride, padding=kernel_size // 2, groups=channels)\n</code></pre>"},{"location":"api_ref/nn/models/modules/#kelp.nn.models.modules.PointWiseConv2d","title":"<code>kelp.nn.models.modules.PointWiseConv2d</code>","text":"<p>             Bases: <code>Conv2d</code></p> <p>Point-wise (1x1) convolution operation</p> Source code in <code>kelp/nn/models/modules.py</code> <pre><code>class PointWiseConv2d(nn.Conv2d):\n    \"\"\"Point-wise (1x1) convolution operation\"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int) -&gt; None:\n        super().__init__(in_channels, out_channels, kernel_size=1, stride=1)\n</code></pre>"},{"location":"api_ref/nn/models/modules/#kelp.nn.models.modules.PreActivatedConv2dReLU","title":"<code>kelp.nn.models.modules.PreActivatedConv2dReLU</code>","text":"<p>             Bases: <code>Sequential</code></p> <p>Pre-activated 2D convolution, as proposed in https://arxiv.org/pdf/1603.05027.pdf. Feature maps are processed by a normalization layer,  followed by a ReLU activation and a 3x3 convolution.</p> Source code in <code>kelp/nn/models/modules.py</code> <pre><code>class PreActivatedConv2dReLU(nn.Sequential):\n    \"\"\"\n    Pre-activated 2D convolution, as proposed in https://arxiv.org/pdf/1603.05027.pdf.\n    Feature maps are processed by a normalization layer,  followed by a ReLU activation and a 3x3 convolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        padding: int = 0,\n        stride: int = 1,\n        use_batchnorm: bool = True,\n    ) -&gt; None:\n        if use_batchnorm:\n            bn = nn.BatchNorm2d(out_channels)\n        else:\n            bn = nn.Identity()\n\n        relu = nn.ReLU(inplace=True)\n\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not use_batchnorm,\n        )\n        super(PreActivatedConv2dReLU, self).__init__(conv, bn, relu)\n</code></pre>"},{"location":"api_ref/nn/models/resunet/","title":"resunet","text":"<p>The ResUNet.</p>"},{"location":"api_ref/nn/models/resunet/#decoder","title":"Decoder","text":"<p>Code credit: https://github.com/jlcsilva/segmentation_models.pytorch</p>"},{"location":"api_ref/nn/models/resunet/#model","title":"Model","text":"<p>Code credit: https://github.com/jlcsilva/segmentation_models.pytorch</p>"},{"location":"api_ref/nn/models/resunet/#kelp.nn.models.resunet.model.ResUnet","title":"<code>kelp.nn.models.resunet.model.ResUnet</code>","text":"<p>             Bases: <code>SegmentationModel</code></p> <p>ResUnet is a fully-convolution neural network for image semantic segmentation. Consist of encoder and decoder parts connected with skip connections. Encoder extract features of different spatial resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use concatenation for fusing decoder blocks with skip connections. Use residual connections inside each decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a backbone)     to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>decoder_channels</code> <code>Optional[List[int]]</code> <p>List of integers which specify in_channels parameter for convolutions used in decoder. Length of the list should be the same as encoder_depth</p> <code>None</code> <code>decoder_use_batchnorm</code> <code>bool</code> <p>If True, BatchNorm2d layer between Conv2D and Activation layers is used. If \"inplace\" InplaceABN will be used, allows to decrease memory consumption. Available options are True, False, \"inplace\"</p> <code>True</code> <code>decoder_attention_type</code> <code>Optional[str]</code> <p>Attention module used in decoder of the model. Available options are None and scse (https://arxiv.org/abs/1808.08127).</p> <code>None</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[Union[str, Callable[[Any], Any]]]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\",     callable and None. Default is None</p> <code>None</code> <code>aux_params</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"         (could be None to return logits)</p> <code>None</code> Reference <p>Zhang et al. 2017</p> Source code in <code>kelp/nn/models/resunet/model.py</code> <pre><code>class ResUnet(SegmentationModel):\n    \"\"\"ResUnet is a fully-convolution neural network for image semantic segmentation. Consist of *encoder*\n    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial\n    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use *concatenation*\n    for fusing decoder blocks with skip connections. Use residual connections inside each decoder block.\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n                to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n            Length of the list should be the same as **encoder_depth**\n        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n            Available options are **True, False, \"inplace\"**\n        decoder_attention_type: Attention module used in decoder of the model. Available options are\n            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n                **callable** and **None**.\n            Default is **None**\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n                    (could be **None** to return logits)\n    Returns:\n        ``torch.nn.Module``: ResUnet\n\n    Reference:\n        [Zhang et al. 2017](https://arxiv.org/abs/1711.10684)\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_use_batchnorm: bool = True,\n        decoder_channels: Optional[List[int]] = None,\n        decoder_attention_type: Optional[str] = None,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, Callable[[Any], Any]]] = None,\n        aux_params: Optional[Dict[str, Any]] = None,\n    ):\n        super().__init__()\n\n        if decoder_channels is None:\n            decoder_channels = [256, 128, 64, 32, 16]\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = ResUnetDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            use_batchnorm=decoder_use_batchnorm,\n            center=True if encoder_name.startswith(\"vgg\") else False,\n            attention_type=decoder_attention_type,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=classes,\n            activation=activation,\n            kernel_size=1,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)\n        else:\n            self.classification_head = None\n\n        self.name = \"resunet-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"api_ref/nn/models/resunetplusplus/","title":"resunet++","text":"<p>The ResUNet++.</p>"},{"location":"api_ref/nn/models/resunetplusplus/#decoder","title":"Decoder","text":"<p>Code credit: https://github.com/jlcsilva/segmentation_models.pytorch</p>"},{"location":"api_ref/nn/models/resunetplusplus/#kelp.nn.models.resunetplusplus.decoder.ASPP","title":"<code>kelp.nn.models.resunetplusplus.decoder.ASPP</code>","text":"<p>             Bases: <code>Module</code></p> <p>ASPP described in https://arxiv.org/pdf/1706.05587.pdf but without the concatenation of 1x1, original feature maps and global average pooling</p> Source code in <code>kelp/nn/models/resunetplusplus/decoder.py</code> <pre><code>class ASPP(nn.Module):\n    \"\"\"\n    ASPP described in https://arxiv.org/pdf/1706.05587.pdf but without the concatenation of 1x1,\n    original feature maps and global average pooling\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, rate: Tuple[int, int, int] = (6, 12, 18)) -&gt; None:\n        super(ASPP, self).__init__()\n\n        # Dilation rates of 6, 12 and 18 for the Atrous Spatial Pyramid Pooling blocks\n        self.aspp_block1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=rate[0], dilation=rate[0]),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n        )\n        self.aspp_block2 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=rate[1], dilation=rate[1]),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n        )\n        self.aspp_block3 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=rate[2], dilation=rate[2]),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n        )\n        self.aspp_block4 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n        )\n\n        self.output = nn.Conv2d((len(rate) + 1) * out_channels, out_channels, kernel_size=1)\n        self._init_weights()\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x1 = self.aspp_block1(x)\n        x2 = self.aspp_block2(x)\n        x3 = self.aspp_block3(x)\n        x4 = self.aspp_block4(x)\n        out = torch.cat([x1, x2, x3, x4], dim=1)\n\n        return self.output(out)\n\n    def _init_weights(self) -&gt; None:\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n</code></pre>"},{"location":"api_ref/nn/models/resunetplusplus/#model","title":"Model","text":"<p>Code credit: https://github.com/jlcsilva/segmentation_models.pytorch</p>"},{"location":"api_ref/nn/models/resunetplusplus/#kelp.nn.models.resunetplusplus.model.ResUnetPlusPlus","title":"<code>kelp.nn.models.resunetplusplus.model.ResUnetPlusPlus</code>","text":"<p>             Bases: <code>SegmentationModel</code></p> <p>ResUnet++ is a full-convolutional neural network for image semantic segmentation. Consist of encoder and decoder parts connected with skip connections. The encoder extracts features of different spatial resolution (skip connections) which are used by decoder to define accurate segmentation mask.</p> <p>Applies attention to the skip connection feature maps, based on themselves and the decoder feature maps. The skip connection feature maps are then fused with the decoder feature maps through concatenation. Uses an Atrous Spatial Pyramid Pooling (ASPP) bridge module and residual connections inside each decoder blocks.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of the classification model that will be used as an encoder (a.k.a. backbone)     to extract features of different spatial resolution</p> <code>'resnet34'</code> <code>encoder_depth</code> <code>int</code> <p>A number of stages used in encoder in range [3, 5]. Each stage generate features two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on). Default is 5</p> <code>5</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>One of None (random initialization), \"imagenet\" (pre-training on ImageNet) and other pretrained weights (see table with available weights for each encoder_name)</p> <code>'imagenet'</code> <code>decoder_channels</code> <code>Optional[List[int]]</code> <p>List of integers which specify in_channels parameter for convolutions used in decoder. Length of the list should be the same as encoder_depth</p> <code>None</code> <code>decoder_use_batchnorm</code> <code>bool</code> <p>If True, BatchNorm2d layer between Conv2D and Activation layers is used. If \"inplace\" InplaceABN will be used, allows to decrease memory consumption. Available options are True, False, \"inplace\"</p> <code>True</code> <code>decoder_attention_type</code> <code>Optional[str]</code> <p>Attention module used in decoder of the model. Available options are None and scse (https://arxiv.org/abs/1808.08127).</p> <code>None</code> <code>in_channels</code> <code>int</code> <p>A number of input channels for the model, default is 3 (RGB images)</p> <code>3</code> <code>classes</code> <code>int</code> <p>A number of classes for output mask (or you can think as a number of channels of output mask)</p> <code>1</code> <code>activation</code> <code>Optional[Union[str, Callable[[Any], Any]]]</code> <p>An activation function to apply after the final convolution layer. Available options are \"sigmoid\", \"softmax\", \"logsoftmax\", \"tanh\", \"identity\",     callable and None. Default is None</p> <code>None</code> <code>aux_params</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build on top of encoder if aux_params is not None (default). Supported params:     - classes (int): A number of classes     - pooling (str): One of \"max\", \"avg\". Default is \"avg\"     - dropout (float): Dropout factor in [0, 1)     - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"         (could be None to return logits)</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.nn.Module</code>: ResUnetPlusPlus</p> Reference <p>Jha et al. 2019</p> Source code in <code>kelp/nn/models/resunetplusplus/model.py</code> <pre><code>class ResUnetPlusPlus(SegmentationModel):\n    \"\"\"ResUnet++ is a full-convolutional neural network for image semantic segmentation. Consist of *encoder*\n    and *decoder* parts connected with *skip connections*. The encoder extracts features of different spatial\n    resolution (skip connections) which are used by decoder to define accurate segmentation mask.\n\n    Applies attention to the skip connection feature maps, based on themselves and the decoder feature maps.\n    The skip connection feature maps are then fused with the decoder feature maps through *concatenation*.\n    Uses an Atrous Spatial Pyramid Pooling (ASPP) bridge module and residual connections inside each decoder\n    blocks.\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a. backbone)\n                to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n            Length of the list should be the same as **encoder_depth**\n        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n            Available options are **True, False, \"inplace\"**\n        decoder_attention_type: Attention module used in decoder of the model. Available options are\n            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n                **callable** and **None**.\n            Default is **None**\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n                    (could be **None** to return logits)\n\n    Returns:\n        ``torch.nn.Module``: ResUnetPlusPlus\n\n    Reference:\n        [Jha et al. 2019](https://arxiv.org/abs/1911.07067)\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_use_batchnorm: bool = True,\n        decoder_channels: Optional[List[int]] = None,\n        decoder_attention_type: Optional[str] = None,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, Callable[[Any], Any]]] = None,\n        aux_params: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if decoder_channels is None:\n            decoder_channels = [256, 128, 64, 32, 16]\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = ResUnetPlusPlusDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            use_batchnorm=decoder_use_batchnorm,\n            attention_type=decoder_attention_type,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=classes,\n            activation=activation,\n            kernel_size=1,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)\n        else:\n            self.classification_head = None\n\n        self.name = \"resunet++-{}\".format(encoder_name)\n        self.initialize()\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/","title":"segmentation","text":"<p>The Kelp Forest Segmentation Task.</p>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>A lightning module for segmentation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Model specific keyword arguments.</p> <code>{}</code> Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>class KelpForestSegmentationTask(pl.LightningModule):\n    \"\"\"\n    A lightning module for segmentation tasks.\n\n    Args:\n        kwargs: Model specific keyword arguments.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        super().__init__()\n        self.save_hyperparameters()  # type: ignore[operator]\n        self.hyperparams = cast(Dict[str, Any], self.hparams)\n        self.loss = resolve_loss(\n            loss_fn=self.hyperparams[\"loss\"],\n            device=self.device,\n            ignore_index=self.hyperparams[\"ignore_index\"],\n            num_classes=self.hyperparams[\"num_classes\"],\n            objective=self.hyperparams[\"objective\"],\n            ce_smooth_factor=self.hyperparams[\"ce_smooth_factor\"],\n            ce_class_weights=self.hyperparams[\"ce_class_weights\"],\n        )\n        self.model = resolve_model(\n            architecture=self.hyperparams[\"architecture\"],\n            encoder=self.hyperparams[\"encoder\"],\n            encoder_weights=self.hyperparams[\"encoder_weights\"],\n            decoder_channels=self.hyperparams.get(\"decoder_channels\", [256, 128, 64, 32, 16]),\n            decoder_attention_type=self.hyperparams[\"decoder_attention_type\"],\n            pretrained=self.hyperparams[\"pretrained\"],\n            in_channels=self.hyperparams[\"in_channels\"],\n            classes=self.hyperparams[\"num_classes\"],\n            compile=self.hyperparams[\"compile\"],\n            compile_mode=self.hyperparams[\"compile_mode\"],\n            compile_dynamic=self.hyperparams[\"compile_dynamic\"],\n            ort=self.hyperparams[\"ort\"],\n        )\n        self.train_metrics = MetricCollection(\n            metrics={\n                \"dice\": Dice(\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                    average=\"macro\",\n                ),\n            },\n            prefix=\"train/\",\n        )\n        self.val_metrics = MetricCollection(\n            metrics={\n                \"dice\": Dice(\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                    average=\"macro\",\n                ),\n                \"iou\": JaccardIndex(\n                    task=self.hparams[\"objective\"],\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                ),\n                \"per_class_iou\": JaccardIndex(\n                    task=\"multiclass\",  # must be 'multiclass' for per-class IoU\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                    average=\"none\",\n                ),\n                \"accuracy\": Accuracy(\n                    task=self.hparams[\"objective\"],\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                ),\n                \"recall\": Recall(\n                    task=self.hparams[\"objective\"],\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                ),\n                \"precision\": Precision(\n                    task=self.hparams[\"objective\"],\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                ),\n                \"f1\": F1Score(\n                    task=self.hparams[\"objective\"],\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                ),\n                \"conf_mtrx\": ConfusionMatrix(\n                    task=self.hparams[\"objective\"],\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                ),\n                \"norm_conf_mtrx\": ConfusionMatrix(\n                    task=self.hparams[\"objective\"],\n                    num_classes=self.hyperparams[\"num_classes\"],\n                    ignore_index=self.hyperparams[\"ignore_index\"],\n                    normalize=\"true\",\n                ),\n            },\n            prefix=\"val/\",\n        )\n        self.test_metrics = self.val_metrics.clone(prefix=\"test/\")\n\n    @property\n    def num_training_steps(self) -&gt; int:\n        \"\"\"Return the number of training steps.\"\"\"\n        return self.trainer.estimated_stepping_batches  # type: ignore[no-any-return]\n\n    def _log_predictions_batch(self, batch: Dict[str, Tensor], batch_idx: int, y_hat_hard: Tensor) -&gt; None:\n        # Ensure global step is non-zero -&gt; that we are not running plotting during sanity val step check\n        epoch = self.current_epoch\n        step = self.global_step\n        if batch_idx &lt; self.hyperparams[\"plot_n_batches\"] and step:\n            datamodule = self.trainer.datamodule  # type: ignore[attr-defined]\n            band_index_lookup = {band: idx for idx, band in enumerate(datamodule.bands_to_use)}\n            can_plot_true_color = all(band in band_index_lookup for band in [\"R\", \"G\", \"B\"])\n            can_plot_color_infrared_color = all(band in band_index_lookup for band in [\"NIR\", \"R\", \"G\"])\n            can_plot_shortwave_infrared_color = all(band in band_index_lookup for band in [\"SWIR\", \"NIR\", \"R\"])\n            batch[\"prediction\"] = y_hat_hard\n            for key in [\"image\", \"mask\", \"prediction\"]:\n                batch[key] = batch[key].cpu()\n            fig_grids = datamodule.plot_batch(\n                batch=batch,\n                band_index_lookup=band_index_lookup,\n                plot_true_color=epoch == 0 and can_plot_true_color,\n                plot_color_infrared_grid=epoch == 0 and can_plot_color_infrared_color,\n                plot_short_wave_infrared_grid=epoch == 0 and can_plot_shortwave_infrared_color,\n                plot_spectral_indices=epoch == 0,\n                plot_qa_grid=epoch == 0 and \"QA\" in band_index_lookup,\n                plot_dem_grid=epoch == 0 and \"DEM\" in band_index_lookup,\n                plot_mask_grid=epoch == 0,\n                plot_prediction_grid=True,\n            )\n            for key, fig in dataclasses.asdict(fig_grids).items():\n                if fig is None:\n                    continue\n\n                if isinstance(fig, dict):\n                    for nested_key, nested_figure in fig.items():\n                        self.logger.experiment.log_figure(  # type: ignore[attr-defined]\n                            run_id=self.logger.run_id,  # type: ignore[attr-defined]\n                            figure=nested_figure,\n                            artifact_file=f\"images/{key}/{nested_key}_{batch_idx=}_{epoch=:02d}_{step=:04d}.jpg\",\n                        )\n                        plt.close(nested_figure)\n                else:\n                    self.logger.experiment.log_figure(  # type: ignore[attr-defined]\n                        run_id=self.logger.run_id,  # type: ignore[attr-defined]\n                        figure=fig,\n                        artifact_file=f\"images/{key}/{key}_{batch_idx=}_{epoch=:02d}_{step=:04d}.jpg\",\n                    )\n            plt.close()\n\n    def _log_confusion_matrices(\n        self, metrics: Dict[str, Tensor], stage: Literal[\"val\", \"test\"], cmap: str = \"Blues\"\n    ) -&gt; None:\n        epoch = self.current_epoch\n        step = self.global_step\n        for metric_key, title, matrix_kind in zip(\n            [f\"{stage}/conf_mtrx\", f\"{stage}/norm_conf_mtrx\"],\n            [\"Confusion matrix\", \"Normalized confusion matrix\"],\n            [\"confusion_matrix\", \"confusion_matrix_normalized\"],\n        ):\n            conf_matrix = metrics.pop(metric_key)\n            # Ensure global step is non-zero -&gt; that we are not running plotting during sanity val step check\n            if step == 0:\n                continue\n            fig, axes = plt.subplots(1, 1, figsize=(7, 5))\n            ConfusionMatrixDisplay(\n                confusion_matrix=conf_matrix.detach().cpu().numpy(),\n                display_labels=consts.data.CLASSES,\n            ).plot(\n                colorbar=True,\n                cmap=cmap,\n                ax=axes,\n            )\n            axes.set_title(title)\n            self.logger.experiment.log_figure(  # type: ignore[attr-defined]\n                run_id=self.logger.run_id,  # type: ignore[attr-defined]\n                figure=fig,\n                artifact_file=f\"images/{stage}_{matrix_kind}/{matrix_kind}_{epoch=:02d}_{step=:04d}.jpg\",\n            )\n            plt.close(fig)\n\n    def _predict_with_extra_steps_if_necessary(self, x: Tensor) -&gt; Tuple[Tensor, Tensor, Optional[Tensor]]:\n        if self.hyperparams.get(\"tta\", False):\n            tta_model = tta.SegmentationTTAWrapper(\n                model=self.model,\n                transforms=_test_time_transforms,\n                merge_mode=self.hyperparams.get(\"tta_merge_mode\", \"mean\"),\n            )\n            y_hat = tta_model(x)\n        else:\n            y_hat = self.forward(x)\n\n        if self.hyperparams.get(\"decision_threshold\", None):\n            y_hat_hard = (  # type: ignore[attr-defined]\n                y_hat.sigmoid()[:, 1, :, :] &gt;= self.hyperparams[\"decision_threshold\"]\n            ).long()\n        else:\n            y_hat_hard = y_hat.argmax(dim=1)\n\n        if self.hyperparams.get(\"soft_labels\", False):\n            y_hat_soft = y_hat.sigmoid()[:, 1, :, :].float()\n        else:\n            y_hat_soft = None\n\n        return y_hat, y_hat_hard, y_hat_soft\n\n    def _guard_against_nan(self, x: Tensor) -&gt; None:\n        if torch.isnan(x):\n            raise ValueError(\"NaN encountered during training! Aborting.\")\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            *args: Positional arguments to be passed to the model.\n            **kwargs: Keyword arguments to be passed to the model.\n\n        Returns: Raw model outputs.\n\n        \"\"\"\n        return self.model(*args, **kwargs)\n\n    def training_step(self, *args: Any, **kwargs: Any) -&gt; Tensor:\n        \"\"\"\n        Compute and return the training loss.\n\n        Args:\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n            batch_idx: The index of this batch.\n            dataloader_idx: The index of the dataloader that produced this batch.\n                (only if multiple dataloaders used)\n\n        Return:\n            - :class:`~torch.Tensor` - The loss tensor\n\n        \"\"\"\n        batch = args[0]\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        y_hat = self.forward(x)\n        y_hat_hard = y_hat.argmax(dim=1)\n        loss = self.loss(y_hat, y)\n        self._guard_against_nan(loss)\n        self.log(\"train/loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n        self.train_metrics(y_hat_hard, y)\n        return cast(Tensor, loss)\n\n    def on_train_epoch_end(self) -&gt; None:\n        \"\"\"Called in the training loop at the very end of the epoch.\"\"\"\n        self.log_dict(self.train_metrics.compute())\n        self.train_metrics.reset()\n\n    def validation_step(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Compute the validation loss and log validation metrics and sample predictions.\n\n        Args:\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n            batch_idx: The index of this batch.\n            dataloader_idx: The index of the dataloader that produced this batch.\n                (only if multiple dataloaders used)\n\n        \"\"\"\n        batch = args[0]\n        batch_idx = args[1]\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        y_hat = self.forward(x)\n        y_hat_hard = y_hat.argmax(dim=1)\n        loss = self.loss(y_hat, y)\n        self._guard_against_nan(loss)\n        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, batch_size=x.shape[0], prog_bar=True)\n        self.val_metrics(y_hat_hard, y)\n        self._log_predictions_batch(batch, batch_idx, y_hat_hard)\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Called in the validation loop at the very end of the epoch.\"\"\"\n        metrics = self.val_metrics.compute()\n        per_class_iou = metrics.pop(\"val/per_class_iou\")\n        self._log_confusion_matrices(metrics, stage=\"val\")\n        per_class_iou_score_dict = {\n            f\"val/iou_{consts.data.CLASSES[idx]}\": iou_score for idx, iou_score in enumerate(per_class_iou)\n        }\n        metrics.update(per_class_iou_score_dict)\n        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n        self.val_metrics.reset()\n\n    def test_step(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Compute the test loss and test metrics.\n\n        Args:\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n            batch_idx: The index of this batch.\n            dataloader_idx: The index of the dataloader that produced this batch.\n                (only if multiple dataloaders used)\n\n        \"\"\"\n        batch = args[0]\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        y_hat, y_hat_hard, _ = self._predict_with_extra_steps_if_necessary(x)\n        loss = self.loss(y_hat, y)\n        self._guard_against_nan(loss)\n        self.log(\"test/loss\", loss, on_step=False, on_epoch=True, batch_size=x.shape[0])\n        self.test_metrics(y_hat_hard, y)\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Called in the test loop at the very end of the epoch.\"\"\"\n        metrics = self.test_metrics.compute()\n        per_class_iou = metrics.pop(\"test/per_class_iou\")\n        self._log_confusion_matrices(metrics, stage=\"test\")\n        per_class_iou_score_dict = {\n            f\"test/iou_{consts.data.CLASSES[idx]}\": iou_score for idx, iou_score in enumerate(per_class_iou)\n        }\n        metrics.update(per_class_iou_score_dict)\n        self.log_dict(metrics, on_step=False, on_epoch=True)\n        self.test_metrics.reset()\n\n    def predict_step(self, *args: Any, **kwargs: Any) -&gt; Dict[str, Tensor]:\n        \"\"\"\n        Runs prediction logic on a single batch of input tensors.\n\n        Args:\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n            batch_idx: The index of this batch.\n            dataloader_idx: The index of the dataloader that produced this batch.\n                (only if multiple dataloaders used)\n\n        Returns: A dictionary with input batch extended with predictions.\n\n        \"\"\"\n        batch = args[0]\n        x = batch.pop(\"image\")\n        y_hat, y_hat_hard, y_hat_soft = self._predict_with_extra_steps_if_necessary(x)\n        batch[\"prediction\"] = y_hat_soft if self.hyperparams.get(\"soft_labels\", False) else y_hat_hard\n        return batch  # type: ignore[no-any-return]\n\n    def configure_optimizers(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Configures Optimizer and LR Scheduler based on passed hyperparameters.\n\n        Returns: Optimizer and learning rate scheduler config.\n\n        \"\"\"\n        optimizer = resolve_optimizer(\n            params=self.model.parameters(),\n            hyperparams=self.hyperparams,\n        )\n        total_steps = self.num_training_steps\n        scheduler = resolve_lr_scheduler(\n            optimizer=optimizer,\n            num_training_steps=total_steps,\n            steps_per_epoch=math.ceil(total_steps / self.hyperparams[\"epochs\"]),\n            hyperparams=self.hyperparams,\n        )\n        if scheduler is None:\n            return {\"optimizer\": optimizer}\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"step\"\n                if self.hyperparams[\"lr_scheduler\"] in [\"onecycle\", \"cyclic\", \"cosine_with_warm_restarts\"]\n                else \"epoch\",\n                \"monitor\": \"val/loss\",\n            },\n        }\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.num_training_steps","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.num_training_steps: int</code>  <code>property</code>","text":"<p>Return the number of training steps.</p>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.configure_optimizers","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.configure_optimizers</code>","text":"<p>Configures Optimizer and LR Scheduler based on passed hyperparameters.</p> <p>Returns: Optimizer and learning rate scheduler config.</p> Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>def configure_optimizers(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Configures Optimizer and LR Scheduler based on passed hyperparameters.\n\n    Returns: Optimizer and learning rate scheduler config.\n\n    \"\"\"\n    optimizer = resolve_optimizer(\n        params=self.model.parameters(),\n        hyperparams=self.hyperparams,\n    )\n    total_steps = self.num_training_steps\n    scheduler = resolve_lr_scheduler(\n        optimizer=optimizer,\n        num_training_steps=total_steps,\n        steps_per_epoch=math.ceil(total_steps / self.hyperparams[\"epochs\"]),\n        hyperparams=self.hyperparams,\n    )\n    if scheduler is None:\n        return {\"optimizer\": optimizer}\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"interval\": \"step\"\n            if self.hyperparams[\"lr_scheduler\"] in [\"onecycle\", \"cyclic\", \"cosine_with_warm_restarts\"]\n            else \"epoch\",\n            \"monitor\": \"val/loss\",\n        },\n    }\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.forward","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.forward</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments to be passed to the model.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the model.</p> <code>{}</code> Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        *args: Positional arguments to be passed to the model.\n        **kwargs: Keyword arguments to be passed to the model.\n\n    Returns: Raw model outputs.\n\n    \"\"\"\n    return self.model(*args, **kwargs)\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.on_test_epoch_end","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.on_test_epoch_end</code>","text":"<p>Called in the test loop at the very end of the epoch.</p> Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Called in the test loop at the very end of the epoch.\"\"\"\n    metrics = self.test_metrics.compute()\n    per_class_iou = metrics.pop(\"test/per_class_iou\")\n    self._log_confusion_matrices(metrics, stage=\"test\")\n    per_class_iou_score_dict = {\n        f\"test/iou_{consts.data.CLASSES[idx]}\": iou_score for idx, iou_score in enumerate(per_class_iou)\n    }\n    metrics.update(per_class_iou_score_dict)\n    self.log_dict(metrics, on_step=False, on_epoch=True)\n    self.test_metrics.reset()\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.on_train_epoch_end","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.on_train_epoch_end</code>","text":"<p>Called in the training loop at the very end of the epoch.</p> Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>def on_train_epoch_end(self) -&gt; None:\n    \"\"\"Called in the training loop at the very end of the epoch.\"\"\"\n    self.log_dict(self.train_metrics.compute())\n    self.train_metrics.reset()\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.on_validation_epoch_end","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.on_validation_epoch_end</code>","text":"<p>Called in the validation loop at the very end of the epoch.</p> Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Called in the validation loop at the very end of the epoch.\"\"\"\n    metrics = self.val_metrics.compute()\n    per_class_iou = metrics.pop(\"val/per_class_iou\")\n    self._log_confusion_matrices(metrics, stage=\"val\")\n    per_class_iou_score_dict = {\n        f\"val/iou_{consts.data.CLASSES[idx]}\": iou_score for idx, iou_score in enumerate(per_class_iou)\n    }\n    metrics.update(per_class_iou_score_dict)\n    self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n    self.val_metrics.reset()\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.predict_step","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.predict_step</code>","text":"<p>Runs prediction logic on a single batch of input tensors.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</p> required <code>batch_idx</code> <p>The index of this batch.</p> required <code>dataloader_idx</code> <p>The index of the dataloader that produced this batch. (only if multiple dataloaders used)</p> required Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>def predict_step(self, *args: Any, **kwargs: Any) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    Runs prediction logic on a single batch of input tensors.\n\n    Args:\n        batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n        batch_idx: The index of this batch.\n        dataloader_idx: The index of the dataloader that produced this batch.\n            (only if multiple dataloaders used)\n\n    Returns: A dictionary with input batch extended with predictions.\n\n    \"\"\"\n    batch = args[0]\n    x = batch.pop(\"image\")\n    y_hat, y_hat_hard, y_hat_soft = self._predict_with_extra_steps_if_necessary(x)\n    batch[\"prediction\"] = y_hat_soft if self.hyperparams.get(\"soft_labels\", False) else y_hat_hard\n    return batch  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.test_step","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.test_step</code>","text":"<p>Compute the test loss and test metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</p> required <code>batch_idx</code> <p>The index of this batch.</p> required <code>dataloader_idx</code> <p>The index of the dataloader that produced this batch. (only if multiple dataloaders used)</p> required Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>def test_step(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Compute the test loss and test metrics.\n\n    Args:\n        batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n        batch_idx: The index of this batch.\n        dataloader_idx: The index of the dataloader that produced this batch.\n            (only if multiple dataloaders used)\n\n    \"\"\"\n    batch = args[0]\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    y_hat, y_hat_hard, _ = self._predict_with_extra_steps_if_necessary(x)\n    loss = self.loss(y_hat, y)\n    self._guard_against_nan(loss)\n    self.log(\"test/loss\", loss, on_step=False, on_epoch=True, batch_size=x.shape[0])\n    self.test_metrics(y_hat_hard, y)\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.training_step","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.training_step</code>","text":"<p>Compute and return the training loss.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</p> required <code>batch_idx</code> <p>The index of this batch.</p> required <code>dataloader_idx</code> <p>The index of the dataloader that produced this batch. (only if multiple dataloaders used)</p> required Return <ul> <li>:class:<code>~torch.Tensor</code> - The loss tensor</li> </ul> Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>def training_step(self, *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Compute and return the training loss.\n\n    Args:\n        batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n        batch_idx: The index of this batch.\n        dataloader_idx: The index of the dataloader that produced this batch.\n            (only if multiple dataloaders used)\n\n    Return:\n        - :class:`~torch.Tensor` - The loss tensor\n\n    \"\"\"\n    batch = args[0]\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    y_hat = self.forward(x)\n    y_hat_hard = y_hat.argmax(dim=1)\n    loss = self.loss(y_hat, y)\n    self._guard_against_nan(loss)\n    self.log(\"train/loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n    self.train_metrics(y_hat_hard, y)\n    return cast(Tensor, loss)\n</code></pre>"},{"location":"api_ref/nn/models/segmentation/#kelp.nn.models.segmentation.KelpForestSegmentationTask.validation_step","title":"<code>kelp.nn.models.segmentation.KelpForestSegmentationTask.validation_step</code>","text":"<p>Compute the validation loss and log validation metrics and sample predictions.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</p> required <code>batch_idx</code> <p>The index of this batch.</p> required <code>dataloader_idx</code> <p>The index of the dataloader that produced this batch. (only if multiple dataloaders used)</p> required Source code in <code>kelp/nn/models/segmentation.py</code> <pre><code>def validation_step(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Compute the validation loss and log validation metrics and sample predictions.\n\n    Args:\n        batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n        batch_idx: The index of this batch.\n        dataloader_idx: The index of the dataloader that produced this batch.\n            (only if multiple dataloaders used)\n\n    \"\"\"\n    batch = args[0]\n    batch_idx = args[1]\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    y_hat = self.forward(x)\n    y_hat_hard = y_hat.argmax(dim=1)\n    loss = self.loss(y_hat, y)\n    self._guard_against_nan(loss)\n    self.log(\"val/loss\", loss, on_step=False, on_epoch=True, batch_size=x.shape[0], prog_bar=True)\n    self.val_metrics(y_hat_hard, y)\n    self._log_predictions_batch(batch, batch_idx, y_hat_hard)\n</code></pre>"},{"location":"api_ref/nn/training/config/","title":"config","text":"<p>The segmentation neural network training config.</p>"},{"location":"api_ref/nn/training/config/#kelp.nn.training.config.TrainConfig","title":"<code>kelp.nn.training.config.TrainConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>The training configuration.</p> Source code in <code>kelp/nn/training/config.py</code> <pre><code>class TrainConfig(ConfigBase):\n    \"\"\"The training configuration.\"\"\"\n\n    # data params\n    data_dir: Path\n    metadata_fp: Path\n    dataset_stats_fp: Path\n    cv_split: int = 0\n    bands: List[str]\n    spectral_indices: List[str]\n    sahi: bool = False\n    image_size: int = 352\n    resize_strategy: Literal[\"pad\", \"resize\"] = \"pad\"\n    interpolation: Literal[\"nearest\", \"nearest-exact\", \"bilinear\", \"bicubic\"] = \"nearest\"\n    batch_size: int = 32\n    num_workers: int = 4\n    normalization_strategy: Literal[\n        \"min-max\",\n        \"z-score\",\n        \"quantile\",\n        \"per-sample-quantile\",\n        \"per-sample-min-max\",\n    ] = \"quantile\"\n    fill_missing_pixels_with_torch_nan: bool = False\n    mask_using_qa: bool = False\n    mask_using_water_mask: bool = False\n    use_weighted_sampler: bool = False\n    samples_per_epoch: int = 10240\n    has_kelp_importance_factor: float = 3.0\n    kelp_pixels_pct_importance_factor: float = 0.2\n    qa_ok_importance_factor: float = 0.0\n    qa_corrupted_pixels_pct_importance_factor: float = -1.0\n    almost_all_water_importance_factor: float = 0.5\n    dem_nan_pixels_pct_importance_factor: float = 0.25\n    dem_zero_pixels_pct_importance_factor: float = -1.0\n\n    # model params\n    architecture: Literal[\n        \"deeplabv3\",\n        \"deeplabv3+\",\n        \"efficientunet++\",\n        \"fcn\",\n        \"fpn\",\n        \"linknet\",\n        \"manet\",\n        \"pan\",\n        \"pspnet\",\n        \"resunet\",\n        \"resunet++\",\n        \"unet\",\n        \"unet++\",\n    ] = \"unet\"\n    encoder: str = \"tu-efficientnet_b5\"\n    encoder_weights: Optional[str] = None\n    encoder_depth: int = 5\n    decoder_channels: List[int] = [256, 128, 64, 32, 16]\n    decoder_attention_type: Optional[str] = None\n    pretrained: bool = False\n    num_classes: int = 2\n    ignore_index: Optional[int] = None\n\n    # optimizer params\n    optimizer: Literal[\"adam\", \"adamw\", \"sgd\"] = \"adamw\"\n    weight_decay: float = 1e-4\n\n    # lr scheduler params\n    lr_scheduler: Optional[\n        Literal[\n            \"onecycle\",\n            \"cosine\",\n            \"cosine_with_warm_restarts\",\n            \"cyclic\",\n            \"reduce_lr_on_plateau\",\n            \"none\",\n        ]\n    ] = None\n    lr: float = 3e-4\n    onecycle_pct_start: float = 0.1\n    onecycle_div_factor: float = 2.0\n    onecycle_final_div_factor: float = 1e2\n    cyclic_base_lr: float = 1e-5\n    cyclic_mode: Literal[\"triangular\", \"triangular2\", \"exp_range\"] = \"exp_range\"\n    cosine_eta_min: float = 1e-7\n    cosine_T_mult: int = 2\n    reduce_lr_on_plateau_factor: float = 0.95\n    reduce_lr_on_plateau_patience: int = 2\n    reduce_lr_on_plateau_threshold: float = 1e-4\n    reduce_lr_on_plateau_min_lr: float = 1e-6\n\n    # loss params\n    objective: Literal[\"binary\", \"multiclass\"] = \"binary\"\n    loss: Literal[\n        \"ce\",\n        \"jaccard\",\n        \"dice\",\n        \"tversky\",\n        \"focal\",\n        \"lovasz\",\n        \"soft_ce\",\n        \"xedice\",\n        \"focal_tversky\",\n        \"log_cosh_dice\",\n        \"hausdorff\",\n        \"t_loss\",\n        \"combo\",\n        \"exp_log_loss\",\n        \"soft_dice\",\n        \"batch_soft_dice\",\n    ] = \"dice\"\n    ce_smooth_factor: float = 0.0\n    ce_class_weights: Optional[List[float]] = None\n\n    # compile/ort params\n    compile: bool = False\n    compile_mode: Literal[\"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"] = \"default\"\n    compile_dynamic: Optional[bool] = None\n    ort: bool = False\n\n    # eval loop extra params\n    plot_n_batches: int = 3\n    tta: bool = False\n    tta_merge_mode: Literal[\"min\", \"max\", \"mean\", \"gmean\", \"sum\", \"tsharpen\"] = \"max\"\n    decision_threshold: Optional[float] = None\n\n    # callback params\n    save_top_k: int = 1\n    monitor_metric: str = \"val/dice\"\n    monitor_mode: Literal[\"min\", \"max\"] = \"max\"\n    early_stopping_patience: int = 10\n    swa: bool = False\n    swa_epoch_start: float = 0.75\n    swa_annealing_epochs: int = 10\n    swa_lr: float = 3e-5\n\n    # trainer params\n    precision: Literal[\n        \"16-true\",\n        \"16-mixed\",\n        \"bf16-true\",\n        \"bf16-mixed\",\n        \"32-true\",\n    ] = \"bf16-mixed\"\n    fast_dev_run: bool = False\n    epochs: int = 1\n    limit_train_batches: Optional[Union[int, float]] = None\n    limit_val_batches: Optional[Union[int, float]] = None\n    limit_test_batches: Optional[Union[int, float]] = None\n    log_every_n_steps: int = 50\n    accumulate_grad_batches: int = 1\n    val_check_interval: Optional[float] = None\n    benchmark: bool = False\n\n    # misc\n    experiment: str = \"kelp-seg-training-exp\"\n    output_dir: Path\n    seed: int = 42\n\n    @model_validator(mode=\"before\")\n    def validate_encoder(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n        if values[\"pretrained\"] and values[\"encoder\"].startswith(\"tu-\"):\n            import timm\n\n            encoder = values[\"encoder\"].replace(\"tu-\", \"\")\n            if not any(e.startswith(encoder) for e in timm.list_pretrained()):\n                _logger.warning(f\"No pretrained weights exist for tu-{encoder}. Forcing training with random init.\")\n                values[\"pretrained\"] = False\n                values[\"encoder_weights\"] = None\n\n        for img_size, bs in zip([224, 256, 336, 384, 448, 512], [32, 32, 32, 32, 4, 4]):\n            if f\"{img_size}\" in values[\"encoder\"] and values[\"image_size\"] != img_size:\n                _logger.warning(f\"Encoder requires image_size={img_size}. Forcing training with adjusted image size.\")\n                values[\"image_size\"] = img_size\n                values[\"resize_strategy\"] = \"resize\"\n                values[\"batch_size\"] = min(bs, values[\"batch_size\"])\n                values[\"accumulate_grad_batches\"] = max(1, 32 // bs)\n                _logger.info(\n                    f\"Adjusted image_size={values['image_size']}, \"\n                    f\"resize_strategy={values['resize_strategy']}, \"\n                    f\"batch_size={values['batch_size']}, \"\n                    f\"accumulate_grad_batches={values['accumulate_grad_batches']}\"\n                )\n                break\n\n        channels_item = values.get(\"decoder_channels\", \"256,128,64,32,16\")\n        channels = (\n            channels_item\n            if isinstance(channels_item, list)\n            else [int(index.strip()) for index in channels_item.split(\",\")]\n        )\n        values[\"decoder_channels\"] = channels\n        values[\"encoder_depth\"] = len(channels)\n\n        return values\n\n    @field_validator(\"bands\", mode=\"before\")\n    def validate_bands(cls, value: Optional[Union[str, List[str]]] = None) -&gt; List[str]:\n        all_bands = list(consts.data.ORIGINAL_BANDS)\n        if value is None:\n            return all_bands\n        bands = value if isinstance(value, list) else [band.strip() for band in value.split(\",\")]\n        if set(bands).issubset(all_bands):\n            return bands\n        raise ValueError(f\"{bands=} should be a subset of {all_bands=}\")\n\n    @field_validator(\"spectral_indices\", mode=\"before\")\n    def validate_spectral_indices(cls, value: Union[str, Optional[List[str]]] = None) -&gt; List[str]:\n        if not value:\n            return []\n\n        indices = value if isinstance(value, list) else [index.strip() for index in value.split(\",\")]\n\n        unknown_indices = set(indices).difference(list(SPECTRAL_INDEX_LOOKUP.keys()))\n        if unknown_indices:\n            raise ValueError(\n                f\"Unknown spectral indices were provided: {', '.join(unknown_indices)}. \"\n                f\"Please provide at most 5 comma separated indices: {', '.join(SPECTRAL_INDEX_LOOKUP.keys())}.\"\n            )\n\n        return indices\n\n    @field_validator(\"ce_class_weights\", mode=\"before\")\n    def validate_ce_class_weights(\n        cls,\n        value: Union[Optional[str], Optional[List[float]]] = None,\n    ) -&gt; Optional[List[float]]:\n        if not value:\n            return None\n\n        weights = value if isinstance(value, list) else [float(index.strip()) for index in value.split(\",\")]\n\n        if len(weights) != consts.data.NUM_CLASSES:\n            raise ValueError(\n                f\"Please provide provide per-class weights! There should be {consts.data.NUM_CLASSES} \"\n                f\"floating point numbers. You provided {len(weights)}\"\n            )\n\n        return weights\n\n    @field_validator(\"lr_scheduler\", mode=\"before\")\n    def validate_lr_scheduler(cls, value: Optional[str] = None) -&gt; Optional[str]:\n        return None if value is None or value == \"none\" else value\n\n    @property\n    def resolved_experiment_name(self) -&gt; str:\n        return os.environ.get(\"MLFLOW_EXPERIMENT_NAME\", self.experiment)\n\n    @property\n    def run_id_from_context(self) -&gt; Optional[str]:\n        return os.environ.get(\"MLFLOW_RUN_ID\", None)\n\n    @property\n    def tags(self) -&gt; Dict[str, Any]:\n        return {\"trained_at\": datetime.utcnow().isoformat()}\n\n    @property\n    def fill_value(self) -&gt; float:\n        return torch.nan if self.fill_missing_pixels_with_torch_nan else 0.0  # type: ignore[no-any-return]\n\n    @property\n    def dataset_stats(self) -&gt; Dict[str, Dict[str, float]]:\n        return json.loads(self.dataset_stats_fp.read_text())  # type: ignore[no-any-return]\n\n    @property\n    def data_module_kwargs(self) -&gt; Dict[str, Any]:\n        return {\n            \"data_dir\": self.data_dir,\n            \"metadata_fp\": self.metadata_fp,\n            \"dataset_stats\": self.dataset_stats,\n            \"cv_split\": self.cv_split,\n            \"bands\": self.bands,\n            \"spectral_indices\": self.spectral_indices,\n            \"image_size\": self.image_size,\n            \"resize_strategy\": self.resize_strategy,\n            \"sahi\": self.sahi,\n            \"interpolation\": self.interpolation,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"normalization_strategy\": self.normalization_strategy,\n            \"missing_pixels_fill_value\": self.fill_value,\n            \"mask_using_qa\": self.mask_using_qa,\n            \"mask_using_water_mask\": self.mask_using_water_mask,\n            \"use_weighted_sampler\": self.use_weighted_sampler,\n            \"samples_per_epoch\": self.samples_per_epoch,\n            \"has_kelp_importance_factor\": self.has_kelp_importance_factor,\n            \"kelp_pixels_pct_importance_factor\": self.kelp_pixels_pct_importance_factor,\n            \"qa_ok_importance_factor\": self.qa_ok_importance_factor,\n            \"qa_corrupted_pixels_pct_importance_factor\": self.qa_corrupted_pixels_pct_importance_factor,\n            \"almost_all_water_importance_factor\": self.almost_all_water_importance_factor,\n            \"dem_nan_pixels_pct_importance_factor\": self.dem_nan_pixels_pct_importance_factor,\n            \"dem_zero_pixels_pct_importance_factor\": self.dem_zero_pixels_pct_importance_factor,\n        }\n\n    @property\n    def callbacks_kwargs(self) -&gt; Dict[str, Any]:\n        return {\n            \"save_top_k\": self.save_top_k,\n            \"monitor_metric\": self.monitor_metric,\n            \"monitor_mode\": self.monitor_mode,\n            \"early_stopping_patience\": self.early_stopping_patience,\n            \"swa\": self.swa,\n            \"swa_epoch_start\": self.swa_epoch_start,\n            \"swa_annealing_epochs\": self.swa_annealing_epochs,\n            \"swa_lr\": self.swa_lr,\n        }\n\n    @property\n    def model_kwargs(self) -&gt; Dict[str, Any]:\n        return {\n            \"architecture\": self.architecture,\n            \"encoder\": self.encoder,\n            \"pretrained\": self.pretrained,\n            \"encoder_weights\": self.encoder_weights,\n            \"encoder_depth\": self.encoder_depth,\n            \"decoder_channels\": self.decoder_channels,\n            \"decoder_attention_type\": self.decoder_attention_type,\n            \"ignore_index\": self.ignore_index,\n            \"num_classes\": self.num_classes,\n            \"optimizer\": self.optimizer,\n            \"weight_decay\": self.weight_decay,\n            \"lr_scheduler\": self.lr_scheduler,\n            \"lr\": self.lr,\n            \"epochs\": self.epochs,\n            \"onecycle_pct_start\": self.onecycle_pct_start,\n            \"onecycle_div_factor\": self.onecycle_div_factor,\n            \"onecycle_final_div_factor\": self.onecycle_final_div_factor,\n            \"cyclic_base_lr\": self.cyclic_base_lr,\n            \"cyclic_mode\": self.cyclic_mode,\n            \"cosine_eta_min\": self.cosine_eta_min,\n            \"cosine_T_mult\": self.cosine_T_mult,\n            \"reduce_lr_on_plateau_factor\": self.reduce_lr_on_plateau_factor,\n            \"reduce_lr_on_plateau_patience\": self.reduce_lr_on_plateau_patience,\n            \"reduce_lr_on_plateau_threshold\": self.reduce_lr_on_plateau_threshold,\n            \"reduce_lr_on_plateau_min_lr\": self.reduce_lr_on_plateau_min_lr,\n            \"objective\": self.objective,\n            \"loss\": self.loss,\n            \"ce_class_weights\": self.ce_class_weights,\n            \"ce_smooth_factor\": self.ce_smooth_factor,\n            \"compile\": self.compile,\n            \"compile_mode\": self.compile_mode,\n            \"compile_dynamic\": self.compile_dynamic,\n            \"ort\": self.ort,\n            \"plot_n_batches\": self.plot_n_batches,\n            \"tta\": self.tta,\n            \"tta_merge_mode\": self.tta_merge_mode,\n            \"decision_threshold\": self.decision_threshold,\n        }\n\n    @property\n    def trainer_kwargs(self) -&gt; Dict[str, Any]:\n        return {\n            \"precision\": self.precision,\n            \"fast_dev_run\": self.fast_dev_run,\n            \"max_epochs\": self.epochs,\n            \"limit_train_batches\": self.limit_train_batches,\n            \"limit_val_batches\": self.limit_val_batches,\n            \"limit_test_batches\": self.limit_test_batches,\n            \"log_every_n_steps\": self.log_every_n_steps,\n            \"accumulate_grad_batches\": self.accumulate_grad_batches,\n            \"val_check_interval\": self.val_check_interval,\n            \"benchmark\": self.benchmark,\n        }\n</code></pre>"},{"location":"api_ref/nn/training/eval/","title":"eval","text":"<p>The segmentation neural network evaluation logic.</p>"},{"location":"api_ref/nn/training/eval/#kelp.nn.training.eval.EvalConfig","title":"<code>kelp.nn.training.eval.EvalConfig</code>","text":"<p>             Bases: <code>PredictConfig</code></p> <p>Config for running NN model evaluation.</p> Source code in <code>kelp/nn/training/eval.py</code> <pre><code>class EvalConfig(PredictConfig):\n    \"\"\"Config for running NN model evaluation.\"\"\"\n\n    metadata_dir: Path\n    experiment_name: str = \"model-eval-exp\"\n    log_model: bool = False\n\n    @property\n    def training_config(self) -&gt; TrainConfig:\n        cfg = super().training_config\n        cfg.metadata_fp = self.metadata_dir / cfg.metadata_fp.name\n        return cfg\n</code></pre>"},{"location":"api_ref/nn/training/eval/#kelp.nn.training.eval.main","title":"<code>kelp.nn.training.eval.main</code>","text":"<p>Main entrypoint for model evaluation.</p> Source code in <code>kelp/nn/training/eval.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for model evaluation.\"\"\"\n    cfg = parse_args()\n    run_eval(\n        run_dir=cfg.run_dir,\n        output_dir=cfg.output_dir,\n        model_checkpoint=cfg.model_checkpoint,\n        use_mlflow=cfg.use_mlflow,\n        train_cfg=cfg.training_config,\n        experiment_name=cfg.experiment_name,\n        tta=cfg.tta,\n        tta_merge_mode=cfg.tta_merge_mode,\n        decision_threshold=cfg.decision_threshold,\n        log_model=cfg.log_model,\n    )\n</code></pre>"},{"location":"api_ref/nn/training/eval/#kelp.nn.training.eval.parse_args","title":"<code>kelp.nn.training.eval.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of EvalConfig.</p> Source code in <code>kelp/nn/training/eval.py</code> <pre><code>def parse_args() -&gt; EvalConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of EvalConfig.\n\n    \"\"\"\n    parser = build_prediction_arg_parser()\n    parser.add_argument(\"--metadata_dir\", type=str, required=True)\n    parser.add_argument(\"--experiment_name\", type=str, default=\"model-eval-exp\")\n    parser.add_argument(\"--log_model\", action=\"store_true\")\n    args = parser.parse_args()\n    cfg = EvalConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/nn/training/eval/#kelp.nn.training.eval.run_eval","title":"<code>kelp.nn.training.eval.run_eval</code>","text":"<p>Runs model evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>run_dir</code> <code>Path</code> <p>The run directory.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required <code>model_checkpoint</code> <code>Path</code> <p>The model checkpoint path.</p> required <code>use_mlflow</code> <code>bool</code> <p>A flag indicating whether to use MLFlow to load the model.</p> required <code>train_cfg</code> <code>TrainConfig</code> <p>The original training config.</p> required <code>experiment_name</code> <code>str</code> <p>The experiment name.</p> required <code>log_model</code> <code>bool</code> <p>A flag indicating whether to log model as an artifact.</p> <code>False</code> <code>tta</code> <code>bool</code> <p>A flag indicating whether to use TTA.</p> <code>False</code> <code>tta_merge_mode</code> <code>str</code> <p>TTA merge mode.</p> <code>'max'</code> <code>decision_threshold</code> <code>Optional[float]</code> <p>An optional decision threshold. Will use :meth:<code>torch.argmax</code> by default.</p> <code>None</code> Source code in <code>kelp/nn/training/eval.py</code> <pre><code>def run_eval(\n    run_dir: Path,\n    output_dir: Path,\n    model_checkpoint: Path,\n    use_mlflow: bool,\n    train_cfg: TrainConfig,\n    experiment_name: str,\n    log_model: bool = False,\n    tta: bool = False,\n    tta_merge_mode: str = \"max\",\n    decision_threshold: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Runs model evaluation.\n\n    Args:\n        run_dir: The run directory.\n        output_dir: The output directory.\n        model_checkpoint: The model checkpoint path.\n        use_mlflow: A flag indicating whether to use MLFlow to load the model.\n        train_cfg: The original training config.\n        experiment_name: The experiment name.\n        log_model: A flag indicating whether to log model as an artifact.\n        tta: A flag indicating whether to use TTA.\n        tta_merge_mode: TTA merge mode.\n        decision_threshold: An optional decision threshold. Will use :meth:`torch.argmax` by default.\n\n    \"\"\"\n    set_gpu_power_limit_if_needed()\n    mlflow.set_experiment(experiment_name)\n    mlflow.pytorch.autolog()\n    run = mlflow.start_run(run_name=run_dir.parts[-1])\n\n    with run:\n        pl.seed_everything(train_cfg.seed, workers=True)\n        mlflow.log_dict(train_cfg.model_dump(mode=\"json\"), artifact_file=\"config.yaml\")\n        mlflow.log_params(train_cfg.model_dump(mode=\"json\"))\n        mlflow.log_params(\n            {\n                \"actual_tta\": tta,\n                \"actual_tta_merge_mode\": tta_merge_mode,\n                \"actual_decision_threshold\": decision_threshold,\n                \"actual_precision\": train_cfg.precision,\n            }\n        )\n        mlflow.set_tags(\n            {\n                \"evaluated_at\": datetime.utcnow().isoformat(),\n                \"original_run_id\": run_dir.parts[-1],\n                \"original_experiment_id\": model_checkpoint.parts[-2],\n            }\n        )\n        mlflow_run_dir = get_mlflow_run_dir(current_run=run, output_dir=output_dir)\n        dm = KelpForestDataModule.from_metadata_file(**train_cfg.data_module_kwargs)\n        model = load_model(\n            model_path=model_checkpoint,\n            use_mlflow=use_mlflow,\n            tta=tta,\n            tta_merge_mode=tta_merge_mode,\n            decision_threshold=decision_threshold,\n        )\n        trainer = pl.Trainer(\n            logger=make_loggers(\n                experiment=train_cfg.resolved_experiment_name,\n                tags=train_cfg.tags,\n            ),\n            callbacks=make_callbacks(\n                output_dir=mlflow_run_dir / \"artifacts\" / \"checkpoints\",\n                **train_cfg.callbacks_kwargs,\n            ),\n            accelerator=\"gpu\",\n            **train_cfg.trainer_kwargs,\n        )\n        trainer.test(model, datamodule=dm)\n        if log_model:\n            mlflow.pytorch.log_model(model, \"model\")\n</code></pre>"},{"location":"api_ref/nn/training/eval_from_folders/","title":"eval_from_folders","text":"<p>The segmentation neural network evaluation from folders logic.</p>"},{"location":"api_ref/nn/training/eval_from_folders/#kelp.nn.training.eval_from_folders.EvaluateFromFoldersConfig","title":"<code>kelp.nn.training.eval_from_folders.EvaluateFromFoldersConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>A config for evaluating from folders.</p> Source code in <code>kelp/nn/training/eval_from_folders.py</code> <pre><code>class EvaluateFromFoldersConfig(ConfigBase):\n    \"\"\"A config for evaluating from folders.\"\"\"\n\n    gt_dir: Path\n    preds_dir: Path\n    tags: Optional[Dict[str, str]] = None\n    experiment_name: str = \"eval-from-folders-exp\"\n    seed: int = consts.reproducibility.SEED\n\n    @field_validator(\"tags\", mode=\"before\")\n    def validate_tags(cls, value: Optional[Union[Dict[str, str], List[str]]]) -&gt; Optional[Dict[str, str]]:\n        if isinstance(value, dict) or value is None:\n            return value\n        tags = {t.split(\"=\")[0]: t.split(\"=\")[1] for t in value}\n        return tags\n</code></pre>"},{"location":"api_ref/nn/training/eval_from_folders/#kelp.nn.training.eval_from_folders.eval_from_folders","title":"<code>kelp.nn.training.eval_from_folders.eval_from_folders</code>","text":"<p>Runs model evaluation using specified ground truth and predictions directories.</p> <p>Parameters:</p> Name Type Description Default <code>gt_dir</code> <code>Path</code> <p>The ground truth directory.</p> required <code>preds_dir</code> <code>Path</code> <p>The predictions' directory.</p> required <code>metrics</code> <code>Optional[MetricCollection]</code> <p>The metrics to use to evaluate the quality of predictions.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for logged metrics.</p> <code>'test'</code> Source code in <code>kelp/nn/training/eval_from_folders.py</code> <pre><code>def eval_from_folders(\n    gt_dir: Path,\n    preds_dir: Path,\n    metrics: Optional[MetricCollection] = None,\n    prefix: Optional[str] = \"test\",\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Runs model evaluation using specified ground truth and predictions directories.\n\n    Args:\n        gt_dir: The ground truth directory.\n        preds_dir: The predictions' directory.\n        metrics: The metrics to use to evaluate the quality of predictions.\n        prefix: The prefix to use for logged metrics.\n\n    Returns: A dictionary of metric names and values.\n\n    \"\"\"\n    gt_fps = sorted(list(gt_dir.glob(\"*.tif\")))\n    preds_fps = sorted(list(preds_dir.rglob(\"*.tif\")))\n\n    if len(gt_fps) != len(preds_fps):\n        raise ValueError(\n            \"Expected all images from GT dir to be present in the Preds dir. \"\n            f\"Found mismatch: GT={len(gt_fps)}, Preds={len(preds_fps)}\"\n        )\n\n    if metrics is None:\n        metrics = MetricCollection(\n            metrics={\n                \"dice\": Dice(num_classes=2, average=\"macro\"),\n                \"iou\": JaccardIndex(task=\"binary\"),\n                \"accuracy\": Accuracy(task=\"binary\"),\n                \"recall\": Recall(task=\"binary\", average=\"macro\"),\n                \"precision\": Precision(task=\"binary\", average=\"macro\"),\n                \"f1\": F1Score(task=\"binary\", average=\"macro\"),\n                \"auroc\": AUROC(task=\"binary\"),\n            },\n            prefix=f\"{prefix}/\",\n        ).to(DEVICE)\n\n    for gt_fp, pred_fp in tqdm(zip(gt_fps, preds_fps), total=len(gt_fps), desc=\"Evaluating masks\"):\n        with rasterio.open(pred_fp) as src:\n            y_pred = src.read(1)\n        with rasterio.open(gt_fp) as src:\n            y_true = src.read(1)\n        metrics(\n            torch.tensor(y_pred, device=DEVICE, dtype=torch.int32),\n            torch.tensor(y_true, device=DEVICE, dtype=torch.int32),\n        )\n\n    metrics_dict = metrics.compute()\n\n    for name, value in metrics_dict.items():\n        metrics_dict[name] = value.item()\n\n    return metrics_dict  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/nn/training/eval_from_folders/#kelp.nn.training.eval_from_folders.main","title":"<code>kelp.nn.training.eval_from_folders.main</code>","text":"<p>Main entrypoint for model evaluation from folders.</p> Source code in <code>kelp/nn/training/eval_from_folders.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for model evaluation from folders.\"\"\"\n    cfg = parse_args()\n    mlflow.set_experiment(cfg.experiment_name)\n    run = mlflow.start_run()\n    with run:\n        pl.seed_everything(cfg.seed, workers=True)\n        mlflow.log_dict(cfg.model_dump(mode=\"json\"), artifact_file=\"config.yaml\")\n        mlflow.log_params(cfg.model_dump(mode=\"json\"))\n        tags = cfg.tags if cfg.tags else {}\n        tags[\"evaluated_at\"] = datetime.utcnow().isoformat()\n        mlflow.log_params(tags)\n        mlflow.set_tags(tags)\n        metrics = eval_from_folders(gt_dir=cfg.gt_dir, preds_dir=cfg.preds_dir)\n        mlflow.log_metrics(metrics)\n        _logger.info(f\"Evaluated metrics: {json.dumps(metrics, indent=4)}\")\n</code></pre>"},{"location":"api_ref/nn/training/eval_from_folders/#kelp.nn.training.eval_from_folders.parse_args","title":"<code>kelp.nn.training.eval_from_folders.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of EvaluateFromFoldersConfig.</p> Source code in <code>kelp/nn/training/eval_from_folders.py</code> <pre><code>def parse_args() -&gt; EvaluateFromFoldersConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of EvaluateFromFoldersConfig.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--gt_dir\", type=str, required=True)\n    parser.add_argument(\"--preds_dir\", type=str, required=True)\n    parser.add_argument(\"--experiment_name\", type=str, default=\"eval-from-folders-exp\")\n    parser.add_argument(\"--tags\", nargs=\"+\")\n    parser.add_argument(\"--seed\", type=int, default=consts.reproducibility.SEED)\n    args = parser.parse_args()\n    cfg = EvaluateFromFoldersConfig(**vars(args))\n    cfg.log_self()\n    return cfg\n</code></pre>"},{"location":"api_ref/nn/training/options/","title":"options","text":"<p>The command line argument parsing for segmentation neural network training.</p>"},{"location":"api_ref/nn/training/options/#kelp.nn.training.options.parse_args","title":"<code>kelp.nn.training.options.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of :class:<code>TrainConfig</code>.</p> Source code in <code>kelp/nn/training/options.py</code> <pre><code>def parse_args() -&gt; TrainConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of :class:`TrainConfig`.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--data_dir\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--metadata_fp\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--dataset_stats_fp\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--experiment\",\n        type=str,\n        default=\"kelp-seg-training-exp\",\n    )\n    parser.add_argument(\n        \"--cv_split\",\n        type=int,\n        default=8,\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        default=32,\n    )\n    parser.add_argument(\n        \"--num_workers\",\n        type=int,\n        default=4,\n    )\n    parser.add_argument(\n        \"--sahi\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--image_size\",\n        type=int,\n        default=352,\n    )\n    parser.add_argument(\n        \"--resize_strategy\",\n        type=str,\n        choices=[\"pad\", \"resize\"],\n        default=\"pad\",\n    )\n    parser.add_argument(\n        \"--interpolation\",\n        type=str,\n        choices=[\"nearest\", \"nearest-exact\", \"bilinear\", \"bicubic\"],\n        default=\"nearest\",\n    )\n    parser.add_argument(\n        \"--normalization_strategy\",\n        type=str,\n        choices=[\n            \"min-max\",\n            \"quantile\",\n            \"per-sample-min-max\",\n            \"per-sample-quantile\",\n            \"z-score\",\n        ],\n        default=\"quantile\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=42,\n    )\n    parser.add_argument(\n        \"--fill_missing_pixels_with_torch_nan\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--mask_using_qa\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--mask_using_water_mask\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--use_weighted_sampler\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--samples_per_epoch\",\n        type=int,\n        default=10240,\n    )\n    parser.add_argument(\n        \"--has_kelp_importance_factor\",\n        type=float,\n        default=3.0,\n    )\n    parser.add_argument(\n        \"--kelp_pixels_pct_importance_factor\",\n        type=float,\n        default=0.2,\n    )\n    parser.add_argument(\n        \"--qa_ok_importance_factor\",\n        type=float,\n        default=0.0,\n    )\n    parser.add_argument(\n        \"--qa_corrupted_pixels_pct_importance_factor\",\n        type=float,\n        default=-1.0,\n    )\n    parser.add_argument(\n        \"--almost_all_water_importance_factor\",\n        type=float,\n        default=0.5,\n    )\n    parser.add_argument(\n        \"--dem_nan_pixels_pct_importance_factor\",\n        type=float,\n        default=0.25,\n    )\n    parser.add_argument(\n        \"--dem_zero_pixels_pct_importance_factor\",\n        type=float,\n        default=-1.0,\n    )\n    parser.add_argument(\n        \"--spectral_indices\",\n        type=str,\n        help=\"A comma separated list of spectral indices to append to the samples during training\",\n    )\n    parser.add_argument(\n        \"--bands\",\n        type=str,\n        help=\"A comma separated list of band names to reorder. Use it to shift input data channels. \"\n        f\"Must be a subset of {consts.data.ORIGINAL_BANDS} if specified.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--architecture\",\n        type=str,\n        required=True,\n        choices=[\n            \"deeplabv3\",\n            \"deeplabv3+\",\n            \"efficientunet++\",\n            \"fcn\",\n            \"fpn\",\n            \"linknet\",\n            \"manet\",\n            \"pan\",\n            \"pspnet\",\n            \"resunet\",\n            \"resunet++\",\n            \"unet\",\n            \"unet++\",\n        ],\n    )\n    parser.add_argument(\n        \"--encoder\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--pretrained\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--encoder_weights\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--decoder_channels\",\n        type=str,\n        default=\"256,128,64,32,16\",\n    )\n    parser.add_argument(\n        \"--decoder_attention_type\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--plot_n_batches\",\n        type=str,\n        default=3,\n    )\n    parser.add_argument(\n        \"--objective\",\n        type=str,\n        choices=[\"binary\", \"multiclass\"],\n        default=\"binary\",\n    )\n    parser.add_argument(\n        \"--optimizer\",\n        type=str,\n        choices=[\"adam\", \"adamw\", \"sgd\"],\n        default=\"adamw\",\n    )\n    parser.add_argument(\n        \"--weight_decay\",\n        type=float,\n        default=1e-4,\n    )\n    parser.add_argument(\n        \"--lr_scheduler\",\n        type=str,\n        choices=[\"onecycle\", \"cosine\", \"cosine_with_warm_restarts\", \"cyclic\", \"reduce_lr_on_plateau\", \"none\"],\n    )\n    parser.add_argument(\n        \"--lr\",\n        type=float,\n        default=3e-4,\n    )\n    parser.add_argument(\n        \"--onecycle_pct_start\",\n        type=float,\n        default=0.1,\n    )\n    parser.add_argument(\n        \"--onecycle_div_factor\",\n        type=float,\n        default=2.0,\n    )\n    parser.add_argument(\n        \"--onecycle_final_div_factor\",\n        type=float,\n        default=1e2,\n    )\n    parser.add_argument(\n        \"--cyclic_base_lr\",\n        type=float,\n        default=1e-5,\n    )\n    parser.add_argument(\n        \"--cyclic_mode\",\n        type=str,\n        choices=[\"triangular\", \"triangular2\", \"exp_range\"],\n        default=\"exp_range\",\n    )\n    parser.add_argument(\n        \"--cosine_eta_min\",\n        type=float,\n        default=1e-7,\n    )\n    parser.add_argument(\n        \"--cosine_T_mult\",\n        type=int,\n        default=2,\n    )\n    parser.add_argument(\n        \"--reduce_lr_on_plateau_factor\",\n        type=float,\n        default=0.95,\n    )\n    parser.add_argument(\n        \"--reduce_lr_on_plateau_patience\",\n        type=float,\n        default=3,\n    )\n    parser.add_argument(\n        \"--reduce_lr_on_plateau_threshold\",\n        type=float,\n        default=1e-4,\n    )\n    parser.add_argument(\n        \"--reduce_lr_on_plateau_min_lr\",\n        type=float,\n        default=1e-6,\n    )\n    parser.add_argument(\n        \"--tta\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--tta_merge_mode\",\n        type=str,\n        choices=[\"min\", \"max\", \"mean\", \"gmean\", \"sum\", \"tsharpen\"],\n        default=\"max\",\n    )\n    parser.add_argument(\n        \"--decision_threshold\",\n        type=float,\n    )\n    parser.add_argument(\n        \"--loss\",\n        type=str,\n        choices=[\n            \"ce\",\n            \"jaccard\",\n            \"dice\",\n            \"tversky\",\n            \"focal\",\n            \"lovasz\",\n            \"soft_ce\",\n            \"xedice\",\n            \"focal_tversky\",\n            \"log_cosh_dice\",\n            \"hausdorff\",\n            \"t_loss\",\n            \"combo\",\n            \"exp_log_loss\",\n            \"soft_dice\",\n            \"batch_soft_dice\",\n        ],\n        default=\"dice\",\n    )\n    parser.add_argument(\n        \"--monitor_metric\",\n        type=str,\n        default=\"val/dice\",\n    )\n    parser.add_argument(\n        \"--ce_smooth_factor\",\n        type=float,\n        default=0.1,\n    )\n    parser.add_argument(\n        \"--ce_class_weights\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--monitor_mode\",\n        type=str,\n        default=\"max\",\n    )\n    parser.add_argument(\n        \"--ort\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--compile\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--compile_dynamic\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--compile_mode\",\n        type=str,\n        choices=[\"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"],\n        default=\"default\",\n    )\n    parser.add_argument(\n        \"--save_top_k\",\n        type=int,\n        default=1,\n    )\n    parser.add_argument(\n        \"--early_stopping_patience\",\n        type=int,\n        default=10,\n    )\n    parser.add_argument(\n        \"--swa\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--swa_epoch_start\",\n        type=float,\n        default=0.5,\n    )\n    parser.add_argument(\n        \"--swa_annealing_epochs\",\n        type=int,\n        default=10,\n    )\n    parser.add_argument(\n        \"--swa_lr\",\n        type=float,\n        default=3e-5,\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        choices=[\n            \"16-true\",\n            \"16-mixed\",\n            \"bf16-true\",\n            \"bf16-mixed\",\n            \"32-true\",\n        ],\n        default=\"16-mixed\",\n    )\n    parser.add_argument(\n        \"--fast_dev_run\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    parser.add_argument(\n        \"--epochs\",\n        type=int,\n        default=10,\n    )\n    parser.add_argument(\n        \"--limit_train_batches\",\n        type=float,\n    )\n    parser.add_argument(\n        \"--limit_val_batches\",\n        type=float,\n    )\n    parser.add_argument(\n        \"--limit_test_batches\",\n        type=float,\n    )\n    parser.add_argument(\n        \"--log_every_n_steps\",\n        type=int,\n        default=50,\n    )\n    parser.add_argument(\n        \"--accumulate_grad_batches\",\n        type=int,\n        default=1,\n    )\n    parser.add_argument(\n        \"--val_check_interval\",\n        type=float,\n    )\n    parser.add_argument(\n        \"--benchmark\",\n        choices=[\"True\", \"False\"],\n        type=str,\n        default=\"False\",\n    )\n    args = parser.parse_args()\n    cfg = TrainConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/nn/training/train/","title":"train","text":"<p>The segmentation neural network training logic.</p>"},{"location":"api_ref/nn/training/train/#kelp.nn.training.train.main","title":"<code>kelp.nn.training.train.main</code>","text":"<p>Main entrypoint for model training.</p> Source code in <code>kelp/nn/training/train.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for model training.\"\"\"\n    cfg = parse_args()\n    set_gpu_power_limit_if_needed()\n\n    mlflow.set_experiment(cfg.resolved_experiment_name)\n    mlflow.pytorch.autolog()\n    run = mlflow.start_run(run_id=cfg.run_id_from_context)\n\n    with run:\n        pl.seed_everything(cfg.seed, workers=True)\n        mlflow.log_dict(cfg.model_dump(mode=\"json\"), artifact_file=\"config.yaml\")\n        mlflow.log_params(cfg.model_dump(mode=\"json\"))\n        mlflow_run_dir = get_mlflow_run_dir(current_run=run, output_dir=cfg.output_dir)\n        datamodule = KelpForestDataModule.from_metadata_file(**cfg.data_module_kwargs)\n        segmentation_task = KelpForestSegmentationTask(in_channels=datamodule.in_channels, **cfg.model_kwargs)\n        trainer = pl.Trainer(\n            logger=make_loggers(\n                experiment=cfg.resolved_experiment_name,\n                tags=cfg.tags,\n            ),\n            callbacks=make_callbacks(\n                output_dir=mlflow_run_dir / \"artifacts\" / \"checkpoints\",\n                **cfg.callbacks_kwargs,\n            ),\n            **cfg.trainer_kwargs,\n        )\n        trainer.fit(model=segmentation_task, datamodule=datamodule)\n\n        # Don't log hp_metric if debugging\n        if not cfg.fast_dev_run:\n            best_score = (\n                trainer.checkpoint_callback.best_model_score.detach().cpu().item()  # type: ignore[attr-defined]\n            )\n            trainer.logger.log_metrics(metrics={\"hp_metric\": best_score})\n\n        trainer.test(model=segmentation_task, datamodule=datamodule)\n</code></pre>"},{"location":"api_ref/nn/training/train/#kelp.nn.training.train.make_callbacks","title":"<code>kelp.nn.training.train.make_callbacks</code>","text":"<p>A factory method for creating lightning callbacks.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required <code>early_stopping_patience</code> <code>int</code> <p>The early stopping patience in epochs.</p> <code>3</code> <code>save_top_k</code> <code>int</code> <p>The number of top model checkpoints to save.</p> <code>1</code> <code>monitor_metric</code> <code>str</code> <p>The metric to monitor for early stopping.</p> <code>'val/dice'</code> <code>monitor_mode</code> <code>str</code> <p>The mode to monitor for early stopping.</p> <code>'max'</code> <code>swa</code> <code>bool</code> <p>A flag indicating whether to use SWA (Stochastic Weight Averaging).</p> <code>False</code> <code>swa_lr</code> <code>float</code> <p>The final learning rate for SWA annealing.</p> <code>3e-05</code> <code>swa_epoch_start</code> <code>float</code> <p>The percentage of all training epochs when to start the SWA.</p> <code>0.5</code> <code>swa_annealing_epochs</code> <code>int</code> <p>The number of epochs to run the SWA for.</p> <code>10</code> Source code in <code>kelp/nn/training/train.py</code> <pre><code>def make_callbacks(\n    output_dir: Path,\n    early_stopping_patience: int = 3,\n    save_top_k: int = 1,\n    monitor_metric: str = \"val/dice\",\n    monitor_mode: str = \"max\",\n    swa: bool = False,\n    swa_lr: float = 3e-5,\n    swa_epoch_start: float = 0.5,\n    swa_annealing_epochs: int = 10,\n) -&gt; List[Callback]:\n    \"\"\"\n    A factory method for creating lightning callbacks.\n\n    Args:\n        output_dir: The output directory.\n        early_stopping_patience: The early stopping patience in epochs.\n        save_top_k: The number of top model checkpoints to save.\n        monitor_metric: The metric to monitor for early stopping.\n        monitor_mode: The mode to monitor for early stopping.\n        swa: A flag indicating whether to use SWA (Stochastic Weight Averaging).\n        swa_lr: The final learning rate for SWA annealing.\n        swa_epoch_start: The percentage of all training epochs when to start the SWA.\n        swa_annealing_epochs: The number of epochs to run the SWA for.\n\n    Returns: A list of lightning callbacks.\n\n    \"\"\"\n    early_stopping = EarlyStopping(\n        monitor=monitor_metric,\n        patience=early_stopping_patience,\n        verbose=True,\n        mode=monitor_mode,\n    )\n    lr_monitor = LearningRateMonitor(logging_interval=\"step\", log_momentum=True, log_weight_decay=True)\n    sanitized_monitor_metric = monitor_metric.replace(\"/\", \"_\")\n    filename_str = \"kelp-epoch={epoch:02d}-\" f\"{sanitized_monitor_metric}=\" f\"{{{monitor_metric}:.3f}}\"\n    checkpoint = ModelCheckpoint(\n        monitor=monitor_metric,\n        mode=monitor_mode,\n        verbose=True,\n        save_top_k=save_top_k,\n        dirpath=output_dir,\n        auto_insert_metric_name=False,\n        filename=filename_str,\n        save_last=True,\n    )\n    callbacks = [early_stopping, lr_monitor, checkpoint]\n    if swa:\n        callbacks.append(\n            StochasticWeightAveraging(\n                swa_lrs=swa_lr,\n                swa_epoch_start=swa_epoch_start,\n                annealing_epochs=swa_annealing_epochs,\n            ),\n        )\n    return callbacks\n</code></pre>"},{"location":"api_ref/nn/training/train/#kelp.nn.training.train.make_loggers","title":"<code>kelp.nn.training.train.make_loggers</code>","text":"<p>Factory method for creating lightning loggers.</p> <p>Parameters:</p> Name Type Description Default <code>experiment</code> <code>str</code> <p>The experiment name.</p> required <code>tags</code> <code>Dict[str, Any]</code> <p>The experiment tags.</p> required Source code in <code>kelp/nn/training/train.py</code> <pre><code>def make_loggers(\n    experiment: str,\n    tags: Dict[str, Any],\n) -&gt; List[Logger]:\n    \"\"\"\n    Factory method for creating lightning loggers.\n\n    Args:\n        experiment: The experiment name.\n        tags: The experiment tags.\n\n    Returns: List of lightning loggers.\n\n    \"\"\"\n    mlflow_logger = MLFlowLogger(\n        experiment_name=experiment,\n        run_id=mlflow.active_run().info.run_id,\n        log_model=True,\n        tags=tags,\n    )\n    return [mlflow_logger]\n</code></pre>"},{"location":"api_ref/xgb/inference/predict/","title":"predict","text":"<p>Single model prediction logic.</p>"},{"location":"api_ref/xgb/inference/predict/#kelp.xgb.inference.predict.PredictConfig","title":"<code>kelp.xgb.inference.predict.PredictConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>XGBoost prediction config</p> Source code in <code>kelp/xgb/inference/predict.py</code> <pre><code>class PredictConfig(ConfigBase):\n    \"\"\"XGBoost prediction config\"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n\n    data_dir: Path\n    original_training_config_fp: Path\n    model_path: Path\n    run_dir: Path\n    output_dir: Path\n\n    @model_validator(mode=\"before\")\n    def validate_inputs(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        run_dir = Path(data[\"run_dir\"])\n        if (run_dir / \"model\").exists():\n            artifacts_dir = run_dir\n        elif (run_dir / \"artifacts\").exists():\n            artifacts_dir = run_dir / \"artifacts\"\n        else:\n            raise ValueError(\"Could not find nor model dir nor artifacts folder in the specified run_dir\")\n        model_checkpoint = artifacts_dir / \"model\"\n        config_fp = artifacts_dir / \"config.yaml\"\n        data[\"model_path\"] = model_checkpoint\n        data[\"original_training_config_fp\"] = config_fp\n        return data\n\n    @property\n    def training_config(self) -&gt; TrainConfig:\n        with open(self.original_training_config_fp, \"r\") as f:\n            cfg = TrainConfig(**yaml.safe_load(f))\n        return cfg\n</code></pre>"},{"location":"api_ref/xgb/inference/predict/#kelp.xgb.inference.predict.build_prediction_arg_parser","title":"<code>kelp.xgb.inference.predict.build_prediction_arg_parser</code>","text":"<p>Builds the base parser for prediction steps.</p> <p>Returns: An instance of :class:<code>argparse.ArgumentParser</code>.</p> Source code in <code>kelp/xgb/inference/predict.py</code> <pre><code>def build_prediction_arg_parser() -&gt; argparse.ArgumentParser:\n    \"\"\"\n    Builds the base parser for prediction steps.\n\n    Returns: An instance of :class:`argparse.ArgumentParser`.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    parser.add_argument(\"--run_dir\", type=str, required=True)\n    return parser\n</code></pre>"},{"location":"api_ref/xgb/inference/predict/#kelp.xgb.inference.predict.main","title":"<code>kelp.xgb.inference.predict.main</code>","text":"<p>Main entrypoint for running XGBoost inference.</p> Source code in <code>kelp/xgb/inference/predict.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for running XGBoost inference.\"\"\"\n    cfg = parse_args()\n    run_prediction(\n        data_dir=cfg.data_dir,\n        output_dir=cfg.output_dir,\n        model_dir=cfg.model_path,\n        spectral_indices=cfg.training_config.spectral_indices,\n    )\n</code></pre>"},{"location":"api_ref/xgb/inference/predict/#kelp.xgb.inference.predict.parse_args","title":"<code>kelp.xgb.inference.predict.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of PredictConfig.</p> Source code in <code>kelp/xgb/inference/predict.py</code> <pre><code>def parse_args() -&gt; PredictConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of PredictConfig.\n\n    \"\"\"\n    parser = build_prediction_arg_parser()\n    args = parser.parse_args()\n    cfg = PredictConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/xgb/inference/predict/#kelp.xgb.inference.predict.predict","title":"<code>kelp.xgb.inference.predict.predict</code>","text":"<p>Runs XGBoost prediction on files in the specified input directory.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>Path</code> <p>The input directory.</p> required <code>model</code> <code>XGBClassifier</code> <p>The XGBoost model.</p> required <code>spectral_indices</code> <code>List[str]</code> <p>The list of spectral indices to append to the input image.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required Source code in <code>kelp/xgb/inference/predict.py</code> <pre><code>def predict(input_dir: Path, model: XGBClassifier, spectral_indices: List[str], output_dir: Path) -&gt; None:\n    \"\"\"\n    Runs XGBoost prediction on files in the specified input directory.\n\n    Args:\n        input_dir: The input directory.\n        model: The XGBoost model.\n        spectral_indices: The list of spectral indices to append to the input image.\n        output_dir: The output directory.\n\n    \"\"\"\n    fps = sorted(list(input_dir.glob(\"*.tif\")))\n    transforms = build_append_index_transforms(spectral_indices)\n    for fp in tqdm(fps, \"Predicting\"):\n        tile_id = fp.name.split(\"_\")[0]\n        with rasterio.open(fp) as src:\n            input_arr = src.read()\n        prediction = predict_on_single_image(\n            model=model,\n            x=input_arr,\n            transforms=transforms,\n            columns=list(consts.data.ORIGINAL_BANDS) + spectral_indices,\n        )\n        dest: DatasetWriter\n        with rasterio.open(output_dir / f\"{tile_id}_kelp.tif\", \"w\", **META) as dest:\n            dest.write(prediction, 1)\n</code></pre>"},{"location":"api_ref/xgb/inference/predict/#kelp.xgb.inference.predict.predict_on_single_image","title":"<code>kelp.xgb.inference.predict.predict_on_single_image</code>","text":"<p>Runs inference on a single satellite image using specified XGBoost model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>XGBClassifier</code> <p>The XGBoost model.</p> required <code>x</code> <code>ndarray</code> <p>The array representing the satellite image.</p> required <code>transforms</code> <code>Callable[[Tensor], Tensor]</code> <p>A set of transforms to apply to the input image.</p> required <code>columns</code> <code>List[str]</code> <p>The column names for the input array.</p> required <code>decision_threshold</code> <code>float</code> <p>The decision threshold.</p> <code>0.5</code> Source code in <code>kelp/xgb/inference/predict.py</code> <pre><code>@torch.inference_mode()\ndef predict_on_single_image(\n    model: XGBClassifier,\n    x: np.ndarray,  # type: ignore[type-arg]\n    transforms: Callable[[Tensor], Tensor],\n    columns: List[str],\n    decision_threshold: float = 0.5,\n) -&gt; np.ndarray:  # type: ignore[type-arg]\n    \"\"\"\n    Runs inference on a single satellite image using specified XGBoost model.\n\n    Args:\n        model: The XGBoost model.\n        x: The array representing the satellite image.\n        transforms: A set of transforms to apply to the input image.\n        columns: The column names for the input array.\n        decision_threshold: The decision threshold.\n\n    Returns: A numpy array with predicted mask.\n\n    \"\"\"\n    tensor = torch.tensor(x, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n    tensor = torch.flatten(transforms(tensor), start_dim=2).squeeze().T\n    df = pd.DataFrame(tensor.detach().cpu().numpy(), columns=columns).replace({np.nan: -32768.0})\n    prediction = model.predict_proba(df)\n    prediction = np.where(prediction[:, 1] &gt;= decision_threshold, 1, 0)\n    prediction = prediction.reshape(x.shape[1], x.shape[2])\n    return prediction  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/xgb/inference/predict/#kelp.xgb.inference.predict.run_prediction","title":"<code>kelp.xgb.inference.predict.run_prediction</code>","text":"<p>Runs the XGBoost inference on specified data directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The data directory.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required <code>model_dir</code> <code>Path</code> <p>The model directory.</p> required <code>spectral_indices</code> <code>List[str]</code> <p>The spectral indices to append to the input image.</p> required Source code in <code>kelp/xgb/inference/predict.py</code> <pre><code>def run_prediction(\n    data_dir: Path,\n    output_dir: Path,\n    model_dir: Path,\n    spectral_indices: List[str],\n) -&gt; None:\n    \"\"\"\n    Runs the XGBoost inference on specified data directory.\n\n    Args:\n        data_dir: The data directory.\n        output_dir: The output directory.\n        model_dir: The model directory.\n        spectral_indices: The spectral indices to append to the input image.\n\n    \"\"\"\n    model = load_model(model_path=model_dir)\n    predict(\n        input_dir=data_dir,\n        model=model,\n        spectral_indices=spectral_indices,\n        output_dir=output_dir,\n    )\n</code></pre>"},{"location":"api_ref/xgb/inference/predict_and_submit/","title":"predict_and_submit","text":"<p>Logic for predicting and creating submission file for a single model.</p>"},{"location":"api_ref/xgb/inference/predict_and_submit/#kelp.xgb.inference.predict_and_submit.copy_run_artifacts","title":"<code>kelp.xgb.inference.predict_and_submit.copy_run_artifacts</code>","text":"<p>Copies run artifacts from run_dir to output_dir.</p> <p>Parameters:</p> Name Type Description Default <code>run_dir</code> <code>Path</code> <p>The directory to copy run artifacts from.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory.</p> required Source code in <code>kelp/xgb/inference/predict_and_submit.py</code> <pre><code>def copy_run_artifacts(run_dir: Path, output_dir: Path) -&gt; None:\n    \"\"\"\n    Copies run artifacts from run_dir to output_dir.\n\n    Args:\n        run_dir: The directory to copy run artifacts from.\n        output_dir: The output directory.\n\n    \"\"\"\n    shutil.copytree(run_dir, output_dir / run_dir.name, dirs_exist_ok=True)\n</code></pre>"},{"location":"api_ref/xgb/inference/predict_and_submit/#kelp.xgb.inference.predict_and_submit.main","title":"<code>kelp.xgb.inference.predict_and_submit.main</code>","text":"<p>Main entrypoint for running XGBoost model predictions and creating a submission file.</p> Source code in <code>kelp/xgb/inference/predict_and_submit.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for running XGBoost model predictions and creating a submission file.\"\"\"\n    cfg = parse_args()\n    now = datetime.utcnow().isoformat()\n    out_dir = cfg.output_dir / now\n    preds_dir = cfg.output_dir / now / \"predictions\"\n    preds_dir.mkdir(exist_ok=False, parents=True)\n    (out_dir / \"predict_config.yaml\").write_text(yaml.dump(cfg.model_dump(mode=\"json\")))\n    run_prediction(\n        data_dir=cfg.data_dir,\n        output_dir=preds_dir,\n        model_dir=cfg.model_path,\n        spectral_indices=cfg.training_config.spectral_indices,\n    )\n    create_submission_tar(\n        preds_dir=preds_dir,\n        output_dir=out_dir,\n    )\n    copy_run_artifacts(\n        run_dir=cfg.run_dir,  # type: ignore[arg-type]\n        output_dir=out_dir,\n    )\n    if cfg.preview_submission:\n        plot_first_n_samples(\n            data_dir=cfg.data_dir,\n            submission_dir=out_dir,\n            output_dir=out_dir / \"previews\",\n            n=cfg.preview_first_n,\n        )\n</code></pre>"},{"location":"api_ref/xgb/inference/predict_and_submit/#kelp.xgb.inference.predict_and_submit.parse_args","title":"<code>kelp.xgb.inference.predict_and_submit.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of :class:<code>PredictAndSubmitConfig</code>.</p> Source code in <code>kelp/xgb/inference/predict_and_submit.py</code> <pre><code>def parse_args() -&gt; PredictAndSubmitConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of :class:`PredictAndSubmitConfig`.\n\n    \"\"\"\n    parser = build_prediction_arg_parser()\n    parser.add_argument(\"--preview_submission\", action=\"store_true\")\n    parser.add_argument(\"--submission_preview_output_dir\", type=str)\n    parser.add_argument(\"--preview_first_n\", type=int, default=10)\n    args = parser.parse_args()\n    cfg = PredictAndSubmitConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/xgb/training/config/","title":"config","text":"<p>The XGBoost training config.</p>"},{"location":"api_ref/xgb/training/config/#kelp.xgb.training.cfg.TrainConfig","title":"<code>kelp.xgb.training.cfg.TrainConfig</code>","text":"<p>             Bases: <code>ConfigBase</code></p> <p>The XGBoost training configuration.</p> Source code in <code>kelp/xgb/training/cfg.py</code> <pre><code>class TrainConfig(ConfigBase):\n    \"\"\"The XGBoost training configuration.\"\"\"\n\n    dataset_fp: Path\n    train_data_dir: Path\n    output_dir: Path\n    spectral_indices: List[str]\n    sample_size: float = 1.0\n    seed: int = consts.reproducibility.SEED\n    plot_n_samples: int = 10\n    experiment: str = \"train-xgb-clf-exp\"\n    explain_model: bool = False\n\n    xgb_learning_rate: float = 0.1\n    xgb_n_estimators: int = 1000\n\n    @field_validator(\"spectral_indices\", mode=\"before\")\n    def validate_spectral_indices(cls, value: Union[str, Optional[List[str]]] = None) -&gt; List[str]:\n        if not value:\n            return [\"DEMWM\", \"NDVI\"]\n\n        if value == \"all\" or value == ALL_INDICES:\n            return ALL_INDICES\n\n        indices = value if isinstance(value, list) else [index.strip() for index in value.split(\",\")]\n\n        if \"DEMWM\" in indices:\n            _logger.warning(\"DEMWM is automatically added during training. No need to add it twice.\")\n            indices.remove(\"DEMWM\")\n\n        if \"NDVI\" in indices:\n            _logger.warning(\"NDVI is automatically added during training. No need to add it twice.\")\n            indices.remove(\"NDVI\")\n\n        unknown_indices = set(indices).difference(list(SPECTRAL_INDEX_LOOKUP.keys()))\n        if unknown_indices:\n            raise ValueError(\n                f\"Unknown spectral indices were provided: {', '.join(unknown_indices)}. \"\n                f\"Please provide comma separated indices. Valid choices are: {', '.join(SPECTRAL_INDEX_LOOKUP.keys())}.\"\n            )\n\n        return [\"DEMWM\", \"NDVI\"] + indices\n\n    @property\n    def resolved_experiment_name(self) -&gt; str:\n        return os.environ.get(\"MLFLOW_EXPERIMENT_NAME\", self.experiment)\n\n    @property\n    def run_id_from_context(self) -&gt; Optional[str]:\n        return os.environ.get(\"MLFLOW_RUN_ID\", None)\n\n    @property\n    def tags(self) -&gt; Dict[str, Any]:\n        return {\"trained_at\": datetime.utcnow().isoformat()}\n\n    @property\n    def columns_to_load(self) -&gt; List[str]:\n        return self.model_input_columns + [\"label\", \"tile_id\", \"split\"]\n\n    @property\n    def model_input_columns(self) -&gt; List[str]:\n        return list(consts.data.ORIGINAL_BANDS) + self.spectral_indices\n\n    @property\n    def random_forest_model_params(self) -&gt; Dict[str, Any]:\n        return {\n            \"random_state\": self.seed,\n        }\n\n    @property\n    def gradient_boosting_tree_model_params(self) -&gt; Dict[str, Any]:\n        return {\n            \"random_state\": self.seed,\n        }\n\n    @property\n    def catboost_model_params(self) -&gt; Dict[str, Any]:\n        return {\n            \"random_state\": self.seed,\n        }\n\n    @property\n    def xgboost_model_params(self) -&gt; Dict[str, Any]:\n        return {\n            \"n_estimators\": self.xgb_n_estimators,\n            \"learning_rate\": self.xgb_learning_rate,\n            \"random_state\": self.seed,\n            \"device\": \"cuda\",\n            \"n_jobs\": -1,\n        }\n\n    @property\n    def lightgbm_model_params(self) -&gt; Dict[str, Any]:\n        return {\n            \"random_state\": self.seed,\n        }\n</code></pre>"},{"location":"api_ref/xgb/training/eval/","title":"eval","text":"<p>The XGBoost evaluation logic.</p>"},{"location":"api_ref/xgb/training/eval/#kelp.xgb.training.eval.EvalConfig","title":"<code>kelp.xgb.training.eval.EvalConfig</code>","text":"<p>             Bases: <code>PredictConfig</code></p> <p>The config for running XGBoost model evaluation.</p> Source code in <code>kelp/xgb/training/eval.py</code> <pre><code>class EvalConfig(PredictConfig):\n    \"\"\"The config for running XGBoost model evaluation.\"\"\"\n\n    metadata_fp: Path\n    eval_split: int = 8\n    experiment_name: str = \"model-eval-exp\"\n    decision_threshold: float = 0.5\n</code></pre>"},{"location":"api_ref/xgb/training/eval/#kelp.xgb.training.eval.eval","title":"<code>kelp.xgb.training.eval.eval</code>","text":"<p>Runs evaluation using data from specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>XGBClassifier</code> <p>The XGBoost model.</p> required <code>data_dir</code> <code>Path</code> <p>The data directory.</p> required <code>metadata</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required <code>spectral_indices</code> <code>List[str]</code> <p>The spectral indices to append to the input image.</p> required <code>prefix</code> <code>str</code> <p>The prefix for logged metrics.</p> required <code>decision_threshold</code> <code>float</code> <p>The decision threshold.</p> <code>0.5</code> Source code in <code>kelp/xgb/training/eval.py</code> <pre><code>def eval(\n    model: XGBClassifier,\n    data_dir: Path,\n    metadata: pd.DataFrame,\n    spectral_indices: List[str],\n    prefix: str,\n    decision_threshold: float = 0.5,\n) -&gt; None:\n    \"\"\"\n    Runs evaluation using data from specified directory.\n\n    Args:\n        model: The XGBoost model.\n        data_dir: The data directory.\n        metadata: The metadata dataframe.\n        spectral_indices: The spectral indices to append to the input image.\n        prefix: The prefix for logged metrics.\n        decision_threshold: The decision threshold.\n\n    \"\"\"\n    tile_ids = metadata[\"tile_id\"].tolist()\n    metrics = MetricCollection(\n        metrics={\n            \"dice\": Dice(num_classes=2, average=\"macro\"),\n            \"iou\": JaccardIndex(task=\"binary\"),\n            \"accuracy\": Accuracy(task=\"binary\"),\n            \"recall\": Recall(task=\"binary\", average=\"macro\"),\n            \"precision\": Precision(task=\"binary\", average=\"macro\"),\n            \"f1\": F1Score(task=\"binary\", average=\"macro\"),\n            \"auroc\": AUROC(task=\"binary\"),\n        },\n        prefix=f\"{prefix}/\",\n    ).to(DEVICE)\n    transforms = build_append_index_transforms(spectral_indices)\n\n    for tile in tqdm(tile_ids, desc=\"Running evaluation on images\"):\n        with rasterio.open(data_dir / \"images\" / f\"{tile}_satellite.tif\") as src:\n            input_arr = src.read()\n        with rasterio.open(data_dir / \"masks\" / f\"{tile}_kelp.tif\") as src:\n            y_true = src.read(1)\n        y_pred = predict_on_single_image(\n            model=model,\n            x=input_arr,\n            transforms=transforms,\n            columns=list(consts.data.ORIGINAL_BANDS) + spectral_indices,\n            decision_threshold=decision_threshold,\n        )\n        metrics(\n            torch.tensor(y_pred, device=DEVICE, dtype=torch.int32),\n            torch.tensor(y_true, device=DEVICE, dtype=torch.int32),\n        )\n\n    metrics_dict = metrics.compute()\n\n    for name, value in metrics_dict.items():\n        metrics_dict[name] = value.item()\n\n    mlflow.log_metrics(metrics_dict)\n    _logger.info(f\"{prefix.upper()} metrics: {json.dumps(metrics_dict, indent=4)}\")\n</code></pre>"},{"location":"api_ref/xgb/training/eval/#kelp.xgb.training.eval.main","title":"<code>kelp.xgb.training.eval.main</code>","text":"<p>Main entry point for running XGBoost model evaluation.</p> Source code in <code>kelp/xgb/training/eval.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for running XGBoost model evaluation.\"\"\"\n    cfg = parse_args()\n    metadata = pd.read_parquet(cfg.metadata_fp)\n    metadata = metadata[metadata[f\"split_{cfg.eval_split}\"] == \"val\"]\n    run_eval(\n        run_dir=cfg.run_dir,\n        data_dir=cfg.data_dir,\n        metadata=metadata,\n        model_dir=cfg.model_path,\n        train_cfg=cfg.training_config,\n        experiment_name=cfg.experiment_name,\n        decision_threshold=cfg.decision_threshold,\n    )\n</code></pre>"},{"location":"api_ref/xgb/training/eval/#kelp.xgb.training.eval.parse_args","title":"<code>kelp.xgb.training.eval.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of :class:<code>EvalConfig</code>.</p> Source code in <code>kelp/xgb/training/eval.py</code> <pre><code>def parse_args() -&gt; EvalConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of :class:`EvalConfig`.\n\n    \"\"\"\n    parser = build_prediction_arg_parser()\n    parser.add_argument(\"--metadata_fp\", type=str, required=True)\n    parser.add_argument(\"--eval_split\", type=int, default=8)\n    parser.add_argument(\"--experiment_name\", type=str, default=\"model-eval-exp\")\n    parser.add_argument(\"--decision_threshold\", type=float, default=0.5)\n    args = parser.parse_args()\n    cfg = EvalConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/xgb/training/eval/#kelp.xgb.training.eval.run_eval","title":"<code>kelp.xgb.training.eval.run_eval</code>","text":"<p>Runs XGBoost model evaluation and logs metrics to MLFlow.</p> <p>Parameters:</p> Name Type Description Default <code>run_dir</code> <code>Path</code> <p>The run directory.</p> required <code>data_dir</code> <code>Path</code> <p>The data directory.</p> required <code>metadata</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required <code>model_dir</code> <code>Path</code> <p>The model directory to.</p> required <code>train_cfg</code> <code>TrainConfig</code> <p>The original training configuration.</p> required <code>experiment_name</code> <code>str</code> <p>The experiment name.</p> required <code>decision_threshold</code> <code>float</code> <p>The decision threshold.</p> <code>0.5</code> Source code in <code>kelp/xgb/training/eval.py</code> <pre><code>def run_eval(\n    run_dir: Path,\n    data_dir: Path,\n    metadata: pd.DataFrame,\n    model_dir: Path,\n    train_cfg: TrainConfig,\n    experiment_name: str,\n    decision_threshold: float = 0.5,\n) -&gt; None:\n    \"\"\"\n    Runs XGBoost model evaluation and logs metrics to MLFlow.\n\n    Args:\n        run_dir: The run directory.\n        data_dir: The data directory.\n        metadata: The metadata dataframe.\n        model_dir: The model directory to.\n        train_cfg: The original training configuration.\n        experiment_name: The experiment name.\n        decision_threshold: The decision threshold.\n\n    \"\"\"\n    mlflow.set_experiment(experiment_name)\n    mlflow.pytorch.autolog()\n    run = mlflow.start_run(run_name=run_dir.parts[-1])\n\n    with run:\n        mlflow.log_dict(train_cfg.model_dump(mode=\"json\"), artifact_file=\"config.yaml\")\n        mlflow.log_params(train_cfg.model_dump(mode=\"json\"))\n        mlflow.log_param(\"decision_threshold\", decision_threshold)\n        mlflow.set_tags(\n            {\n                \"evaluated_at\": datetime.utcnow().isoformat(),\n                \"original_run_id\": run_dir.parts[-1],\n                \"original_experiment_id\": model_dir.parts[-2],\n            }\n        )\n        model = load_model(model_path=model_dir)\n        eval(\n            model=model,\n            metadata=metadata,\n            data_dir=data_dir,\n            spectral_indices=train_cfg.spectral_indices,\n            prefix=\"test\",\n            decision_threshold=decision_threshold,\n        )\n</code></pre>"},{"location":"api_ref/xgb/training/options/","title":"options","text":"<p>The command line argument parsing for XGBoost training.</p>"},{"location":"api_ref/xgb/training/options/#kelp.xgb.training.options.parse_args","title":"<code>kelp.xgb.training.options.parse_args</code>","text":"<p>Parse command line arguments.</p> <p>Returns: An instance of :class:<code>TrainConfig</code>.</p> Source code in <code>kelp/xgb/training/options.py</code> <pre><code>def parse_args() -&gt; TrainConfig:\n    \"\"\"\n    Parse command line arguments.\n\n    Returns: An instance of :class:`TrainConfig`.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--train_data_dir\", type=str, required=True)\n    parser.add_argument(\"--dataset_fp\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    parser.add_argument(\"--spectral_indices\", type=str)\n    parser.add_argument(\"--sample_size\", type=float, default=1.0)\n    parser.add_argument(\"--seed\", type=int, default=consts.reproducibility.SEED)\n    parser.add_argument(\"--plot_n_samples\", type=int, default=10)\n    parser.add_argument(\"--experiment\", type=str, default=\"train-tree-clf-exp\")\n    parser.add_argument(\"--explain_model\", action=\"store_true\")\n    args = parser.parse_args()\n    cfg = TrainConfig(**vars(args))\n    cfg.log_self()\n    cfg.output_dir.mkdir(exist_ok=True, parents=True)\n    return cfg\n</code></pre>"},{"location":"api_ref/xgb/training/train/","title":"train","text":"<p>The XGBoost training logic.</p>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.calculate_metrics","title":"<code>kelp.xgb.training.train.calculate_metrics</code>","text":"<p>Calculates metrics for given model and its predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>XGBClassifier</code> <p>The XGBClassifier model.</p> required <code>x</code> <code>DataFrame</code> <p>The input dataframe.</p> required <code>y_true</code> <code>Series</code> <p>The ground truth series.</p> required <code>y_pred</code> <code>ndarray</code> <p>The prediction series.</p> required <code>prefix</code> <code>str</code> <p>A prefix to use for metrics logging.</p> required Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@torch.inference_mode()\n@timed\ndef calculate_metrics(\n    model: XGBClassifier,\n    x: pd.DataFrame,\n    y_true: pd.Series,\n    y_pred: np.ndarray,  # type: ignore[type-arg]\n    prefix: str,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculates metrics for given model and its predictions.\n\n    Args:\n        model: The XGBClassifier model.\n        x: The input dataframe.\n        y_true: The ground truth series.\n        y_pred: The prediction series.\n        prefix: A prefix to use for metrics logging.\n\n    Returns: A dictionary with metric names and the metric values.\n\n    \"\"\"\n    metrics = MetricCollection(\n        metrics={\n            \"dice\": Dice(num_classes=2, average=\"macro\"),\n            \"iou\": JaccardIndex(task=\"binary\"),\n            \"accuracy\": Accuracy(task=\"binary\"),\n            \"recall\": Recall(task=\"binary\", average=\"macro\"),\n            \"precision\": Precision(task=\"binary\", average=\"macro\"),\n            \"f1\": F1Score(task=\"binary\", average=\"macro\"),\n            \"auroc\": AUROC(task=\"binary\"),\n        },\n        prefix=f\"{prefix}/\",\n    ).to(DEVICE)\n    metrics(\n        torch.tensor(y_pred, device=DEVICE, dtype=torch.int32),\n        torch.tensor(y_true.values, device=DEVICE, dtype=torch.int32),\n    )\n    metrics_dict = metrics.compute()\n    for name, value in metrics_dict.items():\n        metrics_dict[name] = value.item()\n    if hasattr(model, \"predict_proba\"):\n        y_pred_prob = model.predict_proba(x)\n        loss = log_loss(y_true, y_pred_prob)\n        metrics_dict[f\"{prefix}/log_loss\"] = loss\n    _logger.info(f\"{prefix.upper()} metrics: {json.dumps(metrics_dict, indent=4)}\")\n    return metrics_dict  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.eval_model","title":"<code>kelp.xgb.training.train.eval_model</code>","text":"<p>Evaluates the XGBoost model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>XGBClassifier</code> <p>The XGBoost model.</p> required <code>x</code> <code>DataFrame</code> <p>The validation data.</p> required <code>y_true</code> <code>Series</code> <p>The ground truth labels.</p> required <code>prefix</code> <code>str</code> <p>The prefix for the metrics and plots.</p> required <code>seed</code> <code>int</code> <p>The seed for reproducibility.</p> <code>SEED</code> <code>explain_model</code> <code>bool</code> <p>A flag indicating whether to run model feature importance calculation.</p> <code>False</code> Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef eval_model(\n    model: XGBClassifier,\n    x: pd.DataFrame,\n    y_true: pd.Series,\n    prefix: str,\n    seed: int = consts.reproducibility.SEED,\n    explain_model: bool = False,\n) -&gt; None:\n    \"\"\"\n    Evaluates the XGBoost model.\n\n    Args:\n        model: The XGBoost model.\n        x: The validation data.\n        y_true: The ground truth labels.\n        prefix: The prefix for the metrics and plots.\n        seed: The seed for reproducibility.\n        explain_model: A flag indicating whether to run model feature importance calculation.\n\n    \"\"\"\n    _logger.info(f\"Running model eval for {prefix} split\")\n    y_pred = model.predict(x)\n    metrics = calculate_metrics(model=model, x=x, y_true=y_true, y_pred=y_pred, prefix=prefix)\n    mlflow.log_metrics(metrics)\n    log_confusion_matrix(y_true=y_true, y_pred=y_pred, prefix=prefix, normalize=False)\n    log_confusion_matrix(y_true=y_true, y_pred=y_pred, prefix=prefix, normalize=True)\n    log_precision_recall_curve(y_true=y_true, y_pred=y_pred, prefix=prefix)\n    log_roc_curve(y_true=y_true, y_pred=y_pred, prefix=prefix)\n    if prefix == \"test\" and explain_model:  # calculate feature importance only once\n        log_model_feature_importance(model=model, feature_names=x.columns.values)\n        log_permutation_feature_importance(model=model, x=x, y_true=y_true, seed=seed)\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.fit_model","title":"<code>kelp.xgb.training.train.fit_model</code>","text":"<p>Runs the training.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>XGBClassifier</code> <p>The model to be trained.</p> required <code>x</code> <code>DataFrame</code> <p>The training dataset.</p> required <code>y_true</code> <code>Series</code> <p>The training labels.</p> required Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef fit_model(\n    model: XGBClassifier,\n    x: pd.DataFrame,\n    y_true: pd.Series,\n) -&gt; XGBClassifier:\n    \"\"\"\n    Runs the training.\n\n    Args:\n        model: The model to be trained.\n        x: The training dataset.\n        y_true: The training labels.\n\n    Returns: Fitted model.\n\n    \"\"\"\n    model.fit(x, y_true)\n    return model\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.load_data","title":"<code>kelp.xgb.training.train.load_data</code>","text":"<p>Loads the training data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataframe with pixel-level values.</p> required <code>sample_size</code> <code>float</code> <p>The random sample size to use for quicker training times.</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>The seed for reproducibility.</p> <code>SEED</code> Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef load_data(\n    df: pd.DataFrame,\n    sample_size: float = 1.0,\n    seed: int = consts.reproducibility.SEED,\n) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    \"\"\"\n    Loads the training data.\n\n    Args:\n        df: The input dataframe with pixel-level values.\n        sample_size: The random sample size to use for quicker training times.\n        seed: The seed for reproducibility.\n\n    Returns: A tuple containing features and labels in following order: X_train, y_train, X_val, y_val, X_test, y_test\n\n    \"\"\"\n    X_train = df[df[\"split\"] == \"train\"]\n    X_val = df[df[\"split\"] == \"val\"]\n    X_test = df[df[\"split\"] == \"test\"]\n\n    if sample_size != 1.0:\n        X_train = X_train.sample(frac=sample_size, random_state=seed)\n\n    y_train = X_train[\"label\"]\n    y_val = X_val[\"label\"]\n    y_test = X_test[\"label\"]\n\n    X_train = X_train.drop([\"label\", \"split\"], axis=1)\n    X_val = X_val.drop([\"label\", \"split\"], axis=1)\n    X_test = X_test.drop([\"label\", \"split\"], axis=1)\n\n    return X_train, y_train, X_val, y_val, X_test, y_test\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.log_confusion_matrix","title":"<code>kelp.xgb.training.train.log_confusion_matrix</code>","text":"<p>Logs confusion matrix to MLFlow.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Series</code> <p>A pandas Series with ground truth values.</p> required <code>y_pred</code> <code>ndarray</code> <p>A pandas array with prediction values.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use when logging confusion matrix.</p> required <code>normalize</code> <code>bool</code> <p>A flag indicating whether to normalize the confusion matrix.</p> <code>False</code> Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef log_confusion_matrix(\n    y_true: pd.Series,\n    y_pred: np.ndarray,  # type: ignore[type-arg]\n    prefix: str,\n    normalize: bool = False,\n) -&gt; None:\n    \"\"\"\n    Logs confusion matrix to MLFlow.\n\n    Args:\n        y_true: A pandas Series with ground truth values.\n        y_pred: A pandas array with prediction values.\n        prefix: The prefix to use when logging confusion matrix.\n        normalize: A flag indicating whether to normalize the confusion matrix.\n\n    \"\"\"\n    cmd = ConfusionMatrixDisplay.from_predictions(\n        y_true=y_true,\n        y_pred=y_pred,\n        display_labels=consts.data.CLASSES,\n        cmap=\"Blues\",\n        normalize=\"true\" if normalize else None,\n    )\n    cmd.ax_.set_title(\"Normalized confusion matrix\" if normalize else \"Confusion matrix\")\n    plt.tight_layout()\n    fname = \"normalized_confusion_matrix\" if normalize else \"confusion_matrix\"\n    mlflow.log_figure(figure=cmd.figure_, artifact_file=f\"images/{prefix}/{fname}.png\")\n    plt.close()\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.log_model_feature_importance","title":"<code>kelp.xgb.training.train.log_model_feature_importance</code>","text":"<p>Logs the feature importance to MLFlow.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>XGBClassifier</code> <p>The XGBClassifier model.</p> required <code>feature_names</code> <code>ndarray</code> <p>The names of the features.</p> required Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef log_model_feature_importance(\n    model: XGBClassifier,\n    feature_names: np.ndarray,  # type: ignore[type-arg]\n) -&gt; None:\n    \"\"\"\n    Logs the feature importance to MLFlow.\n\n    Args:\n        model: The XGBClassifier model.\n        feature_names: The names of the features.\n\n    \"\"\"\n    sorted_idx = model.feature_importances_.argsort()\n    fig, ax = plt.subplots(figsize=(8, 0.2 * len(feature_names)))\n    ax.barh(feature_names[sorted_idx], model.feature_importances_[sorted_idx])\n    ax.set_title(\"XGB Feature importances\")\n    ax.set_xlabel(\"Feature\")\n    ax.set_ylabel(\"XGB Feature Importance\")\n    fig.tight_layout()\n    mlflow.log_figure(figure=fig, artifact_file=\"images/feature_importances_xgb.png\")\n    plt.close(fig)\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.log_permutation_feature_importance","title":"<code>kelp.xgb.training.train.log_permutation_feature_importance</code>","text":"<p>Logs the permutation feature importance to MLFlow.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>XGBClassifier</code> <p>The XGBClassifier model.</p> required <code>x</code> <code>DataFrame</code> <p>The input data.</p> required <code>y_true</code> <code>Series</code> <p>The ground truth data.</p> required <code>seed</code> <code>int</code> <p>The seed to use for reproducibility.</p> <code>SEED</code> <code>n_repeats</code> <code>int</code> <p>The number of repeats.</p> <code>10</code> Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef log_permutation_feature_importance(\n    model: XGBClassifier,\n    x: pd.DataFrame,\n    y_true: pd.Series,\n    seed: int = consts.reproducibility.SEED,\n    n_repeats: int = 10,\n) -&gt; None:\n    \"\"\"\n    Logs the permutation feature importance to MLFlow.\n\n    Args:\n        model: The XGBClassifier model.\n        x: The input data.\n        y_true: The ground truth data.\n        seed: The seed to use for reproducibility.\n        n_repeats: The number of repeats.\n\n    \"\"\"\n    result = permutation_importance(model, x, y_true, n_repeats=n_repeats, random_state=seed, n_jobs=4)\n    forest_importances = pd.Series(result.importances_mean, index=x.columns.tolist())\n    fig, ax = plt.subplots(figsize=(0.2 * len(x.columns), 8))\n    forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n    ax.set_title(\"Feature importances using permutation on full model\")\n    ax.set_xlabel(\"Feature\")\n    ax.set_ylabel(\"Mean accuracy decrease\")\n    fig.tight_layout()\n    mlflow.log_figure(figure=fig, artifact_file=\"images/feature_importances_pi.png\")\n    plt.close(fig)\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.log_precision_recall_curve","title":"<code>kelp.xgb.training.train.log_precision_recall_curve</code>","text":"<p>Logs the precision and recall curve plot to MLFlow.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Series</code> <p>The ground truth.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use when logging precision and recall curves.</p> required Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef log_precision_recall_curve(\n    y_true: pd.Series,\n    y_pred: np.ndarray,  # type: ignore[type-arg]\n    prefix: str,\n) -&gt; None:\n    \"\"\"\n    Logs the precision and recall curve plot to MLFlow.\n\n    Args:\n        y_true: The ground truth.\n        y_pred: The predicted values.\n        prefix: The prefix to use when logging precision and recall curves.\n\n    \"\"\"\n    prd = PrecisionRecallDisplay.from_predictions(\n        y_true=y_true,\n        y_pred=y_pred,\n    )\n    prd.ax_.set_title(\"Precision recall curve\")\n    plt.tight_layout()\n    mlflow.log_figure(figure=prd.figure_, artifact_file=f\"images/{prefix}/precision_recall_curve.png\")\n    plt.close()\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.log_roc_curve","title":"<code>kelp.xgb.training.train.log_roc_curve</code>","text":"<p>Logs the ROC curve to MLFlow.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Series</code> <p>The ground truth.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use when logging ROC curve plot.</p> required Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef log_roc_curve(\n    y_true: pd.Series,\n    y_pred: np.ndarray,  # type: ignore[type-arg]\n    prefix: str,\n) -&gt; None:\n    \"\"\"\n    Logs the ROC curve to MLFlow.\n\n    Args:\n        y_true: The ground truth.\n        y_pred: The predicted values.\n        prefix: The prefix to use when logging ROC curve plot.\n\n    \"\"\"\n    rc = RocCurveDisplay.from_predictions(\n        y_true=y_true,\n        y_pred=y_pred,\n    )\n    rc.ax_.set_title(\"ROC curve\")\n    plt.tight_layout()\n    mlflow.log_figure(figure=rc.figure_, artifact_file=f\"images/{prefix}/roc_curve.png\")\n    plt.close()\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.log_sample_predictions","title":"<code>kelp.xgb.training.train.log_sample_predictions</code>","text":"<p>Logs sample predictions to MLFlow.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_dir</code> <code>Path</code> <p>The training data directory.</p> required <code>metadata</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required <code>model</code> <code>XGBClassifier</code> <p>The XGBClassifier model.</p> required <code>spectral_indices</code> <code>List[str]</code> <p>The spectral indices to append to the input image before prediction.</p> required <code>sample_size</code> <code>int</code> <p>The number of samples to plot.</p> <code>10</code> <code>seed</code> <code>int</code> <p>The seed for reproducibility.</p> <code>SEED</code> Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef log_sample_predictions(\n    train_data_dir: Path,\n    metadata: pd.DataFrame,\n    model: XGBClassifier,\n    spectral_indices: List[str],\n    sample_size: int = 10,\n    seed: int = consts.reproducibility.SEED,\n) -&gt; None:\n    \"\"\"\n    Logs sample predictions to MLFlow.\n\n    Args:\n        train_data_dir: The training data directory.\n        metadata: The metadata dataframe.\n        model: The XGBClassifier model.\n        spectral_indices: The spectral indices to append to the input image before prediction.\n        sample_size: The number of samples to plot.\n        seed: The seed for reproducibility.\n\n    \"\"\"\n    sample_to_plot = metadata.sample(n=sample_size, random_state=seed)\n    tile_ids = sample_to_plot[\"tile_id\"].tolist()\n    transforms = build_append_index_transforms(spectral_indices)\n    for tile in tqdm(tile_ids, desc=\"Plotting sample predictions\"):\n        with rasterio.open(train_data_dir / \"images\" / f\"{tile}_satellite.tif\") as src:\n            input_arr = src.read()\n        with rasterio.open(train_data_dir / \"masks\" / f\"{tile}_kelp.tif\") as src:\n            mask_arr = src.read(1)\n        prediction = predict_on_single_image(\n            model=model, x=input_arr, transforms=transforms, columns=list(consts.data.ORIGINAL_BANDS) + spectral_indices\n        )\n        input_arr = min_max_normalize(input_arr)\n        fig = plot_sample(input_arr=input_arr, target_arr=mask_arr, predictions_arr=prediction, suptitle=tile)\n        mlflow.log_figure(fig, artifact_file=f\"images/predictions/{tile}.png\")\n        plt.close(fig)\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.main","title":"<code>kelp.xgb.training.train.main</code>","text":"<p>Main entrypoint for training XGBClassifier.</p> Source code in <code>kelp/xgb/training/train.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint for training XGBClassifier.\"\"\"\n    cfg = parse_args()\n    mlflow.xgboost.autolog(model_format=\"json\")\n    mlflow.set_experiment(cfg.resolved_experiment_name)\n    run = mlflow.start_run(run_id=cfg.run_id_from_context)\n    with run:\n        mlflow.log_dict(cfg.model_dump(mode=\"json\"), artifact_file=\"config.yaml\")\n        mlflow.log_params(cfg.model_dump(mode=\"json\"))\n        _ = get_mlflow_run_dir(current_run=run, output_dir=cfg.output_dir)\n        model = XGBClassifier(**cfg.xgboost_model_params)\n        run_training(\n            train_data_dir=cfg.train_data_dir,\n            dataset_fp=cfg.dataset_fp,\n            columns_to_load=cfg.columns_to_load,\n            model=model,\n            spectral_indices=cfg.spectral_indices,\n            sample_size=cfg.sample_size,\n            plot_n_samples=cfg.plot_n_samples,\n            seed=cfg.seed,\n            explain_model=cfg.explain_model,\n        )\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.min_max_normalize","title":"<code>kelp.xgb.training.train.min_max_normalize</code>","text":"<p>Runs min-max quantile normalization on the input array.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The input array.</p> required Source code in <code>kelp/xgb/training/train.py</code> <pre><code>def min_max_normalize(x: np.ndarray) -&gt; np.ndarray:  # type: ignore[type-arg]\n    \"\"\"\n    Runs min-max quantile normalization on the input array.\n\n    Args:\n        x: The input array.\n\n    Returns: Normalized array.\n\n    \"\"\"\n    vmin = np.expand_dims(np.expand_dims(np.quantile(x, q=0.01, axis=(1, 2)), 1), 2)\n    vmax = np.expand_dims(np.expand_dims(np.quantile(x, q=0.99, axis=(1, 2)), 1), 2)\n    return (x - vmin) / (vmax - vmin + consts.data.EPS)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api_ref/xgb/training/train/#kelp.xgb.training.train.run_training","title":"<code>kelp.xgb.training.train.run_training</code>","text":"<p>Runs XGBoost model training.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_dir</code> <code>Path</code> <p>The path to the training data.</p> required <code>dataset_fp</code> <code>Path</code> <p>The path to the training dataset parquet file.</p> required <code>columns_to_load</code> <code>List[str]</code> <p>The columns to load from the metadata dataset.</p> required <code>model</code> <code>XGBClassifier</code> <p>The model to train.</p> required <code>spectral_indices</code> <code>List[str]</code> <p>The spectral indices to append to the input records.</p> required <code>sample_size</code> <code>float</code> <p>The fraction of samples to use for training.</p> <code>1.0</code> <code>plot_n_samples</code> <code>int</code> <p>The number of samples to plot.</p> <code>10</code> <code>seed</code> <code>int</code> <p>The seed for reproducibility.</p> <code>SEED</code> <code>explain_model</code> <code>bool</code> <p>A flag indicating whether to run model feature importance calculation.</p> <code>False</code> Source code in <code>kelp/xgb/training/train.py</code> <pre><code>@timed\ndef run_training(\n    train_data_dir: Path,\n    dataset_fp: Path,\n    columns_to_load: List[str],\n    model: XGBClassifier,\n    spectral_indices: List[str],\n    sample_size: float = 1.0,\n    plot_n_samples: int = 10,\n    seed: int = consts.reproducibility.SEED,\n    explain_model: bool = False,\n) -&gt; XGBClassifier:\n    \"\"\"\n    Runs XGBoost model training.\n\n    Args:\n        train_data_dir: The path to the training data.\n        dataset_fp: The path to the training dataset parquet file.\n        columns_to_load: The columns to load from the metadata dataset.\n        model: The model to train.\n        spectral_indices: The spectral indices to append to the input records.\n        sample_size: The fraction of samples to use for training.\n        plot_n_samples: The number of samples to plot.\n        seed: The seed for reproducibility.\n        explain_model: A flag indicating whether to run model feature importance calculation.\n\n    Returns: A fitted XGBClassifier.\n\n    \"\"\"\n    metadata = pd.read_parquet(dataset_fp, columns=columns_to_load)\n    X_train, y_train, X_val, y_val, X_test, y_test = load_data(\n        df=metadata.drop([\"tile_id\"], axis=1, errors=\"ignore\"),\n        sample_size=sample_size,\n        seed=seed,\n    )\n    model = fit_model(model, X_train, y_train)\n    if plot_n_samples &gt; 0:\n        log_sample_predictions(\n            train_data_dir=train_data_dir,\n            metadata=metadata,\n            model=model,\n            spectral_indices=spectral_indices,\n            sample_size=plot_n_samples,\n            seed=seed,\n        )\n    eval_model(model, X_val, y_val, prefix=\"val\", seed=seed, explain_model=explain_model)\n    eval_model(model, X_test, y_test, prefix=\"test\", seed=seed, explain_model=explain_model)\n    return model\n</code></pre>"},{"location":"guides/contributing/","title":"Contributing","text":"<p>This page explains the coding style and general contributing guidelines for the dev team.</p>"},{"location":"guides/contributing/#instructions","title":"Instructions","text":"<p>There are a couple of rules that you should follow:</p> <ol> <li>Use <code>logging</code> module instead of <code>print()</code> function</li> <li>Try not to leave commended out code</li> <li>If certain functionality is needed in multiple places - please crate a separate module or a class for it and import it in your code.</li> <li>Use notebooks only for quick experimentation, they should not be the source of our production code.</li> <li>Set a guide in your IDE to 120 characters. We use 120 instead of suggested 79 from the PEP8 Guidelines.</li> <li>We use <code>mypy</code> to guard the type annotations.</li> <li>We use <code>isort</code>, <code>black</code> and <code>flake8</code> to maintain PEP8 compliant code.</li> <li>We use google style for the docstrings.</li> <li>Code review should be done for every PR.</li> <li>We do not merge directly to main branch.</li> <li>Always start from main branch when branching to new feature branch.</li> <li>We require at least one approve on each PR and all threads to be resolved before the merge.</li> <li>When merging we use squash before merge to have clean history</li> </ol>"},{"location":"guides/data-prep/","title":"Preparing data","text":"<p>There are a few scripts that run all necessary data preparations for training and inference.</p>"},{"location":"guides/data-prep/#sample-plotting","title":"Sample plotting","text":"<p>This script will plot composite images such as true color, color infrared and shortwave infrared for quick visual inspection. It will also generate plots for each image presenting aforementioned composites together with NDVI, DEM, QA Mask and Kelp Mask. An example can be seen below:</p> <p></p> <pre><code>make sample-plotting\n</code></pre> <p>Or:</p> <pre><code>python ./kelp/data_prep/sample_plotting.py \\\n    --data_dir data/raw \\\n    --metadata_fp data/raw/metadata_fTq0l2T.csv \\\n    --output_dir data/processed\n</code></pre> <p>For the same tile following composites are plotted:</p> <ul> <li>DEM:</li> </ul> <p></p> <ul> <li>True Color:</li> </ul> <p></p> <ul> <li>Color Infrared:</li> </ul> <p></p> <ul> <li>Shortwave Infrared:</li> </ul> <p></p>"},{"location":"guides/data-prep/#aoi-grouping","title":"AOI Grouping","text":"<p>This script will group similar images into AOIs (Areas Of Interest) which then can be used to perform Stratified K-Fold Cross Validation split.</p> <pre><code>make aoi-grouping\n</code></pre> <p>Or:</p> <pre><code>python ./kelp/data_prep/aoi_grouping.py \\\n    --dem_dir data/processed/dem \\\n    --output_dir data/processed/grouped_aoi_results/sim_th=0.97 \\\n    --metadata_fp data/raw/metadata_fTq0l2T.csv \\\n    --batch_size 128 \\\n    --similarity_threshold 0.97\n</code></pre> <p>The script will save results in the specified <code>output_dir</code>.</p> <pre><code>.\n\u251c\u2500\u2500 final_image_groups_similarity_threshold=0.95.json           &lt;- final list of simalr images\n\u251c\u2500\u2500 intermediate_image_groups_similarity_threshold=0.95.json    &lt;- intermediate result of similarity calculation\n\u251c\u2500\u2500 merged_image_groups_similarity_threshold=0.95.json          &lt;- deduplicated list of similar images\n\u2514\u2500\u2500 metadata_similarity_threshold=0.95.parquet                  &lt;- final metadata parquet file created from the final list of similar images\n</code></pre> <p>The metadata parquet file is just the metadata CSV file, but with an additional column <code>aoi_id</code> denoting the AOI ID.</p>"},{"location":"guides/data-prep/#eda","title":"EDA","text":"<p>Exploratory Data Analysis scripts are calculating basic statistics about each image and then plotting them as distribution plots.</p> <p>Run it with:</p> <pre><code>make eda\n</code></pre> <p>Or:</p> <pre><code>python ./kelp/data_prep/eda.py \\\n    --data_dir data/raw \\\n    --metadata_fp data/processed/grouped_aoi_results/sim_th=0.97/metadata_similarity_threshold=0.97.parquet \\\n    --output_dir data/processed/stats_97\n</code></pre> <p>For each image the script will calculate following statistics:</p> <ul> <li><code>has_kelp</code> - a flag indicating if the image has kelp in it</li> <li><code>non_kelp_pixels</code> - number of non-kelp pixels</li> <li><code>kelp_pixels</code> - number of kelp pixels</li> <li><code>kelp_pixels_pct</code> - percentage of all pixels marked as kelp</li> <li><code>high_kelp_pixels_pct</code> - a flag indicating that the kelp pixels denote over 40% of the whole image</li> <li><code>dem_nan_pixels</code> - number of NaN pixels in the DEM layer</li> <li><code>dem_has_nans</code> - a flag indicating if DEM layer has NaN values</li> <li><code>dem_nan_pixels_pct</code> - percentage of all DEM pixels marked as NaN</li> <li><code>dem_zero_pixels</code> - number of zero valued pixels in the DEM layer</li> <li><code>dem_zero_pixels_pct</code> - percentage of all DEM pixels with value=zero</li> <li><code>water_pixels</code> - estimated number of water pixels (pixels with value &lt;= zero)</li> <li><code>water_pixels_pct</code> - percentage of water pixels in the DEM layer</li> <li><code>almost_all_water</code> - a flag indicating that over 98% of the DEM layer pixels are water</li> <li><code>qa_corrupted_pixels</code> - number of corrupted pixels in the QA band</li> <li><code>qa_ok</code> - a flag indicating that no pixels are corrupted in the QA band</li> <li><code>qa_corrupted_pixels_pct</code> - percentage of corrupted pixels in the QA band</li> <li><code>high_corrupted_pixels_pct</code> - a flag indicating that over 40 % of the QA bands' pixels are corrupted</li> </ul> <p>Calculated statistics will be saved in the specified output directory in parquet format.</p> <p>Apart from figures, the script will also display and save descriptive statistics such as min, max, median etc. for each of the numerical statistics.</p> statistic aoi_id non_kelp_pixels kelp_pixels ... qa_corrupted_pixels_pct std 662.475 2695.318 2695.318 ... 0.138 min 0 8937 0 ... 0 mean 1479.905 121670.837 829.163 ... 0.07 max 2947 122500 113563 ... 0.999 count 5635 5635 5635 ... 7061 75% 1895.5 122500 880 ... 0.074 50% 1407 122388 112 ... 0.013 25% 1196 121620 0 ... 0.001 <p>Example plots that are generated in this step:</p> <ul> <li>AOI Images distribution</li> </ul> <p></p> <ul> <li>AOI Images distribution (filtered - without groups with single image)</li> </ul> <p></p> <ul> <li>Correlation matrix</li> </ul> <p></p> <ul> <li>DEM has NaNs</li> </ul> <p></p> <ul> <li>DEM NaN pixels distribution</li> </ul> <p></p> <ul> <li>Has Kelp</li> </ul> <p></p> <ul> <li>High Kelp Pixels distribution</li> </ul> <p></p> <ul> <li>Kelp Pixels Distribution</li> </ul> <p></p> <ul> <li>QA corrupted pixels percentage</li> </ul> <p></p> <ul> <li>QA OK</li> </ul> <p></p> <ul> <li>Images per Splits</li> </ul> <p></p>"},{"location":"guides/data-prep/#calculate-band-statistics","title":"Calculate band statistics","text":"<p>This script calculates per-band statistics to be used for input image normalization during training and inference. Spectral indices are automatically appended to the input Tensor, this way stats for all possible channels are computed.</p> <p>Note: The script will automatically try to use GPU to speed up calculation. Expect ~45x slowdowns if running on CPU!</p> <p>Run it with:</p> <pre><code>make calculate-band-stats\n</code></pre> <p>Or:</p> <pre><code>python ./kelp/data_prep/calculate_band_stats.py \\\n    --data_dir data/raw \\\n    --mask_using_qa \\\n    --mask_using_water_mask \\\n    --fill_missing_pixels_with_torch_nan \\\n    --output_dir data/processed\n</code></pre> <p>Note: The script is not perfect - for certain configurations, the resulting statistics can have NaN or Inf in them. Please adjust them by manually setting those items to reasonable values.</p>"},{"location":"guides/data-prep/#stratified-10-fold-cv-split","title":"Stratified 10-Fold CV split","text":"<p>This script performs stratified 10-fold cross validation split using metadata files generated earlier.</p> <p>Run it with:</p> <pre><code>make train-val-test-split-cv\n</code></pre> <p>Or:</p> <pre><code>python ./kelp/data_prep/train_val_test_split.py \\\n    --dataset_metadata_fp data/processed/stats/dataset_stats.parquet \\\n    --split_strategy cross_val \\\n    --seed 42 \\\n    --splits 10 \\\n    --output_dir data/processed\n</code></pre> <p>Note: By default <code>has_kelp</code>, <code>almost_all_water</code>, <code>qa_ok</code> and <code>high_corrupted_pixels_pct</code> columns are used for making stratification column.</p>"},{"location":"guides/evaluation/","title":"Evaluating models","text":"<p>Models trained both locally and in Azure ML can be evaluated afterward in order to verify different inference settings such as:</p> <ul> <li>Influence of <code>decision_threshold</code></li> <li>Adding and removing TTA</li> <li>Checking different <code>tta_merge_mode</code></li> <li>Trying out different <code>precision</code> settings</li> </ul>"},{"location":"guides/evaluation/#single-model","title":"Single model","text":"<p>To evaluate a single model run:</p> <pre><code>make eval\n</code></pre> <p>Or python script directly:</p> <pre><code>python ./kelp/nn/training/eval.py \\\n    --data_dir data/raw \\\n    --metadata_dir data/processed \\\n    --dataset_stats_dir data/processed \\\n    --run_dir $(RUN_DIR) \\\n    --output_dir mlruns \\\n    --precision bf16-mixed \\\n    --decision_threshold=0.48 \\\n    --experiment_name model-eval-exp\n</code></pre> <p>To apply TTA:</p> <pre><code>python ./kelp/nn/training/eval.py \\\n    --data_dir data/raw \\\n    --metadata_dir data/processed \\\n    --dataset_stats_dir data/processed \\\n    --run_dir $(RUN_DIR) \\\n    --output_dir mlruns \\\n    --precision bf16-mixed \\\n    --tta \\\n    --tta_merge_mode max \\\n    --decision_threshold=0.48 \\\n    --experiment_name model-eval-exp\n</code></pre> <p>All eval runs are logged to MLFlow. You can inspect and compare different eval configurations and select the best local models this way.</p>"},{"location":"guides/evaluation/#multiple-models-with-the-same-eval-config","title":"Multiple models with the same eval config","text":"<p>If you want to eval multiple models at once using the same eval config run:</p> <pre><code>make eval-many\n</code></pre>"},{"location":"guides/evaluation/#from-folders","title":"From folders","text":"<p>If you have a folder with Ground Truth and a folder with predictions you can run:</p> <pre><code>make eval-from-folders\n</code></pre> <p>Which will be equivalent to running:</p> <pre><code>python kelp/nn/training/eval_from_folders.py \\\n    --gt_dir=$(GT_DIR) \\\n    --preds_dir=$(PREDS_DIR) \\\n    --tags fold_0_run_dir=$(FOLD_0_RUN_DIR) \\\n        fold_1_run_dir=$(FOLD_1_RUN_DIR) \\\n        fold_2_run_dir=$(FOLD_2_RUN_DIR) \\\n        fold_3_run_dir=$(FOLD_3_RUN_DIR) \\\n        fold_4_run_dir=$(FOLD_4_RUN_DIR) \\\n        fold_5_run_dir=$(FOLD_5_RUN_DIR) \\\n        fold_6_run_dir=$(FOLD_6_RUN_DIR) \\\n        fold_7_run_dir=$(FOLD_7_RUN_DIR) \\\n        fold_8_run_dir=$(FOLD_8_RUN_DIR) \\\n        fold_9_run_dir=$(FOLD_9_RUN_DIR) \\\n        fold_0_weight=$(FOLD_0_WEIGHT) \\\n        fold_1_weight=$(FOLD_1_WEIGHT) \\\n        fold_2_weight=$(FOLD_2_WEIGHT) \\\n        fold_3_weight=$(FOLD_3_WEIGHT) \\\n        fold_4_weight=$(FOLD_4_WEIGHT) \\\n        fold_5_weight=$(FOLD_5_WEIGHT) \\\n        fold_6_weight=$(FOLD_6_WEIGHT) \\\n        fold_7_weight=$(FOLD_7_WEIGHT) \\\n        fold_8_weight=$(FOLD_8_WEIGHT) \\\n        fold_9_weight=$(FOLD_9_WEIGHT) \\\n        soft_labels=True \\\n        split_decision_threshold=None \\\n        decision_threshold=0.48 \\\n        tta=False \\\n        tta_merge_mode=mean \\\n        precision=bf16-mixed\n</code></pre> <p>In that case the evaluation script will compare corresponding masks and predictions and calculate performance metrics. The <code>--tags</code> will be converted to key-value pairs and logged to MLFlow for you to keep track of model params.</p>"},{"location":"guides/evaluation/#ensemble","title":"Ensemble","text":"<p>Evaluating ensemble is tricky as a separate evaluation dataset is needed. In the example below we use fold=8 validation images to perform evaluation across all folds, which is not ideal.</p> <pre><code>make eval-ensemble\n</code></pre> <p>Under the hood the following steps are executed:</p> <pre><code>rm -rf data/predictions/eval_results\nmake cv-predict AVG_PREDS_VERSION=eval PREDS_INPUT_DIR=data/raw/splits/split_8/images AVG_PREDS_OUTPUT_DIR=data/predictions/eval_results\nmake average-predictions AVG_PREDS_VERSION=eval PREDS_INPUT_DIR=data/raw/splits/split_8/images AVG_PREDS_OUTPUT_DIR=data/predictions/eval_results\nmake eval-from-folders GT_DIR=data/raw/splits/split_8/masks PREDS_DIR=data/predictions/eval_results\n</code></pre>"},{"location":"guides/inference/","title":"Running inference","text":"<p>To run the prediction you have a few options:</p> <ol> <li>Run <code>predict.py</code> to generate predictions and then <code>submit.py</code> to create <code>tar</code> file with predictions</li> <li>Run <code>predict_and_submit.py</code> to generate both predictions and the submission file in one go.</li> <li>Run <code>average_predictions.py</code> that will take the predictions from multiple models, average them and then crate submission file.</li> </ol> <p>If you want to learn more about creating submission files please see: Making submissions page.</p>"},{"location":"guides/inference/#single-model","title":"Single model","text":"<p>Use following Makefile command to generate predictions for images in specified folder (please adjust the configuration):</p> <pre><code>make predict\n</code></pre> <p>The same can be achieved by running python script directly:</p> <pre><code>python ./kelp/nn/inference/predict.py \\\n    --data_dir $(PREDS_INPUT_DIR) \\\n    --dataset_stats_dir=data/processed \\\n    --output_dir $(PREDS_OUTPUT_DIR) \\\n    --run_dir $(RUN_DIR) \\\n    --use_checkpoint $(CHECKPOINT) \\\n    --decision_threshold 0.48 \\\n    --precision bf16-mixed\n</code></pre> <p>Additional notes:</p> <ul> <li>The prediction script has an option to run Test Time Augmentations (TTA) - in order to use it please provide <code>--tta</code> flag and corresponding <code>--tta_merge_mode</code> which can be one of <code>[min, max, mean, gmean, sum, tsharpen]</code>.</li> <li>It also has an option to use optional <code>--decision_threshold</code> value. If not provided <code>torch.argmax</code> is used on raw model outputs.</li> <li>What's more, when <code>--soft_labels</code> flag is passed the model's raw predictions will be passed to <code>torch.sigmoid</code> and only the positive class probability will be returned as predicted mask.</li> </ul> <p>If you want to use model checkpoint directly instead of full experiment run directory you'll need to leverage <code>kelp.nn.inference.predict.run_prediction()</code> function. The docs for this function are here.</p> <p>You can leverage following code snipped to get started.</p> <pre><code>from kelp.nn.inference.predict import run_prediction\n\ndata_dir = ...  # Path to a directory with prediction images\noutput_dir = ...  # Path to output directory where to save the predictions\nmodel_checkpoint = ...  # Path to model checkpoint - a *.ckpt file or  MLFlow `model` directory\nuse_mlflow = ...  # A flag indicating whether to use MLflow to load the model. `True` if checkpoint path is MLFlow model directory.\ntraining_config = ...  # Loaded original training config\ntta = ...  # A flag indicating whether to use TTA for prediction.\ntta_merge_mode = ... # The TTA merge mode.\nsoft_labels = ...  # A flag indicating whether to use soft labels for prediction.\ndecision_threshold = ...  # An optional decision threshold for prediction. `torch.argmax` will be used by default.\n\nrun_prediction(\n    data_dir=data_dir,\n    output_dir=output_dir,\n    model_checkpoint=model_checkpoint,\n    use_mlflow=use_mlflow,\n    train_cfg=training_config,\n    tta=tta,\n    tta_merge_mode=tta_merge_mode,\n    soft_labels=soft_labels,\n    decision_threshold=decision_threshold,\n)\n</code></pre> <p>NOTE: See the <code>training_config</code> property on <code>kelp.nn.inference.predict.PredictConfig</code> class to see how to load original training config from run artifact. Note that if your dataset stats file path is different from the one used for training you must adjust it in for the training config!</p>"},{"location":"guides/inference/#ensemble","title":"Ensemble","text":"<p>Running inference with model ensemble is impractical in real-world scenarios. Which is why it is directly coupled with making a submission file. I expect whoever is working with this repo to never use it in production setting. That being said if you want to run inference with model ensemble you'll need to do the following:</p> <ol> <li>For each model in ensemble - run prediction with it.</li> <li>Using outputs generated from single models, run <code>average_predictions.py</code> to create final segmentation masks.</li> </ol> <p>We are splitting the ensemble prediction into two stages to avoid having to load all models at once and risking GPU OOM errors. It could be done on beefier machine, but would be extremely heavy in terms of memory requirements. You can write your own python script to do it all in one go if you really wish to.</p> <p>Leveraging Makefile commands to run ensemble prediction can yield the same results:</p> <pre><code>make cv-predict\n</code></pre> <p>You need to adjust the Makefile configuration if you are using your own models and prediction strategy.</p> <p>See Making submission file from model ensemble to learn more.</p>"},{"location":"guides/makefile-usage/","title":"Using Makefile commands","text":"<p>In order to use the Makefile commands you need to be on Linux.</p>"},{"location":"guides/makefile-usage/#basic-usage","title":"Basic usage","text":"<p>Makefile commands are easy to use. Just type <code>make</code> in your terminal, hit enter and see the list of available commands.</p> <pre><code>make\n</code></pre> <p>The command above is equivalent to running:</p> <pre><code>make help\n</code></pre>"},{"location":"guides/makefile-usage/#development-work-commands","title":"Development work commands","text":""},{"location":"guides/makefile-usage/#freezing-project-dependencies","title":"Freezing project dependencies","text":"<ul> <li> <p>lock - Creates conda-lock file</p> <pre><code>make lock\n</code></pre> </li> </ul>"},{"location":"guides/makefile-usage/#creating-environments","title":"Creating environments","text":"<ul> <li> <p>env - Creates env from conda-lock file</p> <pre><code>make env\n</code></pre> </li> <li> <p>setup-pre-commit - Installs pre-commit hooks</p> <pre><code>make setup-pre-commit\n</code></pre> </li> <li> <p>setup-editable - Installs the project in an editable mode</p> <pre><code>make setup-editable\n</code></pre> </li> <li> <p>configure-torch-ort - Configures torch-ort</p> <pre><code>make configure-torch-ort\n</code></pre> </li> <li> <p>local-env - Creates local environment and installs pre-commit hooks</p> <pre><code>make local-env\n</code></pre> </li> </ul>"},{"location":"guides/makefile-usage/#helper-commands","title":"Helper commands","text":"<ul> <li> <p>format - Runs code formatting (<code>isort</code>, <code>black</code>, <code>flake8</code>)</p> <pre><code>make format\n</code></pre> </li> <li> <p>type-check - Runs type checking with <code>mypy</code></p> <pre><code>make type-check\n</code></pre> </li> <li> <p>test - Runs pytest</p> <pre><code>make test\n</code></pre> </li> <li> <p>testcov - Runs tests and generates coverage reports</p> <pre><code>make testcov\n</code></pre> </li> <li> <p>mpc - Runs manual <code>pre-commit</code> stuff</p> <pre><code>make mpc\n</code></pre> </li> <li> <p>docs - Builds the documentation</p> <pre><code>make docs\n</code></pre> </li> <li> <p>pc - Runs <code>pre-commit</code> hooks</p> <pre><code>make pc\n</code></pre> </li> <li> <p>clean - Cleans artifacts</p> <pre><code>make clean\n</code></pre> </li> </ul>"},{"location":"guides/makefile-usage/#data-prep","title":"Data prep","text":"<p>The commands in this section expect the data to be in certain directories. Please see Makefile definition for more details.</p> <ul> <li> <p>sample-plotting - Runs tile plotting</p> <pre><code>make sample-plotting\n</code></pre> </li> <li> <p>aoi-grouping - Runs AOI grouping</p> <pre><code>make aoi-grouping\n</code></pre> </li> <li> <p>eda - Runs EDA</p> <pre><code>make eda\n</code></pre> </li> <li> <p>calculate-band-stats - Runs band statistics calculation</p> <pre><code>make calculate-band-stats\n</code></pre> </li> <li> <p>train-val-test-split-cv - Runs train-val-test split using cross validation</p> <pre><code>make train-val-test-split-cv\n</code></pre> </li> <li> <p>train-val-test-split-random - Runs train-val-test split using random split</p> <pre><code>make train-val-test-split-random\n</code></pre> </li> </ul>"},{"location":"guides/makefile-usage/#model-training","title":"Model training","text":"<p>The commands in this section accept arguments that can be modified from command line. Please see the Makefile definition for more details.</p> <ul> <li> <p>train - Trains single CV split</p> <pre><code>make train\n</code></pre> </li> <li> <p>train-all-splits - Trains on all splits</p> <pre><code>make train-all-splits\n</code></pre> </li> </ul>"},{"location":"guides/makefile-usage/#model-evaluation","title":"Model evaluation","text":"<p>The commands in this section accept arguments that can be modified from command line. Please see the Makefile definition for more details.</p> <ul> <li> <p>eval - Runs evaluation for selected run</p> <pre><code>make eval\n</code></pre> </li> <li> <p>eval-many - Runs evaluation for specified runs</p> <pre><code>make eval-many\n</code></pre> </li> <li> <p>eval-from-folders - Runs evaluation by comparing predictions to ground truth mask</p> <pre><code>make eval-from-folders\n</code></pre> </li> <li> <p>eval-ensemble - Runs ensemble evaluation</p> <pre><code>make eval-ensemble\n</code></pre> </li> </ul>"},{"location":"guides/makefile-usage/#making-submissions","title":"Making submissions","text":"<p>The commands in this section accept arguments that can be modified from command line. Please see the Makefile definition for more details.</p> <ul> <li> <p>predict - Runs prediction</p> <pre><code>make predict\n</code></pre> </li> <li> <p>submission - Generates submission file</p> <pre><code>make submission\n</code></pre> </li> <li> <p>predict-and-submit - Runs inference and generates submission file</p> <pre><code>make predict-and-submit\n</code></pre> </li> <li> <p>average-predictions - Runs prediction averaging</p> <pre><code>make average-predictions\n</code></pre> </li> <li> <p>cv-predict - Runs inference on specified folds, averages the predictions and generates submission file</p> <pre><code>make cv-predict\n</code></pre> </li> </ul>"},{"location":"guides/makefile-usage/#best-submissions-reproducibility","title":"Best submissions reproducibility","text":"<ul> <li> <p>repro-best-single-model-submission - Runs reproduction of best single model submission with Priv LB score = 0.7264</p> <pre><code>make repro-best-single-model-submission\n</code></pre> </li> <li> <p>repro-top-1-submission - Runs reproduction of #1 submission with Priv LB score = 0.7318</p> <pre><code>make repro-top-1-submission\n</code></pre> </li> <li> <p>repro-top-2-submission - Runs reproduction of #2 submission with Priv LB score = 0.7318</p> <pre><code>make repro-top-2-submission\n</code></pre> </li> </ul>"},{"location":"guides/mlflow-artifacts/","title":"MLFlow artifacts","text":""},{"location":"guides/mlflow-artifacts/#parameters","title":"Parameters","text":"<p>All parameters from the training config are also logged as Run parameters. The training config is logged as yaml file too. A sample full training config can be found below:</p> <pre><code>accumulate_grad_batches: 1\nalmost_all_water_importance_factor: 0.5\narchitecture: unet\nbands:\n- R\n- G\n- B\n- SWIR\n- NIR\n- QA\n- DEM\nbatch_size: 32\nbenchmark: false\nce_class_weights:\n- 0.4\n- 0.6\nce_smooth_factor: 0.1\ncompile: false\ncompile_dynamic: false\ncompile_mode: default\ncosine_T_mult: 2\ncosine_eta_min: 1.0e-07\ncv_split: 4\ncyclic_base_lr: 1.0e-05\ncyclic_mode: exp_range\ndata_dir: data/raw\ndataset_stats_fp: data/processed/2023-12-31T20:30:39-stats-fill_value=nan-mask_using_qa=True-mask_using_water_mask=True.json\ndecision_threshold: 0.48\ndecoder_attention_type: null\ndecoder_channels:\n- 256\n- 128\n- 64\n- 32\n- 16\ndem_nan_pixels_pct_importance_factor: 0.25\ndem_zero_pixels_pct_importance_factor: -1.0\nearly_stopping_patience: 10\nencoder: tu-efficientnet_b5\nencoder_depth: 5\nencoder_weights: imagenet\nepochs: 10\nexperiment: nn-train-exp\nfast_dev_run: false\nfill_missing_pixels_with_torch_nan: true\nhas_kelp_importance_factor: 3.0\nignore_index: null\nimage_size: 352\ninterpolation: nearest\nkelp_pixels_pct_importance_factor: 0.2\nlimit_test_batches: null\nlimit_train_batches: null\nlimit_val_batches: null\nlog_every_n_steps: 50\nloss: dice\nlr: 0.0003\nlr_scheduler: onecycle\nmask_using_qa: true\nmask_using_water_mask: true\nmetadata_fp: data/processed/train_val_test_dataset_strategy=cross_val.parquet\nmonitor_metric: val/dice\nmonitor_mode: max\nnormalization_strategy: quantile\nnum_classes: 2\nnum_workers: 6\nobjective: binary\nonecycle_div_factor: 2.0\nonecycle_final_div_factor: 100.0\nonecycle_pct_start: 0.1\noptimizer: adamw\nort: false\noutput_dir: mlruns\nplot_n_batches: 3\nprecision: bf16-mixed\npretrained: true\nqa_corrupted_pixels_pct_importance_factor: -1.0\nqa_ok_importance_factor: 0.0\nreduce_lr_on_plateau_factor: 0.95\nreduce_lr_on_plateau_min_lr: 1.0e-06\nreduce_lr_on_plateau_patience: 3\nreduce_lr_on_plateau_threshold: 0.0001\nresize_strategy: pad\nsahi: false\nsamples_per_epoch: 10240\nsave_top_k: 1\nseed: 42\nspectral_indices:\n- DEMWM\n- NDVI\n- ATSAVI\n- AVI\n- CI\n- ClGreen\n- GBNDVI\n- GVMI\n- IPVI\n- KIVU\n- MCARI\n- MVI\n- NormNIR\n- PNDVI\n- SABI\n- WDRVI\n- mCRIG\nswa: false\nswa_annealing_epochs: 10\nswa_epoch_start: 0.5\nswa_lr: 3.0e-05\ntta: false\ntta_merge_mode: max\nuse_weighted_sampler: true\nval_check_interval: null\nweight_decay: 0.0001\n</code></pre>"},{"location":"guides/mlflow-artifacts/#metrics","title":"Metrics","text":"<p>The optimization metric (can be selected via training config and passed through command line arguments) is by default set as <code>val/dice</code>. The same metric is used for early stopping.</p> <p>During the training loop following metrics are logged:</p> <ul> <li><code>epoch</code></li> <li><code>hp_metric</code> - logged only once at the end of training - the <code>val/dice</code> score of the best model</li> <li><code>lr-AdamW</code> - the <code>AdamW</code> part depends on actual optimizer used for training</li> <li><code>lr-AdamW-momentum</code> - the <code>AdamW</code> part depends on actual optimizer used for training</li> <li><code>lr-AdamW-weight_decay</code> - the <code>AdamW</code> part depends on actual optimizer used for training</li> <li><code>train/loss</code></li> <li><code>train/dice</code></li> <li><code>val/loss</code></li> <li><code>val/dice</code></li> <li><code>val/iou</code></li> <li><code>val/iou_kelp</code></li> <li><code>val/iou_background</code></li> <li><code>val/accuracy</code></li> <li><code>val/precision</code></li> <li><code>val/f1</code></li> <li><code>test/loss</code></li> <li><code>test/dice</code></li> <li><code>test/iou</code></li> <li><code>test/iou_kelp</code></li> <li><code>test/iou_background</code></li> <li><code>test/accuracy</code></li> <li><code>test/precision</code></li> <li><code>test/f1</code></li> </ul>"},{"location":"guides/mlflow-artifacts/#images","title":"Images","text":""},{"location":"guides/mlflow-artifacts/#spectral-indices","title":"Spectral indices","text":"<ul> <li>ATSAVI</li> </ul> <ul> <li>AVI</li> </ul> <ul> <li>CI</li> </ul> <ul> <li>ClGreen</li> </ul> <ul> <li>DEMWM</li> </ul> <ul> <li>GBNDVI</li> </ul> <ul> <li>GVMI</li> </ul> <ul> <li>IPVI</li> </ul> <ul> <li>KIVU</li> </ul> <ul> <li>MCARI</li> </ul> <ul> <li>mCRIG</li> </ul> <ul> <li>MVI</li> </ul> <ul> <li>NDVI</li> </ul> <ul> <li>NormNIR</li> </ul> <ul> <li>PNDVI</li> </ul> <ul> <li>SABI</li> </ul> <ul> <li>WDRVI</li> </ul>"},{"location":"guides/mlflow-artifacts/#composites","title":"Composites","text":"<ul> <li>True color</li> </ul> <ul> <li>Color infrared</li> </ul> <ul> <li>Shortwave infrared</li> </ul> <ul> <li>DEM</li> </ul> <ul> <li>QA</li> </ul> <ul> <li>Ground Truth Mask</li> </ul>"},{"location":"guides/mlflow-artifacts/#predictions","title":"Predictions","text":"<p>The predictions for first <code>plot_n_batches</code> in the val dataset are logged as a grid to monitor the model learning progress. The data is logged after every epoch. Here are only predictions from a few epochs.</p> <ul> <li>Epoch #0</li> </ul> <p></p> <ul> <li>Epoch #1</li> </ul> <p></p> <ul> <li>Epoch #2</li> </ul> <p></p> <ul> <li>Epoch #5</li> </ul> <p></p> <ul> <li>Epoch #10</li> </ul> <p></p> <ul> <li>Epoch #20</li> </ul> <p></p> <ul> <li>Epoch #38 (best epoch)</li> </ul> <p></p>"},{"location":"guides/mlflow-artifacts/#confusion-matrix","title":"Confusion matrix","text":"<ul> <li>Normalized confusion matrix</li> </ul> <ul> <li>Full confusion matrix</li> </ul>"},{"location":"guides/mlflow-artifacts/#checkpoints","title":"Checkpoints","text":"<p>MLFlow logger has been configured to log <code>top_k</code> best checkpoints and the last one (needed when running SWA). The checkpoints will be available under <code>checkpoints</code> and <code>model</code> catalog in the run artifacts directory.</p>"},{"location":"guides/reproducibility/","title":"Reproducibility of results","text":"<p>This guide contains steps necessary to reproduce the competition results.</p> <p>Before running any of the commands in this section, please make sure you have configured your local development environment by following this guide.</p> <p>You have two options for reproducing the results:</p> <ol> <li>Running from scratch</li> <li>Using model checkpoints and generated dataset files</li> </ol>"},{"location":"guides/reproducibility/#i-want-to-train-from-scratch","title":"I want to train from scratch","text":"<p>If you want to fully reproduce the solution results starting from raw training data. Please follow this steps.</p>"},{"location":"guides/reproducibility/#preparing-the-data","title":"Preparing the data","text":"<p>To prepare the data follow steps outlined in this section.</p>"},{"location":"guides/reproducibility/#download-the-data","title":"Download the data","text":"<p>Download and extract the data to following directories in the root of the repo:</p> <pre><code>kelp-wanted-competition/\n\u2514\u2500\u2500 data\n   \u2514\u2500\u2500 raw\n      \u251c\u2500\u2500 train\n      \u2502   \u251c\u2500\u2500 images             &lt;= place training images here\n      \u2502   \u2514\u2500\u2500 masks              &lt;= place training masks here\n      \u251c\u2500\u2500 test\n      \u2502   \u2514\u2500\u2500 images             &lt;= place test images here\n      \u2514\u2500\u2500 metadata_fTq0l2T.csv   &lt;= place the metadata file directly in the `raw` dir\n</code></pre> <p>Run in order:</p> <ul> <li> <p>Plot samples - for better understanding of the data and quick visual inspection</p> <pre><code>make sample-plotting\n</code></pre> </li> <li> <p>AOI Grouping - will group the similar images into AOIs and use those groups to generate CV-folds.</p> <pre><code>make aoi-grouping\n</code></pre> </li> <li> <p>EDA - run Exploratory Data Analysis to visualize statistical distributions of different image features</p> <pre><code>make eda\n</code></pre> </li> <li> <p>Calculate band statistics - will calculate per-band min, max, mean, std etc. statistics (including spectral indices)</p> <pre><code>make calculate-band-stats\n</code></pre> </li> <li> <p>Train-Val-Test split with Stratified K-Fold Cross Validation</p> <pre><code>make train-val-test-split-cv\n</code></pre> </li> </ul> <p>The generated <code>train_val_test_dataset_strategy=cross_val.parquet</code> metadata lookup file and <code>YYYY-MM-DD-Thh:mm:ss-stats-fill_value=nan-mask_using_qa=True-mask_using_water_mask=True.json</code> files will have to be used as inputs for the training scripts, both locally and for Azure ML Pipelines.</p>"},{"location":"guides/reproducibility/#training-the-models","title":"Training the models","text":"<p>For model training you have two options. Train them on Azure ML or train them locally.</p>"},{"location":"guides/reproducibility/#via-azure-ml-aml","title":"Via Azure ML (AML)","text":"<p>Note: You'll need Azure Subscription and Azure DevOps Organization for that one. You'll also need a basic knowledge of Azure services such as Entra ID, Blob Storage and Azure ML.</p> <ol> <li>Create Azure ML Workspace</li> <li>Setup Service Principal with access to the Azure ML Workspace</li> <li>Setup Azure DevOps variable group (see .env-sample for what variables are needed)</li> <li>Setup Service Connections for GitHub, AML Workspace and ARM Resource Group (use SP created earlier for it)</li> <li>Setup Azure DevOps Pipelines</li> <li>In Azure ML set up the following:</li> <li>Datasets datastore</li> <li>Training Dataset Data Asset - please upload training data to Blob Storage and register it as Folder Asset</li> <li>Dataset Stats Data Asset - please upload the stats file to Blob Storage and register it as File Asset</li> <li>Dataset Metadata Data Asset - please upload the metadata parquet file (generated by <code>train-val-test-split-cv</code> Makefile command)    to Blob Storage and register it as File Asset</li> <li>Compute Clusters with spot instances</li> <li>Training Environment</li> <li>Once done you'll need to modify the versions and names in the AML components and AML pipelines to match the resource names you have just created. I recommend you use Azure ML CLI to set them up from the terminal. See <code>yaml</code> files in the aml folder for details.</li> <li>You can now trigger the Azure ML Hyperparameter Search or Model Training Pipelines via Azure DevOps Pipelines</li> </ol>"},{"location":"guides/reproducibility/#locally","title":"Locally","text":"<ul> <li> <p>Run all folds training:</p> <pre><code>make train-all-folds\n</code></pre> </li> <li> <p>Run single fold training:</p> <pre><code>make train FOLD_NUMBER=&lt;fold-number&gt;\n</code></pre> </li> </ul> <p>See the Makefile definition to see all available options.</p> <p>Note: Both Azure ML Pipelines and Makefile commands have been adjusted to use hyperparameters used in the best model submission.</p>"},{"location":"guides/reproducibility/#making-predictions","title":"Making predictions","text":"<p>Once the models have been trained you can generate submission file with them by adjusting the run_dir paths in the Makefile and running following commands:</p>"},{"location":"guides/reproducibility/#single-model","title":"Single model","text":"<p>Run:</p> <pre><code>make predict-and-submit\n</code></pre> <p>The submission directory will be created under <code>data/submissions/single-model</code>.</p>"},{"location":"guides/reproducibility/#ensemble","title":"Ensemble","text":"<p>Running ensemble prediction is just as easy with following command:</p> <pre><code>make cv-predict\n</code></pre> <p>The submission directories will be created under <code>data/submissions/avg</code>.</p> <p>This will run prediction with each fold individually and then average the predictions using weights specified in the Makefile. The weights and decision thresholds in the Makefile commands are already set up to be in line with the winning ones. You just need to adjust the fold dir paths.</p> <p>Note: Please note that all folds were used for the best submissions. You'll need to train all folds!</p>"},{"location":"guides/reproducibility/#i-want-to-use-model-checkpoints","title":"I want to use model checkpoints","text":"<p>If you want to just reproduce the final submissions without running everything from scratch follow steps in this section.</p>"},{"location":"guides/reproducibility/#download-models-and-dataset","title":"Download models and dataset","text":"<p>Download following files.</p>"},{"location":"guides/reproducibility/#download-checkpoints","title":"Download checkpoints","text":"<p>The full model training run directories are hosted here:</p> <ul> <li>Best single model (private LB 0.7264): best-single-model.zip</li> <li>Best submission #1 (private LB 0.7318): top-submission-1.zip</li> <li>Best submission #2 (private LB 0.7318): top-submission-2.zip</li> </ul> <p>Please download them and extract to <code>models</code> directory. The final directory structure should look like this:</p> <pre><code>models/\n\u251c\u2500\u2500 best-single-model\n\u2502   \u2514\u2500\u2500 Job_sincere_tangelo_dm0xsbhc_OutputsAndLogs\n\u251c\u2500\u2500 top-submission-1\n\u2502   \u251c\u2500\u2500 Job_elated_atemoya_31s98pwg_OutputsAndLogs\n\u2502   \u251c\u2500\u2500 Job_hungry_loquat_qkrw2n2p_OutputsAndLogs\n\u2502   \u251c\u2500\u2500 Job_icy_market_4l11bvw2_OutputsAndLogs\n\u2502   \u251c\u2500\u2500 Job_keen_evening_3xnlbrsr_OutputsAndLogs\n\u2502   \u251c\u2500\u2500 Job_model_training_exp_65_OutputsAndLogs\n\u2502   \u251c\u2500\u2500 Job_model_training_exp_67_OutputsAndLogs\n\u2502   \u251c\u2500\u2500 Job_nice_cheetah_grnc5x72_OutputsAndLogs\n\u2502   \u251c\u2500\u2500 Job_strong_door_yrq9zpmd_OutputsAndLogs\n\u2502   \u251c\u2500\u2500 Job_willing_pin_72ss6cnc_OutputsAndLogs\n\u2502   \u2514\u2500\u2500 Job_yellow_evening_cmy9cnv7_OutputsAndLogs\n\u2514\u2500\u2500 top-submission-2\n    \u251c\u2500\u2500 Job_boring_foot_hb224t08_OutputsAndLogs\n    \u251c\u2500\u2500 Job_coral_lion_x39ft9cb_OutputsAndLogs\n    \u251c\u2500\u2500 Job_dreamy_nut_fkwzmgxh_OutputsAndLogs\n    \u251c\u2500\u2500 Job_icy_airport_7r8h9q3c_OutputsAndLogs\n    \u251c\u2500\u2500 Job_lemon_drop_cxncbygc_OutputsAndLogs\n    \u251c\u2500\u2500 Job_loving_insect_hvd7v5p9_OutputsAndLogs\n    \u251c\u2500\u2500 Job_plum_angle_0f163gk5_OutputsAndLogs\n    \u251c\u2500\u2500 Job_plum_kettle_36dw15zk_OutputsAndLogs\n    \u251c\u2500\u2500 Job_tender_foot_07bt1687_OutputsAndLogs\n    \u2514\u2500\u2500 Job_wheat_tongue_mjzjpvjw_OutputsAndLogs\n</code></pre>"},{"location":"guides/reproducibility/#download-competition-data","title":"Download competition data","text":"<p>Download the competition data as described in Download the Data section.</p>"},{"location":"guides/reproducibility/#download-prepped-files","title":"Download prepped files","text":"<p>If you don't want to run all data preparation steps you can just download the metadata and dataset stats files from here:</p> <ul> <li>Dataset metadata - train, val and test 10-fold CV split: train_val_test_dataset.parquet - place it in <code>data/processed</code> directory</li> <li>Dataset per-band statistics: ds-stats.zip - extract it and place the JSON file in <code>data/processed</code> directory</li> </ul> <p>The final <code>data</code> directory should have the following structure:</p> <pre><code>data/\n\u251c\u2500\u2500 auxiliary\n\u251c\u2500\u2500 interim\n\u251c\u2500\u2500 predictions                &lt;= single model predictions from ensembles will be saved here\n\u251c\u2500\u2500 processed\n\u2502   \u251c\u2500\u2500 2023-12-31T20:30:39-stats-fill_value=nan-mask_using_qa=True-mask_using_water_mask=True.json\n\u2502   \u2514\u2500\u2500 train_val_test_dataset.parquet\n\u251c\u2500\u2500 raw\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 images             &lt;= place training images here\n\u2502   \u2502   \u2514\u2500\u2500 masks              &lt;= place training masks here\n\u2502   \u251c\u2500\u2500 test\n\u2502   \u2502   \u2514\u2500\u2500 images             &lt;= place test images here\n\u2502   \u2514\u2500\u2500 metadata_fTq0l2T.csv   &lt;= place the metadata file directly in the `raw` dir\n\u2514\u2500\u2500 submissions\n    \u251c\u2500\u2500 avg                    &lt;= ensemble submissions will be saved here\n    \u2514\u2500\u2500 single-model           &lt;= single model submissions will be saved here\n</code></pre>"},{"location":"guides/reproducibility/#making-predictions_1","title":"Making predictions","text":"<p>To reproduce the best submissions follow this steps.</p> <p>NOTE: The Makefile commands expect the model and dataset files to be in correct directories!</p>"},{"location":"guides/reproducibility/#single-model_1","title":"Single model","text":"<p>Run:</p> <pre><code>make repro-best-single-model-submission\n</code></pre> <p>The submission directory will be created under <code>data/submissions/single-model</code>.</p>"},{"location":"guides/reproducibility/#ensemble_1","title":"Ensemble","text":"<p>To reproduce top #1 submission with Priv LB score of 0.7318 run:</p> <pre><code>make repro-top-1-submission\n</code></pre> <p>To reproduce top #2 submission with Priv LB score of 0.7318 run:</p> <pre><code>make repro-top-2-submission\n</code></pre> <p>The submission directories will be created under <code>data/submissions/avg</code>. Individual model predictions will be saved under <code>data/predictions/top-1-submission</code> and <code>data/predictions/top-2-submission</code>.</p>"},{"location":"guides/sahi/","title":"SAHI","text":"<p>SAHI helps overcome the problem with detecting and segmenting small objects in large images by utilizing inference on image slices and prediction merging. Because of this it is slower than running inference on full image but at the same time usually ends up having better performance, especially for smaller features.</p> <p> Source: https://github.com/obss/sahi</p>"},{"location":"guides/sahi/#overview","title":"Overview","text":"<p>The idea is simple:</p> <ol> <li>Generate sliced dataset of small 128x128 non-overlapping tiles from the bigger 350x350 input images</li> <li>Use this dataset to train new model</li> <li>During training resize the crops to e.g. 320x320 resolution and train on those</li> <li>When running inference generate overlapping tiles, inference on those tiles, and merge the predicted masks by averaging the predictions in the overlapping areas</li> <li>Profit?</li> </ol>"},{"location":"guides/sahi/#data-prep","title":"Data prep","text":"<p>To create SAHI dataset of small images run:</p> <pre><code>python ./kelp/data_prep/sahi_dataset_prep.py \\\n    --data_dir=data/raw/train \\\n    --metadata_fp=data/processed/train_val_test_dataset.parquet \\\n    --output_dir=data/processed/sahi \\\n    --image_size=128 \\\n    --stride=128\n</code></pre>"},{"location":"guides/sahi/#training","title":"Training","text":"<p>To run SAHI training run:</p> <pre><code>python ./kelp/nn/training/train.py \\\n    --data_dir data/processed/sahi \\\n    --output_dir mlruns \\\n    --metadata_fp data/processed/sahi/sahi_train_val_test_dataset.parquet \\\n    --dataset_stats_fp data/processed/2023-12-31T20:30:39-stats-fill_value=nan-mask_using_qa=True-mask_using_water_mask=True.json \\\n    --cv_split $(FOLD_NUMBER) \\\n    --batch_size 32 \\\n    --num_workers 4 \\\n    --bands R,G,B,SWIR,NIR,QA,DEM \\\n    --spectral_indices DEMWM,NDVI,ATSAVI,AVI,CI,ClGreen,GBNDVI,GVMI,IPVI,KIVU,MCARI,MVI,NormNIR,PNDVI,SABI,WDRVI,mCRIG \\\n    --image_size 320 \\\n    --resize_strategy resize \\\n    --interpolation nearest \\\n    --fill_missing_pixels_with_torch_nan True \\\n    --mask_using_qa True \\\n    --mask_using_water_mask True \\\n    --use_weighted_sampler True \\\n    --samples_per_epoch 10240 \\\n    --has_kelp_importance_factor 3 \\\n    --kelp_pixels_pct_importance_factor 0.2 \\\n    --qa_ok_importance_factor 0 \\\n    --qa_corrupted_pixels_pct_importance_factor -1 \\\n    --almost_all_water_importance_factor 0.5 \\\n    --dem_nan_pixels_pct_importance_factor 0.25 \\\n    --dem_zero_pixels_pct_importance_factor -1 \\\n    --normalization_strategy quantile \\\n    --architecture unet \\\n    --encoder tu-efficientnet_b5 \\\n    --pretrained True \\\n    --encoder_weights imagenet \\\n    --lr 3e-4 \\\n    --optimizer adamw \\\n    --weight_decay 1e-4 \\\n    --loss dice \\\n    --monitor_metric val/dice \\\n    --save_top_k 1 \\\n    --early_stopping_patience 50 \\\n    --precision 16-mixed \\\n    --epochs 10 \\\n    --swa False \\\n    --sahi True\n</code></pre>"},{"location":"guides/sahi/#inference","title":"Inference","text":"<p>To run model prediction on selected directory of images run:</p> <pre><code>python ./kelp/nn/inference/predict.py \\\n    --data_dir=data/raw/splits/split_8/images \\\n    --dataset_stats_dir=data/processed \\\n    --output_dir=data/predictions/sahi-split=8 \\\n    --run_dir=mlruns/567580247645556359/5691fc348f874ffdb2fc6c9616e11246 \\\n    --decision_threshold=0.48 \\\n    --sahi_tile_size=128 \\\n    --sahi_overlap=64\n</code></pre> <p>Note: The crop resize strategy including image_size will be resolved at runtime using original training config.</p> <p>To make a submission file:</p> <pre><code>python ./kelp/nn/inference/predict_and_submit.py\n    --data_dir=data/raw/test/images \\\n    --dataset_stats_dir=data/processed \\\n    --output_dir=data/submissions/sahi \\\n    --run_dir=mlruns/567580247645556359/5691fc348f874ffdb2fc6c9616e11246 \\\n    --precision=bf16-mixed \\\n    --decision_threshold=0.48 \\\n    --preview_submission \\\n    --preview_first_n=10 \\\n    --sahi_tile_size=128 \\\n    --sahi_overlap=64\n</code></pre>"},{"location":"guides/sahi/#results","title":"Results","text":"<p>Best model trained on 128x128 crops with 320x320 resize and nearest interpolation resulted in public LB score of: 0.6848.</p>"},{"location":"guides/sahi/#visualizations","title":"Visualizations","text":"<ul> <li> <p>True Color </p> </li> <li> <p>Color Infrared </p> </li> <li> <p>Shortwave Infrared </p> </li> <li> <p>DEM </p> </li> <li> <p>QA </p> </li> <li> <p>NDVI </p> </li> <li> <p>Ground Truth Mask </p> </li> <li> <p>Predictions @epoch=9 </p> </li> </ul>"},{"location":"guides/setup-dev-env/","title":"Setting up dev environment","text":""},{"location":"guides/setup-dev-env/#requirements","title":"Requirements","text":"<p>In order to set up local development environment make sure you have installed:</p> <ul> <li>conda</li> <li>conda-lock</li> </ul> <p>You can install <code>conda-lock</code> on your <code>base</code> environment by running:</p> <pre><code>conda install -c conda-forge conda-lock -n base\n</code></pre>"},{"location":"guides/setup-dev-env/#using-makefile","title":"Using Makefile","text":"<p>Run:</p> <pre><code>make local-env\n</code></pre> <p>It will also install <code>pre-commit</code> hooks and the project in an editable mode. Once done you can activate the environment by running:</p> <pre><code>conda activate kelp\n</code></pre>"},{"location":"guides/setup-dev-env/#manually","title":"Manually","text":"<ol> <li> <p>Run <code>conda-lock</code> command:     <pre><code>conda-lock install --mamba -n kelp conda-lock.yml\n</code></pre></p> </li> <li> <p>Activate the env:     <pre><code>conda activate kelp\n</code></pre></p> </li> <li> <p>Install <code>pre-commit</code> hooks:     <pre><code>pre-commit install\n</code></pre></p> </li> <li> <p>Install the project in an editable mode:     <pre><code>pip install -e .\n</code></pre></p> </li> </ol>"},{"location":"guides/setup-dev-env/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>This project uses <code>pre-commit</code> package for managing and maintaining <code>pre-commit</code> hooks.</p> <p>To ensure code quality - please make sure that you have it configured.</p> <ol> <li> <p>Install <code>pre-commit</code> and following packages: <code>isort</code>, <code>black</code>, <code>flake8</code>, <code>mypy</code>, <code>pytest</code>.</p> </li> <li> <p>Install <code>pre-commit</code> hooks by running: <code>pre-commit install</code></p> </li> <li> <p>The command above will automatically run formatters, code checks and other steps defined in the <code>.pre-commit-config.yaml</code></p> </li> <li> <p>All of those checks will also be run whenever a new commit is being created i.e. when you run <code>git commit -m \"blah\"</code></p> </li> <li> <p>You can also run it manually with this command: <code>pre-commit run --all-files</code></p> </li> </ol> <p>You can manually disable <code>pre-commit</code> hooks by running: <code>pre-commit uninstall</code> Use this only in exceptional cases.</p>"},{"location":"guides/setup-dev-env/#setup-environmental-variables","title":"Setup environmental variables","text":"<p>NOTE: The .env files are only needed if you plan to run Azure ML Pipelines.</p> <p>Ask your colleagues for <code>.env</code> files which aren't included in this repository and put them inside the repo's root directory.</p> <p>To see what variables you need see the <code>.env-sample</code> file.</p>"},{"location":"guides/setup-dev-env/#torch-ort-support-optional","title":"Torch-ORT support (optional)","text":"<p>Optionally, you can enable <code>torch-ort</code> support by configuring it via <code>Makefile</code> command:</p> <pre><code>make configure-torch-ort\n</code></pre> <p>Make sure <code>torch-ort</code> is in the <code>conda-lock</code> file before doing so!</p>"},{"location":"guides/submissions/","title":"Making submissions","text":"<p>In order to create a submission file you have three options:</p> <ol> <li>Run <code>predict.py</code> to generate predictions and then <code>submit.py</code> to create <code>tar</code> file with predictions</li> <li>Run <code>predict_and_submit.py</code> to generate both predictions and the submission file in one go.</li> <li>Run <code>average_predictions.py</code> that will take the predictions from multiple models, average them and then crate submission file.</li> </ol>"},{"location":"guides/submissions/#single-model","title":"Single model","text":""},{"location":"guides/submissions/#predict-and-then-submit","title":"Predict and then Submit","text":"<p>Assuming you've already run <code>predict.py</code>:</p> <pre><code>make submission\n</code></pre> <p>The same can be achieved by running python script directly:</p> <pre><code>python ./kelp/core/submission.py \\\n    --predictions_dir $(PREDS_OUTPUT_DIR) \\\n    --output_dir data/submissions/single-model\n</code></pre> <p>If you don't have predictions yet, please refer to the Running inference guide to learn how to generate them.</p>"},{"location":"guides/submissions/#in-one-go","title":"In one go","text":"<p>To generate predictions and submission file please adjust the Makefile model and output paths and run:</p> <pre><code>make predict-and-submit\n</code></pre> <p>The same can be achieved by running python script directly:</p> <pre><code>python ./kelp/nn/inference/predict_and_submit.py \\\n    --data_dir data/raw/test/images \\\n    --dataset_stats_dir=data/processed \\\n    --output_dir data/submissions/single-model \\\n    --run_dir $(RUN_DIR) \\\n    --preview_submission \\\n    --decision_threshold 0.45 \\\n    --precision bf16-mixed\n</code></pre>"},{"location":"guides/submissions/#ensemble","title":"Ensemble","text":"<p>To generate predictions and submission file using multiple models please adjust the Makefile model and output paths and run:</p> <pre><code>make cv-predict\n</code></pre> <p>If you already have predictions from multiple models sitting somewhere on you drive please adjust the Makefile prediction dirs and run:</p> <pre><code>make average-predictions\n</code></pre> <p>The same can be achieved by running the python script directly:</p> <pre><code>python ./kelp/nn/inference/average_predictions.py \\\n    --predictions_dirs \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=0 \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=1 \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=2 \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=3 \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=4 \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=5 \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=6 \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=7 \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=8 \\\n        data/predictions/$(AVG_PREDS_VERSION)/fold=9 \\\n    --weights \\\n        $(FOLD_0_WEIGHT) \\\n        $(FOLD_1_WEIGHT) \\\n        $(FOLD_2_WEIGHT) \\\n        $(FOLD_3_WEIGHT) \\\n        $(FOLD_4_WEIGHT) \\\n        $(FOLD_5_WEIGHT) \\\n        $(FOLD_6_WEIGHT) \\\n        $(FOLD_7_WEIGHT) \\\n        $(FOLD_8_WEIGHT) \\\n        $(FOLD_9_WEIGHT) \\\n    --output_dir=$(AVG_PREDS_OUTPUT_DIR) \\\n    --decision_threshold=0.48 \\\n    --test_data_dir=$(PREDS_INPUT_DIR) \\\n    --preview_submission \\\n    --preview_first_n=10\n</code></pre> <p>Note: The number of weights must match the number of prediction dirs. The script will raise an exception if the two lists do not match in length.</p>"},{"location":"guides/tests/","title":"Running tests","text":"<p>This page describes how to run tests locally using <code>pytest</code>.</p>"},{"location":"guides/tests/#instructions","title":"Instructions","text":"<p>To run tests marked as <code>unit</code> tests:</p> <pre><code>pytest -m \"unit\" -v\n</code></pre> <p>To run tests marked as <code>integration</code> tests:</p> <pre><code>pytest -m \"integration\" -v\n</code></pre> <p>To run tests marked as <code>e2e</code> tests:</p> <pre><code>pytest -m \"e2e\" -v\n</code></pre> <p>To run all tests:</p> <pre><code>pytest\n</code></pre> <p>NOTE: Pre-commit hooks will only run those tests marked as <code>unit</code>.</p>"},{"location":"guides/training/","title":"Training models","text":"<p>This page contains instructions on how to run model training that was used in the best submission.</p>"},{"location":"guides/training/#locally","title":"Locally","text":""},{"location":"guides/training/#single-model","title":"Single model","text":"<p>In order to train a single CV fold locally you can run:</p> <pre><code>make train FOLD_NUMBER=&lt;fold_number&gt;\n</code></pre> <p>Or:</p> <pre><code>python ./kelp/nn/training/train.py \\\n    --data_dir data/raw \\\n    --output_dir mlruns \\\n    --metadata_fp data/processed/train_val_test_dataset.parquet \\\n    --dataset_stats_fp data/processed/2023-12-31T20:30:39-stats-fill_value=nan-mask_using_qa=True-mask_using_water_mask=True.json \\\n    --cv_split $(FOLD_NUMBER) \\\n    --batch_size 32 \\\n    --num_workers 4 \\\n    --bands R,G,B,SWIR,NIR,QA,DEM \\\n    --spectral_indices DEMWM,NDVI,ATSAVI,AVI,CI,ClGreen,GBNDVI,GVMI,IPVI,KIVU,MCARI,MVI,NormNIR,PNDVI,SABI,WDRVI,mCRIG \\\n    --image_size 352 \\\n    --resize_strategy pad \\\n    --interpolation nearest \\\n    --fill_missing_pixels_with_torch_nan True \\\n    --mask_using_qa True \\\n    --mask_using_water_mask True \\\n    --use_weighted_sampler True \\\n    --samples_per_epoch 10240 \\\n    --has_kelp_importance_factor 3 \\\n    --kelp_pixels_pct_importance_factor 0.2 \\\n    --qa_ok_importance_factor 0 \\\n    --qa_corrupted_pixels_pct_importance_factor -1 \\\n    --almost_all_water_importance_factor 0.5 \\\n    --dem_nan_pixels_pct_importance_factor 0.25 \\\n    --dem_zero_pixels_pct_importance_factor -1 \\\n    --normalization_strategy quantile \\\n    --architecture unet \\\n    --encoder tu-efficientnet_b5 \\\n    --pretrained True \\\n    --encoder_weights imagenet \\\n    --lr 3e-4 \\\n    --optimizer adamw \\\n    --weight_decay 1e-4 \\\n    --loss dice \\\n    --monitor_metric val/dice \\\n    --save_top_k 1 \\\n    --early_stopping_patience 50 \\\n    --precision 16-mixed \\\n    --epochs 50 \\\n    --swa False\n</code></pre> <p>The training artifacts including:</p> <ul> <li>Parameters</li> <li>Metrics:<ul> <li>DICE</li> <li>IOU (Jaccard Index)</li> <li>F1-Score</li> <li>Precision</li> <li>Recall</li> <li>Accuracy</li> </ul> </li> <li>Sample batch of images and predictions:<ul> <li>True color composites</li> <li>Color infrared composites</li> <li>Shortwave infrared composites</li> <li>Spectral indices</li> <li>DEM</li> <li>QA</li> <li>Ground truth masks</li> <li>Predicted masks</li> </ul> </li> <li>Confusion matrices</li> <li>Best and latest model checkpoints</li> </ul> <p>Are automatically logged using MLFlow.</p> <p>See MLFlow artifacts to see how those images look like.</p>"},{"location":"guides/training/#all-folds","title":"All folds","text":"<pre><code>make train-all-folds\n</code></pre> <p>Or:</p> <pre><code>make train FOLD_NUMBER=0\nmake train FOLD_NUMBER=1\n...\nmake train FOLD_NUMBER=9\n</code></pre> <p>Or:</p> <pre><code>python ./kelp/nn/training/train.py \\\n    --data_dir data/raw \\\n    ...\n\npython ./kelp/nn/training/train.py \\\n    --data_dir data/raw \\\n    ...\n\n...\n</code></pre>"},{"location":"guides/training/#on-azure-ml","title":"On Azure ML","text":"<p>Note: The training details in this section assume you have configured Azure ML Workspace and Azure DevOps Organization properly and have all assets, resources, secrets and pipelines in place.</p>"},{"location":"guides/training/#single-model_1","title":"Single model","text":"<p>Go to your Azure DevOps organization and trigger the Training Pipeline:</p> <p></p> <p>Most of the configuration settings can be changed in the ADO portal.</p> <p>After going to the Azure ML Workspace the new experiment run will appear on the Jobs page.</p> <p></p> <p>After clicking on a single job, you'll see the training pipeline.</p> <p></p> <p>If you click on the Model Training step you'll see the training job progress, logs and outputs (the same as with local run).</p> <p></p> <p>All images as with local run are also logged.</p> <p></p> <p>The metrics tab will show all metrics that were logged for training, validation and test phases.</p> <p></p>"},{"location":"guides/training/#all-folds_1","title":"All folds","text":"<p>Same as with single model training - there is a dedicated pipeline for training all folds under a single Job.</p> <p></p> <p>Note: The same ADO Pipeline can be used to trigger HParam Search Job. In fact the CV fold training pipeline under the hood uses Sweep Job to perform training of individual folds.</p> <p></p> <p>If you go to the child job of the Sweep Step you'll find the individual fold training jobs.</p> <p></p> <p>Note: Since we are using spot instances, sometimes the training jobs can be interrupted - as seen in the image above. In that case you'll need to re-run failed jobs manually.</p>"},{"location":"guides/training/#downloading-the-results","title":"Downloading the results","text":"<p>After the training is finished you'll be able to download the outputs and logs of the job. While on Outputs + logs tab, click Download all button. All training artifacts will be compressed into zip archive and the download process should start in a couple of minutes. If it does not - please check the notifications in the portal and click on download link manually. Once the download finishes, extract the job outputs to the directory of your choosing.</p> <p></p>"},{"location":"guides/xgb-stuff/","title":"XGBoost","text":"<p>The <code>XGBoost</code> model was trained only on 50% of data since local machine only had 32 GB of RAM and loading full dataset with all spectral indices was not possible. Scaling the training to the cloud was not performed.</p>"},{"location":"guides/xgb-stuff/#data-prep","title":"Data prep","text":"<p>For training <code>XGBoostClassifier</code> model a pixel-level dataset is needed. Creation of such dataset was done by buffering the kelp segmentation masks and extracting values for those pixels. A random sample of all pixels was also added to give model a change to account for inland, corrupted, cloudy and open water pixels.</p> <p>To create this dataset run:</p> <pre><code>python ./kelp/data_prep/dataset_prep.py \\\n    --data_dir=data/raw/train \\\n    --metadata_fp=data/processed/stats/dataset_stats.parquet \\\n    --output_dir=data/processed \\\n    --train_size=0.95 \\\n    --test_size=0.5 \\\n    --buffer_pixels=5 \\\n    --random_sample_pixel_frac=0.02\n</code></pre>"},{"location":"guides/xgb-stuff/#training","title":"Training","text":"<pre><code>python ./kelp/xgb/training/train.py\n    --dataset_fp=data/processed/train_val_test_pixel_level_dataset.parquet \\\n    --train_data_dir=data/raw/train \\\n    --output_dir=mlruns \\\n    --spectral_indices=all \\\n    --sample_size=0.2 \\\n    --experiment=train-tree-clf-exp \\\n    --explain_model\n</code></pre>"},{"location":"guides/xgb-stuff/#training-results","title":"Training results","text":"<p>The best model with all input bands and all spectral indices had public LB score of 0.5125</p> <p>Sample predictions:</p> <p> </p>"},{"location":"guides/xgb-stuff/#inference","title":"Inference","text":"<p>To run inference on test images directory using model from specific <code>run_dir</code> use:</p> <pre><code>python ./kelp/xgb/inference/predict.py \\\n    --run_dir=&lt;run_dir&gt; \\\n    --data_dir=data/raw/test/images \\\n    --output_dir=data/predictions\n</code></pre> <p>To create submission run:</p> <pre><code>python ./kelp/xgb/inference/predict_and_submit.py \\\n    --run_dir=&lt;run_dir&gt; \\\n    --data_dir=data/raw/test/images \\\n    --output_dir=data/submissions/xgb \\\n    --preview_submission \\\n    --preview_first_n=10\n</code></pre>"},{"location":"guides/xgb-stuff/#feature-importance","title":"Feature importance","text":"<p>The feature importance (weight) is logged by enabling <code>--explain_model</code> flag in the training config.</p> <p></p>"},{"location":"guides/xgb-stuff/#logged-figures","title":"Logged figures","text":"<ul> <li>Confusion matrices</li> </ul> <ul> <li>Precision-Recall curve</li> </ul> <ul> <li>ROC curve</li> </ul>"}]}